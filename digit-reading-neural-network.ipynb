{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance d’écriture par réseaux de neurones\n",
    "## Chargement de la base de données digits disponible sous sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.neural_network import *\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage d'une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.matshow(digits.images[10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des données et des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage du nombre d'exemples par classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 178 exemples d'images de 0 dans le dataset\n",
      "Il y a 182 exemples d'images de 1 dans le dataset\n",
      "Il y a 177 exemples d'images de 2 dans le dataset\n",
      "Il y a 183 exemples d'images de 3 dans le dataset\n",
      "Il y a 181 exemples d'images de 4 dans le dataset\n",
      "Il y a 182 exemples d'images de 5 dans le dataset\n",
      "Il y a 181 exemples d'images de 6 dans le dataset\n",
      "Il y a 179 exemples d'images de 7 dans le dataset\n",
      "Il y a 174 exemples d'images de 8 dans le dataset\n",
      "Il y a 180 exemples d'images de 9 dans le dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print (\"Il y a\", digits.data[digits.target == a].shape[0], \"exemples d'images de\",a,\"dans le dataset\") for a in list(range(10))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionalité des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les données ont 64 dimensions\n",
      "Chaque dimension correspond à un pixel, qui peut prendre les valeurs: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16.]\n"
     ]
    }
   ],
   "source": [
    "print(\"les données ont\", X.shape[1], \"dimensions\")\n",
    "print(\"Chaque dimension correspond à un pixel, qui peut prendre les valeurs:\",np.unique(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation en base d'apprentissage et base de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.7, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etude de l'influence du nombre de neurones de la couche cachée sur le résultat (attention, ce calcul prends approximativement 30 minutes d'éxécution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.42375374\n",
      "Iteration 2, loss = 2.29964088\n",
      "Iteration 3, loss = 2.29594055\n",
      "Iteration 4, loss = 2.29361386\n",
      "Iteration 5, loss = 2.29169799\n",
      "Iteration 6, loss = 2.29226893\n",
      "Iteration 7, loss = 2.29256217\n",
      "Iteration 8, loss = 2.29324091\n",
      "Iteration 9, loss = 2.29202182\n",
      "Iteration 10, loss = 2.29221219\n",
      "Iteration 11, loss = 2.29367644\n",
      "Iteration 12, loss = 2.29270532\n",
      "Iteration 13, loss = 2.29389557\n",
      "Iteration 14, loss = 2.29312271\n",
      "Iteration 15, loss = 2.29246813\n",
      "Iteration 16, loss = 2.29253885\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 17, loss = 2.28504954\n",
      "Iteration 18, loss = 2.28432850\n",
      "Iteration 19, loss = 2.28425582\n",
      "Iteration 20, loss = 2.28413567\n",
      "Iteration 21, loss = 2.28432375\n",
      "Iteration 22, loss = 2.28415828\n",
      "Iteration 23, loss = 2.28405715\n",
      "Iteration 24, loss = 2.28403933\n",
      "Iteration 25, loss = 2.28403484\n",
      "Iteration 26, loss = 2.28399809\n",
      "Iteration 27, loss = 2.28405099\n",
      "Iteration 28, loss = 2.28396242\n",
      "Iteration 29, loss = 2.28430899\n",
      "Iteration 30, loss = 2.28429845\n",
      "Iteration 31, loss = 2.28397549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 32, loss = 2.28226237\n",
      "Iteration 33, loss = 2.28216519\n",
      "Iteration 34, loss = 2.28219135\n",
      "Iteration 35, loss = 2.28220469\n",
      "Iteration 36, loss = 2.28211714\n",
      "Iteration 37, loss = 2.28218692\n",
      "Iteration 38, loss = 2.28219364\n",
      "Iteration 39, loss = 2.28220396\n",
      "Iteration 40, loss = 2.28208905\n",
      "Iteration 41, loss = 2.28216081\n",
      "Iteration 42, loss = 2.28205036\n",
      "Iteration 43, loss = 2.28218659\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 44, loss = 2.28176742\n",
      "Iteration 45, loss = 2.28175587\n",
      "Iteration 46, loss = 2.28174498\n",
      "Iteration 47, loss = 2.28176192\n",
      "Iteration 48, loss = 2.28176328\n",
      "Iteration 49, loss = 2.28174656\n",
      "Iteration 50, loss = 2.28175189\n",
      "Iteration 51, loss = 2.28175516\n",
      "Iteration 52, loss = 2.28173318\n",
      "Iteration 53, loss = 2.28176285\n",
      "Iteration 54, loss = 2.28169999\n",
      "Iteration 55, loss = 2.28176619\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 56, loss = 2.28165918\n",
      "Iteration 57, loss = 2.28165687\n",
      "Iteration 58, loss = 2.28165610\n",
      "Iteration 59, loss = 2.28165564\n",
      "Iteration 60, loss = 2.28165438\n",
      "Iteration 61, loss = 2.28165208\n",
      "Iteration 62, loss = 2.28165381\n",
      "Iteration 63, loss = 2.28165512\n",
      "Iteration 64, loss = 2.28165679\n",
      "Iteration 65, loss = 2.28165723\n",
      "Iteration 66, loss = 2.28165586\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 67, loss = 2.28163541\n",
      "Iteration 68, loss = 2.28163517\n",
      "Iteration 69, loss = 2.28163504\n",
      "Iteration 70, loss = 2.28163493\n",
      "Iteration 71, loss = 2.28163509\n",
      "Iteration 72, loss = 2.28163522\n",
      "Iteration 73, loss = 2.28163462\n",
      "Iteration 74, loss = 2.28163514\n",
      "Iteration 75, loss = 2.28163522\n",
      "Iteration 76, loss = 2.28163491\n",
      "Iteration 77, loss = 2.28163508\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 2.48145557\n",
      "Iteration 2, loss = 2.28841747\n",
      "Iteration 3, loss = 2.28278053\n",
      "Iteration 4, loss = 2.28023194\n",
      "Iteration 5, loss = 2.28124609\n",
      "Iteration 6, loss = 2.28102350\n",
      "Iteration 7, loss = 2.27996232\n",
      "Iteration 8, loss = 2.28137104\n",
      "Iteration 9, loss = 2.28117407\n",
      "Iteration 10, loss = 2.28205340\n",
      "Iteration 11, loss = 2.28217687\n",
      "Iteration 12, loss = 2.28132042\n",
      "Iteration 13, loss = 2.28369661\n",
      "Iteration 14, loss = 2.28266202\n",
      "Iteration 15, loss = 2.28165347\n",
      "Iteration 16, loss = 2.28075664\n",
      "Iteration 17, loss = 2.28123065\n",
      "Iteration 18, loss = 2.28077247\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 19, loss = 2.27035632\n",
      "Iteration 20, loss = 2.26833259\n",
      "Iteration 21, loss = 2.26831875\n",
      "Iteration 22, loss = 2.26793972\n",
      "Iteration 23, loss = 2.26748174\n",
      "Iteration 24, loss = 2.26784984\n",
      "Iteration 25, loss = 2.26822953\n",
      "Iteration 26, loss = 2.26792242\n",
      "Iteration 27, loss = 2.26816255\n",
      "Iteration 28, loss = 2.26826326\n",
      "Iteration 29, loss = 2.26832162\n",
      "Iteration 30, loss = 2.26822611\n",
      "Iteration 31, loss = 2.26788987\n",
      "Iteration 32, loss = 2.26808867\n",
      "Iteration 33, loss = 2.26805716\n",
      "Iteration 34, loss = 2.26777249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 35, loss = 2.26530470\n",
      "Iteration 36, loss = 2.26526550\n",
      "Iteration 37, loss = 2.26517800\n",
      "Iteration 38, loss = 2.26506046\n",
      "Iteration 39, loss = 2.26519973\n",
      "Iteration 40, loss = 2.26512115\n",
      "Iteration 41, loss = 2.26512715\n",
      "Iteration 42, loss = 2.26514097\n",
      "Iteration 43, loss = 2.26526861\n",
      "Iteration 44, loss = 2.26516306\n",
      "Iteration 45, loss = 2.26516546\n",
      "Iteration 46, loss = 2.26512727\n",
      "Iteration 47, loss = 2.26507909\n",
      "Iteration 48, loss = 2.26515496\n",
      "Iteration 49, loss = 2.26514365\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 50, loss = 2.26452694\n",
      "Iteration 51, loss = 2.26441817\n",
      "Iteration 52, loss = 2.26450495\n",
      "Iteration 53, loss = 2.26451044\n",
      "Iteration 54, loss = 2.26449104\n",
      "Iteration 55, loss = 2.26447621\n",
      "Iteration 56, loss = 2.26451267\n",
      "Iteration 57, loss = 2.26452142\n",
      "Iteration 58, loss = 2.26449295\n",
      "Iteration 59, loss = 2.26442375\n",
      "Iteration 60, loss = 2.26447744\n",
      "Iteration 61, loss = 2.26447783\n",
      "Iteration 62, loss = 2.26447263\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 63, loss = 2.26434007\n",
      "Iteration 64, loss = 2.26433350\n",
      "Iteration 65, loss = 2.26433960\n",
      "Iteration 66, loss = 2.26433869\n",
      "Iteration 67, loss = 2.26434106\n",
      "Iteration 68, loss = 2.26433803\n",
      "Iteration 69, loss = 2.26432573\n",
      "Iteration 70, loss = 2.26433868\n",
      "Iteration 71, loss = 2.26434362\n",
      "Iteration 72, loss = 2.26433532\n",
      "Iteration 73, loss = 2.26433990\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 74, loss = 2.26430596\n",
      "Iteration 75, loss = 2.26430581\n",
      "Iteration 76, loss = 2.26430494\n",
      "Iteration 77, loss = 2.26430558\n",
      "Iteration 78, loss = 2.26430594\n",
      "Iteration 79, loss = 2.26430584\n",
      "Iteration 80, loss = 2.26430520\n",
      "Iteration 81, loss = 2.26430581\n",
      "Iteration 82, loss = 2.26430485\n",
      "Iteration 83, loss = 2.26430539\n",
      "Iteration 84, loss = 2.26430621\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 2.55346201\n",
      "Iteration 2, loss = 2.27771522\n",
      "Iteration 3, loss = 2.27446296\n",
      "Iteration 4, loss = 2.27319894\n",
      "Iteration 5, loss = 2.27190960\n",
      "Iteration 6, loss = 2.27060684\n",
      "Iteration 7, loss = 2.27269866\n",
      "Iteration 8, loss = 2.27013133\n",
      "Iteration 9, loss = 2.27360400\n",
      "Iteration 10, loss = 2.27192734\n",
      "Iteration 11, loss = 2.27547854\n",
      "Iteration 12, loss = 2.27235572\n",
      "Iteration 13, loss = 2.27227682\n",
      "Iteration 14, loss = 2.27189539\n",
      "Iteration 15, loss = 2.27356119\n",
      "Iteration 16, loss = 2.27222039\n",
      "Iteration 17, loss = 2.27518282\n",
      "Iteration 18, loss = 2.27374037\n",
      "Iteration 19, loss = 2.27150777\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 20, loss = 2.25437922\n",
      "Iteration 21, loss = 2.25349678\n",
      "Iteration 22, loss = 2.25403226\n",
      "Iteration 23, loss = 2.25344909\n",
      "Iteration 24, loss = 2.25350349\n",
      "Iteration 25, loss = 2.25361292\n",
      "Iteration 26, loss = 2.25334884\n",
      "Iteration 27, loss = 2.25338164\n",
      "Iteration 28, loss = 2.25379027\n",
      "Iteration 29, loss = 2.25291947\n",
      "Iteration 30, loss = 2.25314116\n",
      "Iteration 31, loss = 2.25343704\n",
      "Iteration 32, loss = 2.25315524\n",
      "Iteration 33, loss = 2.25351762\n",
      "Iteration 34, loss = 2.25336439\n",
      "Iteration 35, loss = 2.25355801\n",
      "Iteration 36, loss = 2.25319016\n",
      "Iteration 37, loss = 2.25378009\n",
      "Iteration 38, loss = 2.25337376\n",
      "Iteration 39, loss = 2.25345270\n",
      "Iteration 40, loss = 2.25357743\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 41, loss = 2.24933595\n",
      "Iteration 42, loss = 2.24903025\n",
      "Iteration 43, loss = 2.24894021\n",
      "Iteration 44, loss = 2.24895249\n",
      "Iteration 45, loss = 2.24905307\n",
      "Iteration 46, loss = 2.24904473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47, loss = 2.24892445\n",
      "Iteration 48, loss = 2.24910316\n",
      "Iteration 49, loss = 2.24905134\n",
      "Iteration 50, loss = 2.24898956\n",
      "Iteration 51, loss = 2.24884759\n",
      "Iteration 52, loss = 2.24904379\n",
      "Iteration 53, loss = 2.24897068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 54, loss = 2.24804248\n",
      "Iteration 55, loss = 2.24797433\n",
      "Iteration 56, loss = 2.24796231\n",
      "Iteration 57, loss = 2.24793526\n",
      "Iteration 58, loss = 2.24800895\n",
      "Iteration 59, loss = 2.24798421\n",
      "Iteration 60, loss = 2.24796654\n",
      "Iteration 61, loss = 2.24800421\n",
      "Iteration 62, loss = 2.24801180\n",
      "Iteration 63, loss = 2.24795237\n",
      "Iteration 64, loss = 2.24797857\n",
      "Iteration 65, loss = 2.24797319\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 66, loss = 2.24775438\n",
      "Iteration 67, loss = 2.24774142\n",
      "Iteration 68, loss = 2.24774988\n",
      "Iteration 69, loss = 2.24774117\n",
      "Iteration 70, loss = 2.24773571\n",
      "Iteration 71, loss = 2.24774440\n",
      "Iteration 72, loss = 2.24775512\n",
      "Iteration 73, loss = 2.24774706\n",
      "Iteration 74, loss = 2.24774750\n",
      "Iteration 75, loss = 2.24774022\n",
      "Iteration 76, loss = 2.24774502\n",
      "Iteration 77, loss = 2.24774758\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 78, loss = 2.24769381\n",
      "Iteration 79, loss = 2.24769439\n",
      "Iteration 80, loss = 2.24769410\n",
      "Iteration 81, loss = 2.24769360\n",
      "Iteration 82, loss = 2.24769317\n",
      "Iteration 83, loss = 2.24769418\n",
      "Iteration 84, loss = 2.24769458\n",
      "Iteration 85, loss = 2.24769321\n",
      "Iteration 86, loss = 2.24769352\n",
      "Iteration 87, loss = 2.24769289\n",
      "Iteration 88, loss = 2.24769489\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 2.59303129\n",
      "Iteration 2, loss = 2.26399794\n",
      "Iteration 3, loss = 2.26371866\n",
      "Iteration 4, loss = 2.26424396\n",
      "Iteration 5, loss = 2.26492224\n",
      "Iteration 6, loss = 2.26555653\n",
      "Iteration 7, loss = 2.26409397\n",
      "Iteration 8, loss = 2.26120161\n",
      "Iteration 9, loss = 2.26275901\n",
      "Iteration 10, loss = 2.26231619\n",
      "Iteration 11, loss = 2.26234755\n",
      "Iteration 12, loss = 2.26371165\n",
      "Iteration 13, loss = 2.26483110\n",
      "Iteration 14, loss = 2.26450822\n",
      "Iteration 15, loss = 2.26396222\n",
      "Iteration 16, loss = 2.26301965\n",
      "Iteration 17, loss = 2.26497587\n",
      "Iteration 18, loss = 2.25995410\n",
      "Iteration 19, loss = 2.26213353\n",
      "Iteration 20, loss = 2.26259988\n",
      "Iteration 21, loss = 2.26272958\n",
      "Iteration 22, loss = 2.26110762\n",
      "Iteration 23, loss = 2.26320471\n",
      "Iteration 24, loss = 2.26584066\n",
      "Iteration 25, loss = 2.26323951\n",
      "Iteration 26, loss = 2.26406257\n",
      "Iteration 27, loss = 2.26019247\n",
      "Iteration 28, loss = 2.26238667\n",
      "Iteration 29, loss = 2.26217322\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 30, loss = 2.24134528\n",
      "Iteration 31, loss = 2.24012525\n",
      "Iteration 32, loss = 2.23996219\n",
      "Iteration 33, loss = 2.23954031\n",
      "Iteration 34, loss = 2.23963964\n",
      "Iteration 35, loss = 2.24035216\n",
      "Iteration 36, loss = 2.24041897\n",
      "Iteration 37, loss = 2.23963788\n",
      "Iteration 38, loss = 2.23966240\n",
      "Iteration 39, loss = 2.23959210\n",
      "Iteration 40, loss = 2.23995943\n",
      "Iteration 41, loss = 2.23954372\n",
      "Iteration 42, loss = 2.23998460\n",
      "Iteration 43, loss = 2.23923723\n",
      "Iteration 44, loss = 2.23982069\n",
      "Iteration 45, loss = 2.23976474\n",
      "Iteration 46, loss = 2.23959265\n",
      "Iteration 47, loss = 2.24081612\n",
      "Iteration 48, loss = 2.23933174\n",
      "Iteration 49, loss = 2.24028370\n",
      "Iteration 50, loss = 2.23982858\n",
      "Iteration 51, loss = 2.23993363\n",
      "Iteration 52, loss = 2.23943610\n",
      "Iteration 53, loss = 2.23951623\n",
      "Iteration 54, loss = 2.23961925\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 55, loss = 2.23489220\n",
      "Iteration 56, loss = 2.23474248\n",
      "Iteration 57, loss = 2.23481117\n",
      "Iteration 58, loss = 2.23469255\n",
      "Iteration 59, loss = 2.23462759\n",
      "Iteration 60, loss = 2.23452369\n",
      "Iteration 61, loss = 2.23472713\n",
      "Iteration 62, loss = 2.23459644\n",
      "Iteration 63, loss = 2.23437635\n",
      "Iteration 64, loss = 2.23460652\n",
      "Iteration 65, loss = 2.23457906\n",
      "Iteration 66, loss = 2.23463674\n",
      "Iteration 67, loss = 2.23459743\n",
      "Iteration 68, loss = 2.23468229\n",
      "Iteration 69, loss = 2.23472658\n",
      "Iteration 70, loss = 2.23461937\n",
      "Iteration 71, loss = 2.23464995\n",
      "Iteration 72, loss = 2.23453354\n",
      "Iteration 73, loss = 2.23474422\n",
      "Iteration 74, loss = 2.23448857\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 75, loss = 2.23347965\n",
      "Iteration 76, loss = 2.23346320\n",
      "Iteration 77, loss = 2.23338000\n",
      "Iteration 78, loss = 2.23340507\n",
      "Iteration 79, loss = 2.23340409\n",
      "Iteration 80, loss = 2.23346910\n",
      "Iteration 81, loss = 2.23343977\n",
      "Iteration 82, loss = 2.23346697\n",
      "Iteration 83, loss = 2.23341977\n",
      "Iteration 84, loss = 2.23343684\n",
      "Iteration 85, loss = 2.23341041\n",
      "Iteration 86, loss = 2.23335196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 87, loss = 2.23319534\n",
      "Iteration 88, loss = 2.23315280\n",
      "Iteration 89, loss = 2.23314942\n",
      "Iteration 90, loss = 2.23315335\n",
      "Iteration 91, loss = 2.23314572\n",
      "Iteration 92, loss = 2.23314461\n",
      "Iteration 93, loss = 2.23315250\n",
      "Iteration 94, loss = 2.23314222\n",
      "Iteration 95, loss = 2.23315857\n",
      "Iteration 96, loss = 2.23314010\n",
      "Iteration 97, loss = 2.23314847\n",
      "Iteration 98, loss = 2.23314556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 99, loss = 2.23308894\n",
      "Iteration 100, loss = 2.23308812\n",
      "Iteration 101, loss = 2.23308870\n",
      "Iteration 102, loss = 2.23308828\n",
      "Iteration 103, loss = 2.23308720\n",
      "Iteration 104, loss = 2.23308749\n",
      "Iteration 105, loss = 2.23308752\n",
      "Iteration 106, loss = 2.23308731\n",
      "Iteration 107, loss = 2.23308705\n",
      "Iteration 108, loss = 2.23308693\n",
      "Iteration 109, loss = 2.23308717\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 2.63577674\n",
      "Iteration 2, loss = 2.26034922\n",
      "Iteration 3, loss = 2.25546531\n",
      "Iteration 4, loss = 2.25564117\n",
      "Iteration 5, loss = 2.25412216\n",
      "Iteration 6, loss = 2.25612687\n",
      "Iteration 7, loss = 2.25191636\n",
      "Iteration 8, loss = 2.25334960\n",
      "Iteration 9, loss = 2.25106921\n",
      "Iteration 10, loss = 2.25550463\n",
      "Iteration 11, loss = 2.25505491\n",
      "Iteration 12, loss = 2.25999961\n",
      "Iteration 13, loss = 2.25422113\n",
      "Iteration 14, loss = 2.25592064\n",
      "Iteration 15, loss = 2.25280443\n",
      "Iteration 16, loss = 2.25500667\n",
      "Iteration 17, loss = 2.25220275\n",
      "Iteration 18, loss = 2.25380754\n",
      "Iteration 19, loss = 2.25285972\n",
      "Iteration 20, loss = 2.25092270\n",
      "Iteration 21, loss = 2.25323318\n",
      "Iteration 22, loss = 2.25614503\n",
      "Iteration 23, loss = 2.25431684\n",
      "Iteration 24, loss = 2.25411582\n",
      "Iteration 25, loss = 2.25379666\n",
      "Iteration 26, loss = 2.25484464\n",
      "Iteration 27, loss = 2.25363346\n",
      "Iteration 28, loss = 2.25299811\n",
      "Iteration 29, loss = 2.25291783\n",
      "Iteration 30, loss = 2.25307315\n",
      "Iteration 31, loss = 2.25771007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 32, loss = 2.22833978\n",
      "Iteration 33, loss = 2.22783321\n",
      "Iteration 34, loss = 2.22798869\n",
      "Iteration 35, loss = 2.22844635\n",
      "Iteration 36, loss = 2.22750232\n",
      "Iteration 37, loss = 2.22808046\n",
      "Iteration 38, loss = 2.22793687\n",
      "Iteration 39, loss = 2.22855551\n",
      "Iteration 40, loss = 2.22804314\n",
      "Iteration 41, loss = 2.22711978\n",
      "Iteration 42, loss = 2.22879538\n",
      "Iteration 43, loss = 2.22789998\n",
      "Iteration 44, loss = 2.22865713\n",
      "Iteration 45, loss = 2.22818608\n",
      "Iteration 46, loss = 2.22847263\n",
      "Iteration 47, loss = 2.22812682\n",
      "Iteration 48, loss = 2.22801015\n",
      "Iteration 49, loss = 2.22768455\n",
      "Iteration 50, loss = 2.22742411\n",
      "Iteration 51, loss = 2.22801926\n",
      "Iteration 52, loss = 2.22807452\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 53, loss = 2.22291326\n",
      "Iteration 54, loss = 2.22231767\n",
      "Iteration 55, loss = 2.22204580\n",
      "Iteration 56, loss = 2.22226893\n",
      "Iteration 57, loss = 2.22211919\n",
      "Iteration 58, loss = 2.22221694\n",
      "Iteration 59, loss = 2.22244911\n",
      "Iteration 60, loss = 2.22217081\n",
      "Iteration 61, loss = 2.22243167\n",
      "Iteration 62, loss = 2.22232286\n",
      "Iteration 63, loss = 2.22226768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64, loss = 2.22214716\n",
      "Iteration 65, loss = 2.22209699\n",
      "Iteration 66, loss = 2.22220447\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 67, loss = 2.22098037\n",
      "Iteration 68, loss = 2.22091754\n",
      "Iteration 69, loss = 2.22095659\n",
      "Iteration 70, loss = 2.22084062\n",
      "Iteration 71, loss = 2.22094271\n",
      "Iteration 72, loss = 2.22090087\n",
      "Iteration 73, loss = 2.22094888\n",
      "Iteration 74, loss = 2.22091840\n",
      "Iteration 75, loss = 2.22091401\n",
      "Iteration 76, loss = 2.22092306\n",
      "Iteration 77, loss = 2.22090333\n",
      "Iteration 78, loss = 2.22092259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 79, loss = 2.22062743\n",
      "Iteration 80, loss = 2.22059719\n",
      "Iteration 81, loss = 2.22059820\n",
      "Iteration 82, loss = 2.22060064\n",
      "Iteration 83, loss = 2.22059680\n",
      "Iteration 84, loss = 2.22059868\n",
      "Iteration 85, loss = 2.22061126\n",
      "Iteration 86, loss = 2.22059540\n",
      "Iteration 87, loss = 2.22059253\n",
      "Iteration 88, loss = 2.22059471\n",
      "Iteration 89, loss = 2.22058886\n",
      "Iteration 90, loss = 2.22059013\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 91, loss = 2.22052513\n",
      "Iteration 92, loss = 2.22052531\n",
      "Iteration 93, loss = 2.22052444\n",
      "Iteration 94, loss = 2.22052439\n",
      "Iteration 95, loss = 2.22052401\n",
      "Iteration 96, loss = 2.22052385\n",
      "Iteration 97, loss = 2.22052527\n",
      "Iteration 98, loss = 2.22052434\n",
      "Iteration 99, loss = 2.22052454\n",
      "Iteration 100, loss = 2.22052400\n",
      "Iteration 101, loss = 2.22052553\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 2.70517469\n",
      "Iteration 2, loss = 2.24835850\n",
      "Iteration 3, loss = 2.24662521\n",
      "Iteration 4, loss = 2.24784496\n",
      "Iteration 5, loss = 2.24688190\n",
      "Iteration 6, loss = 2.24609352\n",
      "Iteration 7, loss = 2.24617076\n",
      "Iteration 8, loss = 2.24486198\n",
      "Iteration 9, loss = 2.24833518\n",
      "Iteration 10, loss = 2.24886799\n",
      "Iteration 11, loss = 2.24215661\n",
      "Iteration 12, loss = 2.24539186\n",
      "Iteration 13, loss = 2.24877342\n",
      "Iteration 14, loss = 2.24657100\n",
      "Iteration 15, loss = 2.24692464\n",
      "Iteration 16, loss = 2.24509549\n",
      "Iteration 17, loss = 2.24536174\n",
      "Iteration 18, loss = 2.24721664\n",
      "Iteration 19, loss = 2.24563490\n",
      "Iteration 20, loss = 2.24902989\n",
      "Iteration 21, loss = 2.24746977\n",
      "Iteration 22, loss = 2.24822657\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 23, loss = 2.21915164\n",
      "Iteration 24, loss = 2.21502057\n",
      "Iteration 25, loss = 2.21564324\n",
      "Iteration 26, loss = 2.21621976\n",
      "Iteration 27, loss = 2.21560483\n",
      "Iteration 28, loss = 2.21644806\n",
      "Iteration 29, loss = 2.21641754\n",
      "Iteration 30, loss = 2.21468562\n",
      "Iteration 31, loss = 2.21676043\n",
      "Iteration 32, loss = 2.21605646\n",
      "Iteration 33, loss = 2.21520485\n",
      "Iteration 34, loss = 2.21526549\n",
      "Iteration 35, loss = 2.21618031\n",
      "Iteration 36, loss = 2.21631846\n",
      "Iteration 37, loss = 2.21617630\n",
      "Iteration 38, loss = 2.21492032\n",
      "Iteration 39, loss = 2.21581381\n",
      "Iteration 40, loss = 2.21486162\n",
      "Iteration 41, loss = 2.21585972\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 42, loss = 2.20936717\n",
      "Iteration 43, loss = 2.20870655\n",
      "Iteration 44, loss = 2.20879837\n",
      "Iteration 45, loss = 2.20891544\n",
      "Iteration 46, loss = 2.20852347\n",
      "Iteration 47, loss = 2.20894298\n",
      "Iteration 48, loss = 2.20890602\n",
      "Iteration 49, loss = 2.20868337\n",
      "Iteration 50, loss = 2.20881978\n",
      "Iteration 51, loss = 2.20879598\n",
      "Iteration 52, loss = 2.20854388\n",
      "Iteration 53, loss = 2.20882920\n",
      "Iteration 54, loss = 2.20899921\n",
      "Iteration 55, loss = 2.20891156\n",
      "Iteration 56, loss = 2.20884036\n",
      "Iteration 57, loss = 2.20884585\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 58, loss = 2.20723327\n",
      "Iteration 59, loss = 2.20709056\n",
      "Iteration 60, loss = 2.20715048\n",
      "Iteration 61, loss = 2.20706151\n",
      "Iteration 62, loss = 2.20715801\n",
      "Iteration 63, loss = 2.20715662\n",
      "Iteration 64, loss = 2.20711777\n",
      "Iteration 65, loss = 2.20700137\n",
      "Iteration 66, loss = 2.20707999\n",
      "Iteration 67, loss = 2.20717626\n",
      "Iteration 68, loss = 2.20714567\n",
      "Iteration 69, loss = 2.20705067\n",
      "Iteration 70, loss = 2.20718633\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 71, loss = 2.20680751\n",
      "Iteration 72, loss = 2.20675986\n",
      "Iteration 73, loss = 2.20676351\n",
      "Iteration 74, loss = 2.20674060\n",
      "Iteration 75, loss = 2.20676154\n",
      "Iteration 76, loss = 2.20674386\n",
      "Iteration 77, loss = 2.20676263\n",
      "Iteration 78, loss = 2.20674740\n",
      "Iteration 79, loss = 2.20675437\n",
      "Iteration 80, loss = 2.20674595\n",
      "Iteration 81, loss = 2.20674977\n",
      "Iteration 82, loss = 2.20674942\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 83, loss = 2.20666415\n",
      "Iteration 84, loss = 2.20666642\n",
      "Iteration 85, loss = 2.20666465\n",
      "Iteration 86, loss = 2.20666671\n",
      "Iteration 87, loss = 2.20666621\n",
      "Iteration 88, loss = 2.20666496\n",
      "Iteration 89, loss = 2.20666563\n",
      "Iteration 90, loss = 2.20666472\n",
      "Iteration 91, loss = 2.20666607\n",
      "Iteration 92, loss = 2.20666557\n",
      "Iteration 93, loss = 2.20666457\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 2.72165217\n",
      "Iteration 2, loss = 2.24906214\n",
      "Iteration 3, loss = 2.24619042\n",
      "Iteration 4, loss = 2.24498033\n",
      "Iteration 5, loss = 2.23945807\n",
      "Iteration 6, loss = 2.24442794\n",
      "Iteration 7, loss = 2.24050388\n",
      "Iteration 8, loss = 2.24465958\n",
      "Iteration 9, loss = 2.24164754\n",
      "Iteration 10, loss = 2.24068169\n",
      "Iteration 11, loss = 2.24213729\n",
      "Iteration 12, loss = 2.24059311\n",
      "Iteration 13, loss = 2.24402189\n",
      "Iteration 14, loss = 2.24032599\n",
      "Iteration 15, loss = 2.24029493\n",
      "Iteration 16, loss = 2.24327622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 17, loss = 2.20841491\n",
      "Iteration 18, loss = 2.20405828\n",
      "Iteration 19, loss = 2.20352081\n",
      "Iteration 20, loss = 2.20343567\n",
      "Iteration 21, loss = 2.20274238\n",
      "Iteration 22, loss = 2.20342273\n",
      "Iteration 23, loss = 2.20274223\n",
      "Iteration 24, loss = 2.20355722\n",
      "Iteration 25, loss = 2.20220721\n",
      "Iteration 26, loss = 2.20347991\n",
      "Iteration 27, loss = 2.20377181\n",
      "Iteration 28, loss = 2.20320975\n",
      "Iteration 29, loss = 2.20387596\n",
      "Iteration 30, loss = 2.20269317\n",
      "Iteration 31, loss = 2.20308512\n",
      "Iteration 32, loss = 2.20268633\n",
      "Iteration 33, loss = 2.20291610\n",
      "Iteration 34, loss = 2.20353257\n",
      "Iteration 35, loss = 2.20428638\n",
      "Iteration 36, loss = 2.20257809\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 37, loss = 2.19561415\n",
      "Iteration 38, loss = 2.19507388\n",
      "Iteration 39, loss = 2.19478314\n",
      "Iteration 40, loss = 2.19460277\n",
      "Iteration 41, loss = 2.19446685\n",
      "Iteration 42, loss = 2.19458977\n",
      "Iteration 43, loss = 2.19486799\n",
      "Iteration 44, loss = 2.19451886\n",
      "Iteration 45, loss = 2.19483695\n",
      "Iteration 46, loss = 2.19472370\n",
      "Iteration 47, loss = 2.19435758\n",
      "Iteration 48, loss = 2.19487329\n",
      "Iteration 49, loss = 2.19454483\n",
      "Iteration 50, loss = 2.19469266\n",
      "Iteration 51, loss = 2.19479416\n",
      "Iteration 52, loss = 2.19448795\n",
      "Iteration 53, loss = 2.19463910\n",
      "Iteration 54, loss = 2.19483162\n",
      "Iteration 55, loss = 2.19471914\n",
      "Iteration 56, loss = 2.19467534\n",
      "Iteration 57, loss = 2.19487331\n",
      "Iteration 58, loss = 2.19449973\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 59, loss = 2.19288141\n",
      "Iteration 60, loss = 2.19277450\n",
      "Iteration 61, loss = 2.19276029\n",
      "Iteration 62, loss = 2.19276338\n",
      "Iteration 63, loss = 2.19273968\n",
      "Iteration 64, loss = 2.19268535\n",
      "Iteration 65, loss = 2.19271197\n",
      "Iteration 66, loss = 2.19273335\n",
      "Iteration 67, loss = 2.19278374\n",
      "Iteration 68, loss = 2.19270363\n",
      "Iteration 69, loss = 2.19273009\n",
      "Iteration 70, loss = 2.19276908\n",
      "Iteration 71, loss = 2.19284539\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 72, loss = 2.19231817\n",
      "Iteration 73, loss = 2.19227760\n",
      "Iteration 74, loss = 2.19227225\n",
      "Iteration 75, loss = 2.19225047\n",
      "Iteration 76, loss = 2.19228949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77, loss = 2.19228860\n",
      "Iteration 78, loss = 2.19227694\n",
      "Iteration 79, loss = 2.19228133\n",
      "Iteration 80, loss = 2.19226419\n",
      "Iteration 81, loss = 2.19229074\n",
      "Iteration 82, loss = 2.19227794\n",
      "Iteration 83, loss = 2.19227869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 84, loss = 2.19217847\n",
      "Iteration 85, loss = 2.19217754\n",
      "Iteration 86, loss = 2.19217638\n",
      "Iteration 87, loss = 2.19217666\n",
      "Iteration 88, loss = 2.19217503\n",
      "Iteration 89, loss = 2.19217606\n",
      "Iteration 90, loss = 2.19217495\n",
      "Iteration 91, loss = 2.19217891\n",
      "Iteration 92, loss = 2.19217713\n",
      "Iteration 93, loss = 2.19218020\n",
      "Iteration 94, loss = 2.19217748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 2.78905808\n",
      "Iteration 2, loss = 2.23547456\n",
      "Iteration 3, loss = 2.24030029\n",
      "Iteration 4, loss = 2.23400932\n",
      "Iteration 5, loss = 2.23124385\n",
      "Iteration 6, loss = 2.23729965\n",
      "Iteration 7, loss = 2.23221984\n",
      "Iteration 8, loss = 2.23167484\n",
      "Iteration 9, loss = 2.23095255\n",
      "Iteration 10, loss = 2.23050161\n",
      "Iteration 11, loss = 2.23088629\n",
      "Iteration 12, loss = 2.22943174\n",
      "Iteration 13, loss = 2.23679244\n",
      "Iteration 14, loss = 2.23444088\n",
      "Iteration 15, loss = 2.23148490\n",
      "Iteration 16, loss = 2.23154954\n",
      "Iteration 17, loss = 2.23241447\n",
      "Iteration 18, loss = 2.23353642\n",
      "Iteration 19, loss = 2.22928714\n",
      "Iteration 20, loss = 2.23035883\n",
      "Iteration 21, loss = 2.23371582\n",
      "Iteration 22, loss = 2.22982304\n",
      "Iteration 23, loss = 2.23160182\n",
      "Iteration 24, loss = 2.23102747\n",
      "Iteration 25, loss = 2.23789732\n",
      "Iteration 26, loss = 2.22865327\n",
      "Iteration 27, loss = 2.23321905\n",
      "Iteration 28, loss = 2.23018575\n",
      "Iteration 29, loss = 2.23442833\n",
      "Iteration 30, loss = 2.23546174\n",
      "Iteration 31, loss = 2.23526944\n",
      "Iteration 32, loss = 2.23498438\n",
      "Iteration 33, loss = 2.23076635\n",
      "Iteration 34, loss = 2.23661686\n",
      "Iteration 35, loss = 2.23829213\n",
      "Iteration 36, loss = 2.23104961\n",
      "Iteration 37, loss = 2.23248874\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 38, loss = 2.19627915\n",
      "Iteration 39, loss = 2.19190243\n",
      "Iteration 40, loss = 2.19185113\n",
      "Iteration 41, loss = 2.19209028\n",
      "Iteration 42, loss = 2.19286867\n",
      "Iteration 43, loss = 2.19146146\n",
      "Iteration 44, loss = 2.19173561\n",
      "Iteration 45, loss = 2.19214604\n",
      "Iteration 46, loss = 2.19185308\n",
      "Iteration 47, loss = 2.19244609\n",
      "Iteration 48, loss = 2.19230132\n",
      "Iteration 49, loss = 2.19201168\n",
      "Iteration 50, loss = 2.19156819\n",
      "Iteration 51, loss = 2.19228377\n",
      "Iteration 52, loss = 2.19251260\n",
      "Iteration 53, loss = 2.19276527\n",
      "Iteration 54, loss = 2.19318146\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 55, loss = 2.18507038\n",
      "Iteration 56, loss = 2.18355804\n",
      "Iteration 57, loss = 2.18349123\n",
      "Iteration 58, loss = 2.18339880\n",
      "Iteration 59, loss = 2.18375249\n",
      "Iteration 60, loss = 2.18371659\n",
      "Iteration 61, loss = 2.18355379\n",
      "Iteration 62, loss = 2.18326237\n",
      "Iteration 63, loss = 2.18360584\n",
      "Iteration 64, loss = 2.18370719\n",
      "Iteration 65, loss = 2.18368560\n",
      "Iteration 66, loss = 2.18345837\n",
      "Iteration 67, loss = 2.18386062\n",
      "Iteration 68, loss = 2.18359895\n",
      "Iteration 69, loss = 2.18326133\n",
      "Iteration 70, loss = 2.18363858\n",
      "Iteration 71, loss = 2.18361536\n",
      "Iteration 72, loss = 2.18337735\n",
      "Iteration 73, loss = 2.18336401\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 74, loss = 2.18179444\n",
      "Iteration 75, loss = 2.18161016\n",
      "Iteration 76, loss = 2.18143344\n",
      "Iteration 77, loss = 2.18160310\n",
      "Iteration 78, loss = 2.18155293\n",
      "Iteration 79, loss = 2.18155514\n",
      "Iteration 80, loss = 2.18145328\n",
      "Iteration 81, loss = 2.18156296\n",
      "Iteration 82, loss = 2.18152068\n",
      "Iteration 83, loss = 2.18151284\n",
      "Iteration 84, loss = 2.18153440\n",
      "Iteration 85, loss = 2.18150805\n",
      "Iteration 86, loss = 2.18153145\n",
      "Iteration 87, loss = 2.18149497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 88, loss = 2.18111068\n",
      "Iteration 89, loss = 2.18104150\n",
      "Iteration 90, loss = 2.18104003\n",
      "Iteration 91, loss = 2.18103427\n",
      "Iteration 92, loss = 2.18102176\n",
      "Iteration 93, loss = 2.18103725\n",
      "Iteration 94, loss = 2.18102807\n",
      "Iteration 95, loss = 2.18103093\n",
      "Iteration 96, loss = 2.18103548\n",
      "Iteration 97, loss = 2.18102711\n",
      "Iteration 98, loss = 2.18103350\n",
      "Iteration 99, loss = 2.18102932\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 100, loss = 2.18092313\n",
      "Iteration 101, loss = 2.18092205\n",
      "Iteration 102, loss = 2.18092495\n",
      "Iteration 103, loss = 2.18092263\n",
      "Iteration 104, loss = 2.18092361\n",
      "Iteration 105, loss = 2.18092345\n",
      "Iteration 106, loss = 2.18092251\n",
      "Iteration 107, loss = 2.18092289\n",
      "Iteration 108, loss = 2.18092167\n",
      "Iteration 109, loss = 2.18092264\n",
      "Iteration 110, loss = 2.18092214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 2.82122961\n",
      "Iteration 2, loss = 2.23160605\n",
      "Iteration 3, loss = 2.22845807\n",
      "Iteration 4, loss = 2.22833256\n",
      "Iteration 5, loss = 2.22379514\n",
      "Iteration 6, loss = 2.22457956\n",
      "Iteration 7, loss = 2.23207448\n",
      "Iteration 8, loss = 2.22795276\n",
      "Iteration 9, loss = 2.22736273\n",
      "Iteration 10, loss = 2.22807017\n",
      "Iteration 11, loss = 2.22956371\n",
      "Iteration 12, loss = 2.22562888\n",
      "Iteration 13, loss = 2.22114571\n",
      "Iteration 14, loss = 2.23013111\n",
      "Iteration 15, loss = 2.22580116\n",
      "Iteration 16, loss = 2.22453956\n",
      "Iteration 17, loss = 2.22813740\n",
      "Iteration 18, loss = 2.23042473\n",
      "Iteration 19, loss = 2.22818410\n",
      "Iteration 20, loss = 2.22544992\n",
      "Iteration 21, loss = 2.22722634\n",
      "Iteration 22, loss = 2.22241414\n",
      "Iteration 23, loss = 2.23076505\n",
      "Iteration 24, loss = 2.22688192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 25, loss = 2.18717289\n",
      "Iteration 26, loss = 2.18077098\n",
      "Iteration 27, loss = 2.18011857\n",
      "Iteration 28, loss = 2.18146915\n",
      "Iteration 29, loss = 2.18010173\n",
      "Iteration 30, loss = 2.18076246\n",
      "Iteration 31, loss = 2.18069872\n",
      "Iteration 32, loss = 2.18121639\n",
      "Iteration 33, loss = 2.18084966\n",
      "Iteration 34, loss = 2.18126599\n",
      "Iteration 35, loss = 2.18056855\n",
      "Iteration 36, loss = 2.18036268\n",
      "Iteration 37, loss = 2.18224087\n",
      "Iteration 38, loss = 2.18075217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 39, loss = 2.17227506\n",
      "Iteration 40, loss = 2.17073112\n",
      "Iteration 41, loss = 2.17073914\n",
      "Iteration 42, loss = 2.17054993\n",
      "Iteration 43, loss = 2.17048981\n",
      "Iteration 44, loss = 2.17037876\n",
      "Iteration 45, loss = 2.17060467\n",
      "Iteration 46, loss = 2.17048599\n",
      "Iteration 47, loss = 2.17082412\n",
      "Iteration 48, loss = 2.17049717\n",
      "Iteration 49, loss = 2.17047651\n",
      "Iteration 50, loss = 2.17029501\n",
      "Iteration 51, loss = 2.17050072\n",
      "Iteration 52, loss = 2.17039033\n",
      "Iteration 53, loss = 2.17048157\n",
      "Iteration 54, loss = 2.17079372\n",
      "Iteration 55, loss = 2.17065125\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 56, loss = 2.16841196\n",
      "Iteration 57, loss = 2.16829389\n",
      "Iteration 58, loss = 2.16818911\n",
      "Iteration 59, loss = 2.16822771\n",
      "Iteration 60, loss = 2.16812299\n",
      "Iteration 61, loss = 2.16827031\n",
      "Iteration 62, loss = 2.16828688\n",
      "Iteration 63, loss = 2.16825898\n",
      "Iteration 64, loss = 2.16820573\n",
      "Iteration 65, loss = 2.16825014\n",
      "Iteration 66, loss = 2.16829583\n",
      "Iteration 67, loss = 2.16817646\n",
      "Iteration 68, loss = 2.16813129\n",
      "Iteration 69, loss = 2.16818224\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 70, loss = 2.16775498\n",
      "Iteration 71, loss = 2.16768207\n",
      "Iteration 72, loss = 2.16768104\n",
      "Iteration 73, loss = 2.16768556\n",
      "Iteration 74, loss = 2.16767188\n",
      "Iteration 75, loss = 2.16766075\n",
      "Iteration 76, loss = 2.16768725\n",
      "Iteration 77, loss = 2.16767152\n",
      "Iteration 78, loss = 2.16766589\n",
      "Iteration 79, loss = 2.16766364\n",
      "Iteration 80, loss = 2.16767112\n",
      "Iteration 81, loss = 2.16766662\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 82, loss = 2.16754799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 83, loss = 2.16754698\n",
      "Iteration 84, loss = 2.16754562\n",
      "Iteration 85, loss = 2.16754313\n",
      "Iteration 86, loss = 2.16754474\n",
      "Iteration 87, loss = 2.16754375\n",
      "Iteration 88, loss = 2.16754561\n",
      "Iteration 89, loss = 2.16754391\n",
      "Iteration 90, loss = 2.16754406\n",
      "Iteration 91, loss = 2.16754278\n",
      "Iteration 92, loss = 2.16754204\n",
      "Iteration 93, loss = 2.16754280\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 2.88312823\n",
      "Iteration 2, loss = 2.22923200\n",
      "Iteration 3, loss = 2.22109897\n",
      "Iteration 4, loss = 2.21750645\n",
      "Iteration 5, loss = 2.22144032\n",
      "Iteration 6, loss = 2.22661584\n",
      "Iteration 7, loss = 2.22287003\n",
      "Iteration 8, loss = 2.22381341\n",
      "Iteration 9, loss = 2.22084056\n",
      "Iteration 10, loss = 2.22412869\n",
      "Iteration 11, loss = 2.22088120\n",
      "Iteration 12, loss = 2.21826704\n",
      "Iteration 13, loss = 2.22044477\n",
      "Iteration 14, loss = 2.22184207\n",
      "Iteration 15, loss = 2.22207571\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 16, loss = 2.17603938\n",
      "Iteration 17, loss = 2.17238436\n",
      "Iteration 18, loss = 2.17114968\n",
      "Iteration 19, loss = 2.17176337\n",
      "Iteration 20, loss = 2.17175833\n",
      "Iteration 21, loss = 2.17223681\n",
      "Iteration 22, loss = 2.17152436\n",
      "Iteration 23, loss = 2.17195240\n",
      "Iteration 24, loss = 2.17262183\n",
      "Iteration 25, loss = 2.17299974\n",
      "Iteration 26, loss = 2.17147536\n",
      "Iteration 27, loss = 2.17188745\n",
      "Iteration 28, loss = 2.17169283\n",
      "Iteration 29, loss = 2.17136157\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 30, loss = 2.16231508\n",
      "Iteration 31, loss = 2.16122085\n",
      "Iteration 32, loss = 2.16083368\n",
      "Iteration 33, loss = 2.16082098\n",
      "Iteration 34, loss = 2.16116307\n",
      "Iteration 35, loss = 2.16060150\n",
      "Iteration 36, loss = 2.16092105\n",
      "Iteration 37, loss = 2.16064294\n",
      "Iteration 38, loss = 2.16061884\n",
      "Iteration 39, loss = 2.16044755\n",
      "Iteration 40, loss = 2.16080325\n",
      "Iteration 41, loss = 2.16091979\n",
      "Iteration 42, loss = 2.16063131\n",
      "Iteration 43, loss = 2.16089477\n",
      "Iteration 44, loss = 2.16073640\n",
      "Iteration 45, loss = 2.16073255\n",
      "Iteration 46, loss = 2.16090626\n",
      "Iteration 47, loss = 2.16035850\n",
      "Iteration 48, loss = 2.16092770\n",
      "Iteration 49, loss = 2.16039587\n",
      "Iteration 50, loss = 2.16085744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 51, loss = 2.15829255\n",
      "Iteration 52, loss = 2.15823852\n",
      "Iteration 53, loss = 2.15828450\n",
      "Iteration 54, loss = 2.15822179\n",
      "Iteration 55, loss = 2.15814571\n",
      "Iteration 56, loss = 2.15822091\n",
      "Iteration 57, loss = 2.15824888\n",
      "Iteration 58, loss = 2.15814708\n",
      "Iteration 59, loss = 2.15808396\n",
      "Iteration 60, loss = 2.15822724\n",
      "Iteration 61, loss = 2.15818125\n",
      "Iteration 62, loss = 2.15816569\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 63, loss = 2.15771134\n",
      "Iteration 64, loss = 2.15764321\n",
      "Iteration 65, loss = 2.15759182\n",
      "Iteration 66, loss = 2.15762953\n",
      "Iteration 67, loss = 2.15762193\n",
      "Iteration 68, loss = 2.15761213\n",
      "Iteration 69, loss = 2.15761830\n",
      "Iteration 70, loss = 2.15762172\n",
      "Iteration 71, loss = 2.15762211\n",
      "Iteration 72, loss = 2.15762080\n",
      "Iteration 73, loss = 2.15763322\n",
      "Iteration 74, loss = 2.15760712\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 75, loss = 2.15749193\n",
      "Iteration 76, loss = 2.15748774\n",
      "Iteration 77, loss = 2.15748936\n",
      "Iteration 78, loss = 2.15748483\n",
      "Iteration 79, loss = 2.15748586\n",
      "Iteration 80, loss = 2.15748568\n",
      "Iteration 81, loss = 2.15748288\n",
      "Iteration 82, loss = 2.15748308\n",
      "Iteration 83, loss = 2.15748199\n",
      "Iteration 84, loss = 2.15748579\n",
      "Iteration 85, loss = 2.15748427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 2.89341773\n",
      "Iteration 2, loss = 2.22018630\n",
      "Iteration 3, loss = 2.21220397\n",
      "Iteration 4, loss = 2.21228790\n",
      "Iteration 5, loss = 2.21144313\n",
      "Iteration 6, loss = 2.21365807\n",
      "Iteration 7, loss = 2.21845031\n",
      "Iteration 8, loss = 2.21681532\n",
      "Iteration 9, loss = 2.20887299\n",
      "Iteration 10, loss = 2.21543718\n",
      "Iteration 11, loss = 2.22075602\n",
      "Iteration 12, loss = 2.21142903\n",
      "Iteration 13, loss = 2.20953326\n",
      "Iteration 14, loss = 2.21293127\n",
      "Iteration 15, loss = 2.21566660\n",
      "Iteration 16, loss = 2.20919612\n",
      "Iteration 17, loss = 2.21123792\n",
      "Iteration 18, loss = 2.21466963\n",
      "Iteration 19, loss = 2.20765850\n",
      "Iteration 20, loss = 2.21075388\n",
      "Iteration 21, loss = 2.21509333\n",
      "Iteration 22, loss = 2.21907372\n",
      "Iteration 23, loss = 2.21916433\n",
      "Iteration 24, loss = 2.21048374\n",
      "Iteration 25, loss = 2.20728705\n",
      "Iteration 26, loss = 2.21609191\n",
      "Iteration 27, loss = 2.21656091\n",
      "Iteration 28, loss = 2.21285787\n",
      "Iteration 29, loss = 2.21102677\n",
      "Iteration 30, loss = 2.21656319\n",
      "Iteration 31, loss = 2.21263690\n",
      "Iteration 32, loss = 2.21739319\n",
      "Iteration 33, loss = 2.21349816\n",
      "Iteration 34, loss = 2.21691821\n",
      "Iteration 35, loss = 2.21044625\n",
      "Iteration 36, loss = 2.21307835\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 37, loss = 2.16885832\n",
      "Iteration 38, loss = 2.16210248\n",
      "Iteration 39, loss = 2.16210420\n",
      "Iteration 40, loss = 2.16220942\n",
      "Iteration 41, loss = 2.16129456\n",
      "Iteration 42, loss = 2.16198170\n",
      "Iteration 43, loss = 2.16175165\n",
      "Iteration 44, loss = 2.16202657\n",
      "Iteration 45, loss = 2.16182591\n",
      "Iteration 46, loss = 2.16093253\n",
      "Iteration 47, loss = 2.16176749\n",
      "Iteration 48, loss = 2.16150370\n",
      "Iteration 49, loss = 2.16149342\n",
      "Iteration 50, loss = 2.16080581\n",
      "Iteration 51, loss = 2.16219570\n",
      "Iteration 52, loss = 2.16200894\n",
      "Iteration 53, loss = 2.16160907\n",
      "Iteration 54, loss = 2.16147924\n",
      "Iteration 55, loss = 2.16172652\n",
      "Iteration 56, loss = 2.16156077\n",
      "Iteration 57, loss = 2.16158145\n",
      "Iteration 58, loss = 2.16227642\n",
      "Iteration 59, loss = 2.16078376\n",
      "Iteration 60, loss = 2.16256774\n",
      "Iteration 61, loss = 2.16199755\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 62, loss = 2.15096946\n",
      "Iteration 63, loss = 2.14980704\n",
      "Iteration 64, loss = 2.14964005\n",
      "Iteration 65, loss = 2.15001789\n",
      "Iteration 66, loss = 2.14984359\n",
      "Iteration 67, loss = 2.14966670\n",
      "Iteration 68, loss = 2.14990022\n",
      "Iteration 69, loss = 2.14982977\n",
      "Iteration 70, loss = 2.14962155\n",
      "Iteration 71, loss = 2.14989861\n",
      "Iteration 72, loss = 2.14993360\n",
      "Iteration 73, loss = 2.14982249\n",
      "Iteration 74, loss = 2.14983831\n",
      "Iteration 75, loss = 2.15043867\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 76, loss = 2.14726057\n",
      "Iteration 77, loss = 2.14721159\n",
      "Iteration 78, loss = 2.14715675\n",
      "Iteration 79, loss = 2.14705293\n",
      "Iteration 80, loss = 2.14718523\n",
      "Iteration 81, loss = 2.14714088\n",
      "Iteration 82, loss = 2.14709717\n",
      "Iteration 83, loss = 2.14717490\n",
      "Iteration 84, loss = 2.14716157\n",
      "Iteration 85, loss = 2.14710942\n",
      "Iteration 86, loss = 2.14707670\n",
      "Iteration 87, loss = 2.14708244\n",
      "Iteration 88, loss = 2.14714698\n",
      "Iteration 89, loss = 2.14717944\n",
      "Iteration 90, loss = 2.14720193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 91, loss = 2.14656619\n",
      "Iteration 92, loss = 2.14651722\n",
      "Iteration 93, loss = 2.14649863\n",
      "Iteration 94, loss = 2.14652087\n",
      "Iteration 95, loss = 2.14650922\n",
      "Iteration 96, loss = 2.14650911\n",
      "Iteration 97, loss = 2.14651451\n",
      "Iteration 98, loss = 2.14651147\n",
      "Iteration 99, loss = 2.14650824\n",
      "Iteration 100, loss = 2.14649347\n",
      "Iteration 101, loss = 2.14652367\n",
      "Iteration 102, loss = 2.14651102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 103, loss = 2.14636004\n",
      "Iteration 104, loss = 2.14635657\n",
      "Iteration 105, loss = 2.14635754\n",
      "Iteration 106, loss = 2.14635678\n",
      "Iteration 107, loss = 2.14635545\n",
      "Iteration 108, loss = 2.14635589\n",
      "Iteration 109, loss = 2.14635614\n",
      "Iteration 110, loss = 2.14635715\n",
      "Iteration 111, loss = 2.14635509\n",
      "Iteration 112, loss = 2.14635585\n",
      "Iteration 113, loss = 2.14635726\n",
      "Iteration 114, loss = 2.14635925\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.90858224\n",
      "Iteration 2, loss = 2.21780212\n",
      "Iteration 3, loss = 2.21393132\n",
      "Iteration 4, loss = 2.21602742\n",
      "Iteration 5, loss = 2.21120231\n",
      "Iteration 6, loss = 2.21295826\n",
      "Iteration 7, loss = 2.21055037\n",
      "Iteration 8, loss = 2.20877289\n",
      "Iteration 9, loss = 2.21892766\n",
      "Iteration 10, loss = 2.21139024\n",
      "Iteration 11, loss = 2.20939388\n",
      "Iteration 12, loss = 2.20612469\n",
      "Iteration 13, loss = 2.20448021\n",
      "Iteration 14, loss = 2.20587869\n",
      "Iteration 15, loss = 2.21211560\n",
      "Iteration 16, loss = 2.21216806\n",
      "Iteration 17, loss = 2.21274565\n",
      "Iteration 18, loss = 2.21083422\n",
      "Iteration 19, loss = 2.20525275\n",
      "Iteration 20, loss = 2.21336439\n",
      "Iteration 21, loss = 2.21094761\n",
      "Iteration 22, loss = 2.21451631\n",
      "Iteration 23, loss = 2.20761061\n",
      "Iteration 24, loss = 2.21351017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 25, loss = 2.15595723\n",
      "Iteration 26, loss = 2.15133331\n",
      "Iteration 27, loss = 2.15278689\n",
      "Iteration 28, loss = 2.15228315\n",
      "Iteration 29, loss = 2.15199754\n",
      "Iteration 30, loss = 2.15257477\n",
      "Iteration 31, loss = 2.15241286\n",
      "Iteration 32, loss = 2.15074967\n",
      "Iteration 33, loss = 2.15346548\n",
      "Iteration 34, loss = 2.15201617\n",
      "Iteration 35, loss = 2.15346011\n",
      "Iteration 36, loss = 2.15258841\n",
      "Iteration 37, loss = 2.15341939\n",
      "Iteration 38, loss = 2.15021371\n",
      "Iteration 39, loss = 2.15259869\n",
      "Iteration 40, loss = 2.15174587\n",
      "Iteration 41, loss = 2.15195603\n",
      "Iteration 42, loss = 2.15339471\n",
      "Iteration 43, loss = 2.15109613\n",
      "Iteration 44, loss = 2.15235133\n",
      "Iteration 45, loss = 2.15039090\n",
      "Iteration 46, loss = 2.15250092\n",
      "Iteration 47, loss = 2.15311328\n",
      "Iteration 48, loss = 2.15377565\n",
      "Iteration 49, loss = 2.15305528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 50, loss = 2.14098095\n",
      "Iteration 51, loss = 2.14013257\n",
      "Iteration 52, loss = 2.13972186\n",
      "Iteration 53, loss = 2.13928432\n",
      "Iteration 54, loss = 2.14013567\n",
      "Iteration 55, loss = 2.13920774\n",
      "Iteration 56, loss = 2.13971095\n",
      "Iteration 57, loss = 2.13994741\n",
      "Iteration 58, loss = 2.13937208\n",
      "Iteration 59, loss = 2.13969089\n",
      "Iteration 60, loss = 2.13947030\n",
      "Iteration 61, loss = 2.13990920\n",
      "Iteration 62, loss = 2.13967342\n",
      "Iteration 63, loss = 2.13992931\n",
      "Iteration 64, loss = 2.13986614\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 65, loss = 2.13671143\n",
      "Iteration 66, loss = 2.13675376\n",
      "Iteration 67, loss = 2.13669567\n",
      "Iteration 68, loss = 2.13654940\n",
      "Iteration 69, loss = 2.13664235\n",
      "Iteration 70, loss = 2.13666101\n",
      "Iteration 71, loss = 2.13671570\n",
      "Iteration 72, loss = 2.13673149\n",
      "Iteration 73, loss = 2.13673675\n",
      "Iteration 74, loss = 2.13665365\n",
      "Iteration 75, loss = 2.13674509\n",
      "Iteration 76, loss = 2.13658240\n",
      "Iteration 77, loss = 2.13675211\n",
      "Iteration 78, loss = 2.13672288\n",
      "Iteration 79, loss = 2.13664316\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 80, loss = 2.13612947\n",
      "Iteration 81, loss = 2.13600285\n",
      "Iteration 82, loss = 2.13601698\n",
      "Iteration 83, loss = 2.13600056\n",
      "Iteration 84, loss = 2.13600127\n",
      "Iteration 85, loss = 2.13600002\n",
      "Iteration 86, loss = 2.13601041\n",
      "Iteration 87, loss = 2.13599038\n",
      "Iteration 88, loss = 2.13599827\n",
      "Iteration 89, loss = 2.13598862\n",
      "Iteration 90, loss = 2.13601256\n",
      "Iteration 91, loss = 2.13600192\n",
      "Iteration 92, loss = 2.13600789\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 93, loss = 2.13584876\n",
      "Iteration 94, loss = 2.13584581\n",
      "Iteration 95, loss = 2.13584552\n",
      "Iteration 96, loss = 2.13584534\n",
      "Iteration 97, loss = 2.13584639\n",
      "Iteration 98, loss = 2.13584696\n",
      "Iteration 99, loss = 2.13584798\n",
      "Iteration 100, loss = 2.13584537\n",
      "Iteration 101, loss = 2.13584819\n",
      "Iteration 102, loss = 2.13584641\n",
      "Iteration 103, loss = 2.13584703\n",
      "Iteration 104, loss = 2.13584987\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 2.97114270\n",
      "Iteration 2, loss = 2.20909936\n",
      "Iteration 3, loss = 2.21433696\n",
      "Iteration 4, loss = 2.21220853\n",
      "Iteration 5, loss = 2.20581820\n",
      "Iteration 6, loss = 2.21020199\n",
      "Iteration 7, loss = 2.20757718\n",
      "Iteration 8, loss = 2.20455490\n",
      "Iteration 9, loss = 2.20495864\n",
      "Iteration 10, loss = 2.21157729\n",
      "Iteration 11, loss = 2.20734496\n",
      "Iteration 12, loss = 2.19894297\n",
      "Iteration 13, loss = 2.20598547\n",
      "Iteration 14, loss = 2.21153579\n",
      "Iteration 15, loss = 2.20538700\n",
      "Iteration 16, loss = 2.20425723\n",
      "Iteration 17, loss = 2.20742814\n",
      "Iteration 18, loss = 2.20442427\n",
      "Iteration 19, loss = 2.20566523\n",
      "Iteration 20, loss = 2.20481144\n",
      "Iteration 21, loss = 2.20879329\n",
      "Iteration 22, loss = 2.20274499\n",
      "Iteration 23, loss = 2.20927530\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 24, loss = 2.15092812\n",
      "Iteration 25, loss = 2.14303564\n",
      "Iteration 26, loss = 2.14364923\n",
      "Iteration 27, loss = 2.14116174\n",
      "Iteration 28, loss = 2.14479896\n",
      "Iteration 29, loss = 2.14348072\n",
      "Iteration 30, loss = 2.14237334\n",
      "Iteration 31, loss = 2.14368253\n",
      "Iteration 32, loss = 2.14394996\n",
      "Iteration 33, loss = 2.14180702\n",
      "Iteration 34, loss = 2.14433462\n",
      "Iteration 35, loss = 2.14359449\n",
      "Iteration 36, loss = 2.14335177\n",
      "Iteration 37, loss = 2.14271271\n",
      "Iteration 38, loss = 2.14331427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 39, loss = 2.13075233\n",
      "Iteration 40, loss = 2.12983786\n",
      "Iteration 41, loss = 2.12884554\n",
      "Iteration 42, loss = 2.12892724\n",
      "Iteration 43, loss = 2.12945969\n",
      "Iteration 44, loss = 2.12911145\n",
      "Iteration 45, loss = 2.12917302\n",
      "Iteration 46, loss = 2.12930323\n",
      "Iteration 47, loss = 2.12895481\n",
      "Iteration 48, loss = 2.12926236\n",
      "Iteration 49, loss = 2.12862582\n",
      "Iteration 50, loss = 2.12921629\n",
      "Iteration 51, loss = 2.12861199\n",
      "Iteration 52, loss = 2.12892545\n",
      "Iteration 53, loss = 2.12871927\n",
      "Iteration 54, loss = 2.12935181\n",
      "Iteration 55, loss = 2.12930817\n",
      "Iteration 56, loss = 2.12934782\n",
      "Iteration 57, loss = 2.12900539\n",
      "Iteration 58, loss = 2.12892462\n",
      "Iteration 59, loss = 2.12894857\n",
      "Iteration 60, loss = 2.12908853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 61, loss = 2.12606779\n",
      "Iteration 62, loss = 2.12581598\n",
      "Iteration 63, loss = 2.12594158\n",
      "Iteration 64, loss = 2.12588872\n",
      "Iteration 65, loss = 2.12583480\n",
      "Iteration 66, loss = 2.12579443\n",
      "Iteration 67, loss = 2.12584157\n",
      "Iteration 68, loss = 2.12584312\n",
      "Iteration 69, loss = 2.12570016\n",
      "Iteration 70, loss = 2.12589099\n",
      "Iteration 71, loss = 2.12585507\n",
      "Iteration 72, loss = 2.12567829\n",
      "Iteration 73, loss = 2.12583982\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 74, loss = 2.12523536\n",
      "Iteration 75, loss = 2.12513070\n",
      "Iteration 76, loss = 2.12510201\n",
      "Iteration 77, loss = 2.12510895\n",
      "Iteration 78, loss = 2.12509689\n",
      "Iteration 79, loss = 2.12510262\n",
      "Iteration 80, loss = 2.12509112\n",
      "Iteration 81, loss = 2.12508348\n",
      "Iteration 82, loss = 2.12509402\n",
      "Iteration 83, loss = 2.12509857\n",
      "Iteration 84, loss = 2.12508764\n",
      "Iteration 85, loss = 2.12508422\n",
      "Iteration 86, loss = 2.12510535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 87, loss = 2.12493345\n",
      "Iteration 88, loss = 2.12492892\n",
      "Iteration 89, loss = 2.12492857\n",
      "Iteration 90, loss = 2.12493003\n",
      "Iteration 91, loss = 2.12492775\n",
      "Iteration 92, loss = 2.12492787\n",
      "Iteration 93, loss = 2.12492714\n",
      "Iteration 94, loss = 2.12492634\n",
      "Iteration 95, loss = 2.12492549\n",
      "Iteration 96, loss = 2.12492259\n",
      "Iteration 97, loss = 2.12492768\n",
      "Iteration 98, loss = 2.12492770\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.01281240\n",
      "Iteration 2, loss = 2.19896886\n",
      "Iteration 3, loss = 2.20060619\n",
      "Iteration 4, loss = 2.19909394\n",
      "Iteration 5, loss = 2.20588691\n",
      "Iteration 6, loss = 2.20109737\n",
      "Iteration 7, loss = 2.20065272\n",
      "Iteration 8, loss = 2.20007918\n",
      "Iteration 9, loss = 2.20661405\n",
      "Iteration 10, loss = 2.20078113\n",
      "Iteration 11, loss = 2.19727838\n",
      "Iteration 12, loss = 2.19910535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 2.20053600\n",
      "Iteration 14, loss = 2.20028318\n",
      "Iteration 15, loss = 2.19626198\n",
      "Iteration 16, loss = 2.20144327\n",
      "Iteration 17, loss = 2.19736639\n",
      "Iteration 18, loss = 2.20345086\n",
      "Iteration 19, loss = 2.19506169\n",
      "Iteration 20, loss = 2.19955686\n",
      "Iteration 21, loss = 2.19888537\n",
      "Iteration 22, loss = 2.19837092\n",
      "Iteration 23, loss = 2.20481303\n",
      "Iteration 24, loss = 2.20326973\n",
      "Iteration 25, loss = 2.19849237\n",
      "Iteration 26, loss = 2.19498867\n",
      "Iteration 27, loss = 2.20330738\n",
      "Iteration 28, loss = 2.19925137\n",
      "Iteration 29, loss = 2.19790335\n",
      "Iteration 30, loss = 2.20098058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 31, loss = 2.14028156\n",
      "Iteration 32, loss = 2.13400681\n",
      "Iteration 33, loss = 2.13278067\n",
      "Iteration 34, loss = 2.13500156\n",
      "Iteration 35, loss = 2.13560911\n",
      "Iteration 36, loss = 2.13498304\n",
      "Iteration 37, loss = 2.13474434\n",
      "Iteration 38, loss = 2.13391268\n",
      "Iteration 39, loss = 2.13317464\n",
      "Iteration 40, loss = 2.13352666\n",
      "Iteration 41, loss = 2.13317524\n",
      "Iteration 42, loss = 2.13385949\n",
      "Iteration 43, loss = 2.13432178\n",
      "Iteration 44, loss = 2.13473471\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 45, loss = 2.12252313\n",
      "Iteration 46, loss = 2.12092318\n",
      "Iteration 47, loss = 2.12039597\n",
      "Iteration 48, loss = 2.12073269\n",
      "Iteration 49, loss = 2.12054940\n",
      "Iteration 50, loss = 2.12065944\n",
      "Iteration 51, loss = 2.12050257\n",
      "Iteration 52, loss = 2.12054109\n",
      "Iteration 53, loss = 2.12067537\n",
      "Iteration 54, loss = 2.12082097\n",
      "Iteration 55, loss = 2.12016861\n",
      "Iteration 56, loss = 2.12074693\n",
      "Iteration 57, loss = 2.11990147\n",
      "Iteration 58, loss = 2.12084127\n",
      "Iteration 59, loss = 2.12035527\n",
      "Iteration 60, loss = 2.12029247\n",
      "Iteration 61, loss = 2.12064543\n",
      "Iteration 62, loss = 2.12071598\n",
      "Iteration 63, loss = 2.12027284\n",
      "Iteration 64, loss = 2.12053566\n",
      "Iteration 65, loss = 2.11990596\n",
      "Iteration 66, loss = 2.12065382\n",
      "Iteration 67, loss = 2.12046635\n",
      "Iteration 68, loss = 2.12065307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 69, loss = 2.11752866\n",
      "Iteration 70, loss = 2.11718347\n",
      "Iteration 71, loss = 2.11731528\n",
      "Iteration 72, loss = 2.11715460\n",
      "Iteration 73, loss = 2.11735536\n",
      "Iteration 74, loss = 2.11728036\n",
      "Iteration 75, loss = 2.11717023\n",
      "Iteration 76, loss = 2.11716778\n",
      "Iteration 77, loss = 2.11733645\n",
      "Iteration 78, loss = 2.11730017\n",
      "Iteration 79, loss = 2.11708156\n",
      "Iteration 80, loss = 2.11724537\n",
      "Iteration 81, loss = 2.11731524\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 82, loss = 2.11652661\n",
      "Iteration 83, loss = 2.11646983\n",
      "Iteration 84, loss = 2.11645857\n",
      "Iteration 85, loss = 2.11647579\n",
      "Iteration 86, loss = 2.11647559\n",
      "Iteration 87, loss = 2.11646183\n",
      "Iteration 88, loss = 2.11644523\n",
      "Iteration 89, loss = 2.11645632\n",
      "Iteration 90, loss = 2.11644721\n",
      "Iteration 91, loss = 2.11646085\n",
      "Iteration 92, loss = 2.11647014\n",
      "Iteration 93, loss = 2.11647603\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 94, loss = 2.11628556\n",
      "Iteration 95, loss = 2.11628678\n",
      "Iteration 96, loss = 2.11628309\n",
      "Iteration 97, loss = 2.11628258\n",
      "Iteration 98, loss = 2.11628278\n",
      "Iteration 99, loss = 2.11628513\n",
      "Iteration 100, loss = 2.11628485\n",
      "Iteration 101, loss = 2.11628123\n",
      "Iteration 102, loss = 2.11628145\n",
      "Iteration 103, loss = 2.11628386\n",
      "Iteration 104, loss = 2.11628609\n",
      "Iteration 105, loss = 2.11628196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.05946132\n",
      "Iteration 2, loss = 2.19570444\n",
      "Iteration 3, loss = 2.19572075\n",
      "Iteration 4, loss = 2.18794129\n",
      "Iteration 5, loss = 2.19588196\n",
      "Iteration 6, loss = 2.19562170\n",
      "Iteration 7, loss = 2.19865757\n",
      "Iteration 8, loss = 2.18795138\n",
      "Iteration 9, loss = 2.19741259\n",
      "Iteration 10, loss = 2.19652891\n",
      "Iteration 11, loss = 2.19481953\n",
      "Iteration 12, loss = 2.19416909\n",
      "Iteration 13, loss = 2.19546870\n",
      "Iteration 14, loss = 2.20096091\n",
      "Iteration 15, loss = 2.19399893\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 16, loss = 2.13311451\n",
      "Iteration 17, loss = 2.12713277\n",
      "Iteration 18, loss = 2.12600558\n",
      "Iteration 19, loss = 2.12730172\n",
      "Iteration 20, loss = 2.12557469\n",
      "Iteration 21, loss = 2.12747664\n",
      "Iteration 22, loss = 2.12561077\n",
      "Iteration 23, loss = 2.12671630\n",
      "Iteration 24, loss = 2.12527788\n",
      "Iteration 25, loss = 2.12686453\n",
      "Iteration 26, loss = 2.12674910\n",
      "Iteration 27, loss = 2.12746351\n",
      "Iteration 28, loss = 2.12710065\n",
      "Iteration 29, loss = 2.12540550\n",
      "Iteration 30, loss = 2.12671637\n",
      "Iteration 31, loss = 2.12735910\n",
      "Iteration 32, loss = 2.12755240\n",
      "Iteration 33, loss = 2.12676402\n",
      "Iteration 34, loss = 2.12751853\n",
      "Iteration 35, loss = 2.12674116\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 36, loss = 2.11332390\n",
      "Iteration 37, loss = 2.11203354\n",
      "Iteration 38, loss = 2.11146923\n",
      "Iteration 39, loss = 2.11161651\n",
      "Iteration 40, loss = 2.11158995\n",
      "Iteration 41, loss = 2.11121917\n",
      "Iteration 42, loss = 2.11206783\n",
      "Iteration 43, loss = 2.11166692\n",
      "Iteration 44, loss = 2.11164697\n",
      "Iteration 45, loss = 2.11186274\n",
      "Iteration 46, loss = 2.11193036\n",
      "Iteration 47, loss = 2.11188197\n",
      "Iteration 48, loss = 2.11176745\n",
      "Iteration 49, loss = 2.11166030\n",
      "Iteration 50, loss = 2.11162454\n",
      "Iteration 51, loss = 2.11145368\n",
      "Iteration 52, loss = 2.11181747\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 53, loss = 2.10834685\n",
      "Iteration 54, loss = 2.10810092\n",
      "Iteration 55, loss = 2.10828509\n",
      "Iteration 56, loss = 2.10819381\n",
      "Iteration 57, loss = 2.10807174\n",
      "Iteration 58, loss = 2.10813924\n",
      "Iteration 59, loss = 2.10817723\n",
      "Iteration 60, loss = 2.10809515\n",
      "Iteration 61, loss = 2.10836264\n",
      "Iteration 62, loss = 2.10814001\n",
      "Iteration 63, loss = 2.10820099\n",
      "Iteration 64, loss = 2.10833349\n",
      "Iteration 65, loss = 2.10823245\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 66, loss = 2.10743668\n",
      "Iteration 67, loss = 2.10740930\n",
      "Iteration 68, loss = 2.10741691\n",
      "Iteration 69, loss = 2.10740942\n",
      "Iteration 70, loss = 2.10741224\n",
      "Iteration 71, loss = 2.10743704\n",
      "Iteration 72, loss = 2.10739724\n",
      "Iteration 73, loss = 2.10741756\n",
      "Iteration 74, loss = 2.10741457\n",
      "Iteration 75, loss = 2.10740177\n",
      "Iteration 76, loss = 2.10740782\n",
      "Iteration 77, loss = 2.10741998\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 78, loss = 2.10723275\n",
      "Iteration 79, loss = 2.10722977\n",
      "Iteration 80, loss = 2.10722503\n",
      "Iteration 81, loss = 2.10722742\n",
      "Iteration 82, loss = 2.10722824\n",
      "Iteration 83, loss = 2.10722552\n",
      "Iteration 84, loss = 2.10722367\n",
      "Iteration 85, loss = 2.10722220\n",
      "Iteration 86, loss = 2.10722460\n",
      "Iteration 87, loss = 2.10722438\n",
      "Iteration 88, loss = 2.10722543\n",
      "Iteration 89, loss = 2.10722168\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.07765544\n",
      "Iteration 2, loss = 2.19526556\n",
      "Iteration 3, loss = 2.19756089\n",
      "Iteration 4, loss = 2.19803767\n",
      "Iteration 5, loss = 2.19210985\n",
      "Iteration 6, loss = 2.18866357\n",
      "Iteration 7, loss = 2.19152061\n",
      "Iteration 8, loss = 2.19000324\n",
      "Iteration 9, loss = 2.19241950\n",
      "Iteration 10, loss = 2.19666507\n",
      "Iteration 11, loss = 2.18339832\n",
      "Iteration 12, loss = 2.18618211\n",
      "Iteration 13, loss = 2.18715511\n",
      "Iteration 14, loss = 2.18801204\n",
      "Iteration 15, loss = 2.18692332\n",
      "Iteration 16, loss = 2.18389301\n",
      "Iteration 17, loss = 2.18967068\n",
      "Iteration 18, loss = 2.18352886\n",
      "Iteration 19, loss = 2.18912329\n",
      "Iteration 20, loss = 2.18780719\n",
      "Iteration 21, loss = 2.18570374\n",
      "Iteration 22, loss = 2.19235416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 23, loss = 2.12359805\n",
      "Iteration 24, loss = 2.11623283\n",
      "Iteration 25, loss = 2.11845074\n",
      "Iteration 26, loss = 2.11640850\n",
      "Iteration 27, loss = 2.11581432\n",
      "Iteration 28, loss = 2.11734643\n",
      "Iteration 29, loss = 2.11788715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 2.11661524\n",
      "Iteration 31, loss = 2.11761353\n",
      "Iteration 32, loss = 2.11826303\n",
      "Iteration 33, loss = 2.11738230\n",
      "Iteration 34, loss = 2.11895822\n",
      "Iteration 35, loss = 2.11723346\n",
      "Iteration 36, loss = 2.11795957\n",
      "Iteration 37, loss = 2.11803467\n",
      "Iteration 38, loss = 2.11676296\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 39, loss = 2.10466624\n",
      "Iteration 40, loss = 2.10206693\n",
      "Iteration 41, loss = 2.10156881\n",
      "Iteration 42, loss = 2.10156734\n",
      "Iteration 43, loss = 2.10213246\n",
      "Iteration 44, loss = 2.10217900\n",
      "Iteration 45, loss = 2.10200809\n",
      "Iteration 46, loss = 2.10214847\n",
      "Iteration 47, loss = 2.10163287\n",
      "Iteration 48, loss = 2.10189370\n",
      "Iteration 49, loss = 2.10193340\n",
      "Iteration 50, loss = 2.10221394\n",
      "Iteration 51, loss = 2.10190846\n",
      "Iteration 52, loss = 2.10158310\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 53, loss = 2.09873226\n",
      "Iteration 54, loss = 2.09838072\n",
      "Iteration 55, loss = 2.09827809\n",
      "Iteration 56, loss = 2.09807996\n",
      "Iteration 57, loss = 2.09834238\n",
      "Iteration 58, loss = 2.09779441\n",
      "Iteration 59, loss = 2.09819666\n",
      "Iteration 60, loss = 2.09829203\n",
      "Iteration 61, loss = 2.09818032\n",
      "Iteration 62, loss = 2.09823772\n",
      "Iteration 63, loss = 2.09816737\n",
      "Iteration 64, loss = 2.09822259\n",
      "Iteration 65, loss = 2.09832428\n",
      "Iteration 66, loss = 2.09814315\n",
      "Iteration 67, loss = 2.09820001\n",
      "Iteration 68, loss = 2.09823743\n",
      "Iteration 69, loss = 2.09806451\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 70, loss = 2.09746868\n",
      "Iteration 71, loss = 2.09736187\n",
      "Iteration 72, loss = 2.09734812\n",
      "Iteration 73, loss = 2.09732236\n",
      "Iteration 74, loss = 2.09735503\n",
      "Iteration 75, loss = 2.09735467\n",
      "Iteration 76, loss = 2.09733965\n",
      "Iteration 77, loss = 2.09736209\n",
      "Iteration 78, loss = 2.09736796\n",
      "Iteration 79, loss = 2.09733567\n",
      "Iteration 80, loss = 2.09732317\n",
      "Iteration 81, loss = 2.09731974\n",
      "Iteration 82, loss = 2.09732454\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 83, loss = 2.09714915\n",
      "Iteration 84, loss = 2.09714726\n",
      "Iteration 85, loss = 2.09714254\n",
      "Iteration 86, loss = 2.09714416\n",
      "Iteration 87, loss = 2.09714112\n",
      "Iteration 88, loss = 2.09714218\n",
      "Iteration 89, loss = 2.09713941\n",
      "Iteration 90, loss = 2.09714003\n",
      "Iteration 91, loss = 2.09713647\n",
      "Iteration 92, loss = 2.09714313\n",
      "Iteration 93, loss = 2.09713978\n",
      "Iteration 94, loss = 2.09713965\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.10777554\n",
      "Iteration 2, loss = 2.19752642\n",
      "Iteration 3, loss = 2.19179514\n",
      "Iteration 4, loss = 2.19468140\n",
      "Iteration 5, loss = 2.19193193\n",
      "Iteration 6, loss = 2.19647702\n",
      "Iteration 7, loss = 2.18504153\n",
      "Iteration 8, loss = 2.18509879\n",
      "Iteration 9, loss = 2.19151145\n",
      "Iteration 10, loss = 2.18845857\n",
      "Iteration 11, loss = 2.18557037\n",
      "Iteration 12, loss = 2.18348136\n",
      "Iteration 13, loss = 2.19041390\n",
      "Iteration 14, loss = 2.18718495\n",
      "Iteration 15, loss = 2.18558155\n",
      "Iteration 16, loss = 2.19144783\n",
      "Iteration 17, loss = 2.18466402\n",
      "Iteration 18, loss = 2.18861006\n",
      "Iteration 19, loss = 2.18253287\n",
      "Iteration 20, loss = 2.18454974\n",
      "Iteration 21, loss = 2.19107644\n",
      "Iteration 22, loss = 2.18422431\n",
      "Iteration 23, loss = 2.18315553\n",
      "Iteration 24, loss = 2.18816421\n",
      "Iteration 25, loss = 2.19049922\n",
      "Iteration 26, loss = 2.18363659\n",
      "Iteration 27, loss = 2.19173103\n",
      "Iteration 28, loss = 2.18579428\n",
      "Iteration 29, loss = 2.18853432\n",
      "Iteration 30, loss = 2.17768418\n",
      "Iteration 31, loss = 2.18185400\n",
      "Iteration 32, loss = 2.18569025\n",
      "Iteration 33, loss = 2.19065054\n",
      "Iteration 34, loss = 2.18741151\n",
      "Iteration 35, loss = 2.18130280\n",
      "Iteration 36, loss = 2.17368642\n",
      "Iteration 37, loss = 2.18759273\n",
      "Iteration 38, loss = 2.18986170\n",
      "Iteration 39, loss = 2.18559417\n",
      "Iteration 40, loss = 2.17641208\n",
      "Iteration 41, loss = 2.18602256\n",
      "Iteration 42, loss = 2.18346998\n",
      "Iteration 43, loss = 2.17865443\n",
      "Iteration 44, loss = 2.18138391\n",
      "Iteration 45, loss = 2.17990686\n",
      "Iteration 46, loss = 2.17615236\n",
      "Iteration 47, loss = 2.18404973\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 48, loss = 2.11807990\n",
      "Iteration 49, loss = 2.11067885\n",
      "Iteration 50, loss = 2.10982367\n",
      "Iteration 51, loss = 2.11121455\n",
      "Iteration 52, loss = 2.11143361\n",
      "Iteration 53, loss = 2.11081802\n",
      "Iteration 54, loss = 2.10974066\n",
      "Iteration 55, loss = 2.11125575\n",
      "Iteration 56, loss = 2.11168564\n",
      "Iteration 57, loss = 2.11004689\n",
      "Iteration 58, loss = 2.11133893\n",
      "Iteration 59, loss = 2.11087164\n",
      "Iteration 60, loss = 2.11138939\n",
      "Iteration 61, loss = 2.11198989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 62, loss = 2.09786935\n",
      "Iteration 63, loss = 2.09561867\n",
      "Iteration 64, loss = 2.09548707\n",
      "Iteration 65, loss = 2.09520424\n",
      "Iteration 66, loss = 2.09526458\n",
      "Iteration 67, loss = 2.09467510\n",
      "Iteration 68, loss = 2.09546413\n",
      "Iteration 69, loss = 2.09541328\n",
      "Iteration 70, loss = 2.09530000\n",
      "Iteration 71, loss = 2.09536410\n",
      "Iteration 72, loss = 2.09559902\n",
      "Iteration 73, loss = 2.09533743\n",
      "Iteration 74, loss = 2.09546799\n",
      "Iteration 75, loss = 2.09549784\n",
      "Iteration 76, loss = 2.09525533\n",
      "Iteration 77, loss = 2.09496956\n",
      "Iteration 78, loss = 2.09534564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 79, loss = 2.09165439\n",
      "Iteration 80, loss = 2.09149969\n",
      "Iteration 81, loss = 2.09155465\n",
      "Iteration 82, loss = 2.09162435\n",
      "Iteration 83, loss = 2.09161388\n",
      "Iteration 84, loss = 2.09166410\n",
      "Iteration 85, loss = 2.09159664\n",
      "Iteration 86, loss = 2.09148410\n",
      "Iteration 87, loss = 2.09148313\n",
      "Iteration 88, loss = 2.09150731\n",
      "Iteration 89, loss = 2.09152433\n",
      "Iteration 90, loss = 2.09153039\n",
      "Iteration 91, loss = 2.09154114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 92, loss = 2.09081199\n",
      "Iteration 93, loss = 2.09068382\n",
      "Iteration 94, loss = 2.09068638\n",
      "Iteration 95, loss = 2.09064772\n",
      "Iteration 96, loss = 2.09067275\n",
      "Iteration 97, loss = 2.09065371\n",
      "Iteration 98, loss = 2.09066854\n",
      "Iteration 99, loss = 2.09067904\n",
      "Iteration 100, loss = 2.09062802\n",
      "Iteration 101, loss = 2.09067896\n",
      "Iteration 102, loss = 2.09066395\n",
      "Iteration 103, loss = 2.09065128\n",
      "Iteration 104, loss = 2.09068453\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 105, loss = 2.09047367\n",
      "Iteration 106, loss = 2.09047131\n",
      "Iteration 107, loss = 2.09046728\n",
      "Iteration 108, loss = 2.09047347\n",
      "Iteration 109, loss = 2.09047060\n",
      "Iteration 110, loss = 2.09046899\n",
      "Iteration 111, loss = 2.09047082\n",
      "Iteration 112, loss = 2.09046953\n",
      "Iteration 113, loss = 2.09047099\n",
      "Iteration 114, loss = 2.09046745\n",
      "Iteration 115, loss = 2.09046869\n",
      "Iteration 116, loss = 2.09046951\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.12200330\n",
      "Iteration 2, loss = 2.18398493\n",
      "Iteration 3, loss = 2.18843922\n",
      "Iteration 4, loss = 2.18075069\n",
      "Iteration 5, loss = 2.18454732\n",
      "Iteration 6, loss = 2.18258459\n",
      "Iteration 7, loss = 2.17889874\n",
      "Iteration 8, loss = 2.17703489\n",
      "Iteration 9, loss = 2.19407713\n",
      "Iteration 10, loss = 2.17541124\n",
      "Iteration 11, loss = 2.18156304\n",
      "Iteration 12, loss = 2.17813461\n",
      "Iteration 13, loss = 2.17760762\n",
      "Iteration 14, loss = 2.17931803\n",
      "Iteration 15, loss = 2.18586198\n",
      "Iteration 16, loss = 2.17276162\n",
      "Iteration 17, loss = 2.18282621\n",
      "Iteration 18, loss = 2.17849135\n",
      "Iteration 19, loss = 2.18132903\n",
      "Iteration 20, loss = 2.17601002\n",
      "Iteration 21, loss = 2.17809970\n",
      "Iteration 22, loss = 2.17950878\n",
      "Iteration 23, loss = 2.17563900\n",
      "Iteration 24, loss = 2.18524915\n",
      "Iteration 25, loss = 2.17724862\n",
      "Iteration 26, loss = 2.17483080\n",
      "Iteration 27, loss = 2.17822235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 28, loss = 2.11643426\n",
      "Iteration 29, loss = 2.10401136\n",
      "Iteration 30, loss = 2.10412584\n",
      "Iteration 31, loss = 2.10431358\n",
      "Iteration 32, loss = 2.10400298\n",
      "Iteration 33, loss = 2.10135424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 2.10211017\n",
      "Iteration 35, loss = 2.10367501\n",
      "Iteration 36, loss = 2.10312420\n",
      "Iteration 37, loss = 2.10424724\n",
      "Iteration 38, loss = 2.10254628\n",
      "Iteration 39, loss = 2.10377015\n",
      "Iteration 40, loss = 2.10421425\n",
      "Iteration 41, loss = 2.10299967\n",
      "Iteration 42, loss = 2.10418484\n",
      "Iteration 43, loss = 2.10254445\n",
      "Iteration 44, loss = 2.10420742\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 45, loss = 2.08802747\n",
      "Iteration 46, loss = 2.08647852\n",
      "Iteration 47, loss = 2.08570889\n",
      "Iteration 48, loss = 2.08598083\n",
      "Iteration 49, loss = 2.08566009\n",
      "Iteration 50, loss = 2.08631309\n",
      "Iteration 51, loss = 2.08580918\n",
      "Iteration 52, loss = 2.08640455\n",
      "Iteration 53, loss = 2.08657142\n",
      "Iteration 54, loss = 2.08614619\n",
      "Iteration 55, loss = 2.08665880\n",
      "Iteration 56, loss = 2.08656995\n",
      "Iteration 57, loss = 2.08602417\n",
      "Iteration 58, loss = 2.08558173\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 59, loss = 2.08294748\n",
      "Iteration 60, loss = 2.08222251\n",
      "Iteration 61, loss = 2.08214482\n",
      "Iteration 62, loss = 2.08241967\n",
      "Iteration 63, loss = 2.08228970\n",
      "Iteration 64, loss = 2.08220765\n",
      "Iteration 65, loss = 2.08224173\n",
      "Iteration 66, loss = 2.08238422\n",
      "Iteration 67, loss = 2.08229663\n",
      "Iteration 68, loss = 2.08210360\n",
      "Iteration 69, loss = 2.08233492\n",
      "Iteration 70, loss = 2.08231203\n",
      "Iteration 71, loss = 2.08232171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 72, loss = 2.08142948\n",
      "Iteration 73, loss = 2.08135287\n",
      "Iteration 74, loss = 2.08133468\n",
      "Iteration 75, loss = 2.08132817\n",
      "Iteration 76, loss = 2.08134095\n",
      "Iteration 77, loss = 2.08135160\n",
      "Iteration 78, loss = 2.08132059\n",
      "Iteration 79, loss = 2.08133440\n",
      "Iteration 80, loss = 2.08132864\n",
      "Iteration 81, loss = 2.08134020\n",
      "Iteration 82, loss = 2.08133798\n",
      "Iteration 83, loss = 2.08132443\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 84, loss = 2.08112442\n",
      "Iteration 85, loss = 2.08112144\n",
      "Iteration 86, loss = 2.08112305\n",
      "Iteration 87, loss = 2.08112292\n",
      "Iteration 88, loss = 2.08112030\n",
      "Iteration 89, loss = 2.08111670\n",
      "Iteration 90, loss = 2.08111938\n",
      "Iteration 91, loss = 2.08111987\n",
      "Iteration 92, loss = 2.08111588\n",
      "Iteration 93, loss = 2.08111239\n",
      "Iteration 94, loss = 2.08111427\n",
      "Iteration 95, loss = 2.08111520\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.15503902\n",
      "Iteration 2, loss = 2.19495982\n",
      "Iteration 3, loss = 2.18041535\n",
      "Iteration 4, loss = 2.18279461\n",
      "Iteration 5, loss = 2.18228301\n",
      "Iteration 6, loss = 2.17806128\n",
      "Iteration 7, loss = 2.17297652\n",
      "Iteration 8, loss = 2.17745145\n",
      "Iteration 9, loss = 2.17734633\n",
      "Iteration 10, loss = 2.17062222\n",
      "Iteration 11, loss = 2.16681804\n",
      "Iteration 12, loss = 2.17851201\n",
      "Iteration 13, loss = 2.17382359\n",
      "Iteration 14, loss = 2.17469819\n",
      "Iteration 15, loss = 2.17081069\n",
      "Iteration 16, loss = 2.17570280\n",
      "Iteration 17, loss = 2.17502926\n",
      "Iteration 18, loss = 2.16971205\n",
      "Iteration 19, loss = 2.17862145\n",
      "Iteration 20, loss = 2.16967708\n",
      "Iteration 21, loss = 2.17301618\n",
      "Iteration 22, loss = 2.17226609\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 23, loss = 2.10108634\n",
      "Iteration 24, loss = 2.09580294\n",
      "Iteration 25, loss = 2.09622997\n",
      "Iteration 26, loss = 2.09570188\n",
      "Iteration 27, loss = 2.09583704\n",
      "Iteration 28, loss = 2.09736333\n",
      "Iteration 29, loss = 2.09438659\n",
      "Iteration 30, loss = 2.09505160\n",
      "Iteration 31, loss = 2.09549951\n",
      "Iteration 32, loss = 2.09420039\n",
      "Iteration 33, loss = 2.09513240\n",
      "Iteration 34, loss = 2.09322151\n",
      "Iteration 35, loss = 2.09666345\n",
      "Iteration 36, loss = 2.09588568\n",
      "Iteration 37, loss = 2.09296548\n",
      "Iteration 38, loss = 2.09512631\n",
      "Iteration 39, loss = 2.09350774\n",
      "Iteration 40, loss = 2.09534963\n",
      "Iteration 41, loss = 2.09568586\n",
      "Iteration 42, loss = 2.09488797\n",
      "Iteration 43, loss = 2.09555971\n",
      "Iteration 44, loss = 2.09559803\n",
      "Iteration 45, loss = 2.09270145\n",
      "Iteration 46, loss = 2.09505541\n",
      "Iteration 47, loss = 2.09599232\n",
      "Iteration 48, loss = 2.09425660\n",
      "Iteration 49, loss = 2.09465024\n",
      "Iteration 50, loss = 2.09450542\n",
      "Iteration 51, loss = 2.09500148\n",
      "Iteration 52, loss = 2.09644234\n",
      "Iteration 53, loss = 2.09385982\n",
      "Iteration 54, loss = 2.09476329\n",
      "Iteration 55, loss = 2.09667137\n",
      "Iteration 56, loss = 2.09509302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 57, loss = 2.08077482\n",
      "Iteration 58, loss = 2.07752178\n",
      "Iteration 59, loss = 2.07780780\n",
      "Iteration 60, loss = 2.07741443\n",
      "Iteration 61, loss = 2.07706765\n",
      "Iteration 62, loss = 2.07666344\n",
      "Iteration 63, loss = 2.07743472\n",
      "Iteration 64, loss = 2.07764373\n",
      "Iteration 65, loss = 2.07763924\n",
      "Iteration 66, loss = 2.07771989\n",
      "Iteration 67, loss = 2.07711008\n",
      "Iteration 68, loss = 2.07758293\n",
      "Iteration 69, loss = 2.07696054\n",
      "Iteration 70, loss = 2.07775320\n",
      "Iteration 71, loss = 2.07731279\n",
      "Iteration 72, loss = 2.07717073\n",
      "Iteration 73, loss = 2.07722055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 74, loss = 2.07352901\n",
      "Iteration 75, loss = 2.07318452\n",
      "Iteration 76, loss = 2.07293210\n",
      "Iteration 77, loss = 2.07303765\n",
      "Iteration 78, loss = 2.07309798\n",
      "Iteration 79, loss = 2.07334258\n",
      "Iteration 80, loss = 2.07287254\n",
      "Iteration 81, loss = 2.07329259\n",
      "Iteration 82, loss = 2.07319994\n",
      "Iteration 83, loss = 2.07316464\n",
      "Iteration 84, loss = 2.07320197\n",
      "Iteration 85, loss = 2.07321408\n",
      "Iteration 86, loss = 2.07302803\n",
      "Iteration 87, loss = 2.07310434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 88, loss = 2.07251941\n",
      "Iteration 89, loss = 2.07223187\n",
      "Iteration 90, loss = 2.07217677\n",
      "Iteration 91, loss = 2.07216104\n",
      "Iteration 92, loss = 2.07210594\n",
      "Iteration 93, loss = 2.07213893\n",
      "Iteration 94, loss = 2.07214995\n",
      "Iteration 95, loss = 2.07213557\n",
      "Iteration 96, loss = 2.07213667\n",
      "Iteration 97, loss = 2.07213360\n",
      "Iteration 98, loss = 2.07215061\n",
      "Iteration 99, loss = 2.07214649\n",
      "Iteration 100, loss = 2.07216529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 101, loss = 2.07191777\n",
      "Iteration 102, loss = 2.07191839\n",
      "Iteration 103, loss = 2.07191448\n",
      "Iteration 104, loss = 2.07191657\n",
      "Iteration 105, loss = 2.07191276\n",
      "Iteration 106, loss = 2.07191551\n",
      "Iteration 107, loss = 2.07191613\n",
      "Iteration 108, loss = 2.07191570\n",
      "Iteration 109, loss = 2.07191661\n",
      "Iteration 110, loss = 2.07191421\n",
      "Iteration 111, loss = 2.07191198\n",
      "Iteration 112, loss = 2.07191250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.23956779\n",
      "Iteration 2, loss = 2.17674289\n",
      "Iteration 3, loss = 2.17173985\n",
      "Iteration 4, loss = 2.16395368\n",
      "Iteration 5, loss = 2.17741269\n",
      "Iteration 6, loss = 2.17142833\n",
      "Iteration 7, loss = 2.16459705\n",
      "Iteration 8, loss = 2.16623516\n",
      "Iteration 9, loss = 2.17428494\n",
      "Iteration 10, loss = 2.16908523\n",
      "Iteration 11, loss = 2.17118214\n",
      "Iteration 12, loss = 2.16731929\n",
      "Iteration 13, loss = 2.16752665\n",
      "Iteration 14, loss = 2.16601584\n",
      "Iteration 15, loss = 2.17258513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 16, loss = 2.09704876\n",
      "Iteration 17, loss = 2.08985817\n",
      "Iteration 18, loss = 2.08925966\n",
      "Iteration 19, loss = 2.08918132\n",
      "Iteration 20, loss = 2.08769719\n",
      "Iteration 21, loss = 2.08950764\n",
      "Iteration 22, loss = 2.08947602\n",
      "Iteration 23, loss = 2.08974111\n",
      "Iteration 24, loss = 2.08962002\n",
      "Iteration 25, loss = 2.08866359\n",
      "Iteration 26, loss = 2.09023479\n",
      "Iteration 27, loss = 2.08879515\n",
      "Iteration 28, loss = 2.08952983\n",
      "Iteration 29, loss = 2.08828031\n",
      "Iteration 30, loss = 2.09025544\n",
      "Iteration 31, loss = 2.09096029\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 32, loss = 2.07340204\n",
      "Iteration 33, loss = 2.07142558\n",
      "Iteration 34, loss = 2.07188828\n",
      "Iteration 35, loss = 2.07183156\n",
      "Iteration 36, loss = 2.07160602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 2.07174185\n",
      "Iteration 38, loss = 2.07114013\n",
      "Iteration 39, loss = 2.07155447\n",
      "Iteration 40, loss = 2.07117314\n",
      "Iteration 41, loss = 2.07107339\n",
      "Iteration 42, loss = 2.07138501\n",
      "Iteration 43, loss = 2.07100765\n",
      "Iteration 44, loss = 2.07160805\n",
      "Iteration 45, loss = 2.07080838\n",
      "Iteration 46, loss = 2.07143028\n",
      "Iteration 47, loss = 2.07089451\n",
      "Iteration 48, loss = 2.07097149\n",
      "Iteration 49, loss = 2.07078808\n",
      "Iteration 50, loss = 2.07126216\n",
      "Iteration 51, loss = 2.07136330\n",
      "Iteration 52, loss = 2.07075757\n",
      "Iteration 53, loss = 2.07123540\n",
      "Iteration 54, loss = 2.07116640\n",
      "Iteration 55, loss = 2.07144495\n",
      "Iteration 56, loss = 2.07135027\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 57, loss = 2.06766430\n",
      "Iteration 58, loss = 2.06724906\n",
      "Iteration 59, loss = 2.06732489\n",
      "Iteration 60, loss = 2.06709766\n",
      "Iteration 61, loss = 2.06700718\n",
      "Iteration 62, loss = 2.06718529\n",
      "Iteration 63, loss = 2.06695686\n",
      "Iteration 64, loss = 2.06730825\n",
      "Iteration 65, loss = 2.06714220\n",
      "Iteration 66, loss = 2.06716449\n",
      "Iteration 67, loss = 2.06731169\n",
      "Iteration 68, loss = 2.06716956\n",
      "Iteration 69, loss = 2.06700593\n",
      "Iteration 70, loss = 2.06727909\n",
      "Iteration 71, loss = 2.06711362\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 72, loss = 2.06629529\n",
      "Iteration 73, loss = 2.06617472\n",
      "Iteration 74, loss = 2.06615080\n",
      "Iteration 75, loss = 2.06615324\n",
      "Iteration 76, loss = 2.06612452\n",
      "Iteration 77, loss = 2.06615505\n",
      "Iteration 78, loss = 2.06612820\n",
      "Iteration 79, loss = 2.06615744\n",
      "Iteration 80, loss = 2.06615221\n",
      "Iteration 81, loss = 2.06615788\n",
      "Iteration 82, loss = 2.06614150\n",
      "Iteration 83, loss = 2.06613199\n",
      "Iteration 84, loss = 2.06616028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 85, loss = 2.06591977\n",
      "Iteration 86, loss = 2.06591685\n",
      "Iteration 87, loss = 2.06591416\n",
      "Iteration 88, loss = 2.06591612\n",
      "Iteration 89, loss = 2.06591395\n",
      "Iteration 90, loss = 2.06591740\n",
      "Iteration 91, loss = 2.06591686\n",
      "Iteration 92, loss = 2.06591329\n",
      "Iteration 93, loss = 2.06591331\n",
      "Iteration 94, loss = 2.06591771\n",
      "Iteration 95, loss = 2.06591347\n",
      "Iteration 96, loss = 2.06591375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.27565603\n",
      "Iteration 2, loss = 2.18106652\n",
      "Iteration 3, loss = 2.16852228\n",
      "Iteration 4, loss = 2.16588834\n",
      "Iteration 5, loss = 2.17085171\n",
      "Iteration 6, loss = 2.16968715\n",
      "Iteration 7, loss = 2.16914831\n",
      "Iteration 8, loss = 2.16596463\n",
      "Iteration 9, loss = 2.17066166\n",
      "Iteration 10, loss = 2.16855108\n",
      "Iteration 11, loss = 2.17442333\n",
      "Iteration 12, loss = 2.16780750\n",
      "Iteration 13, loss = 2.17135813\n",
      "Iteration 14, loss = 2.16749446\n",
      "Iteration 15, loss = 2.17065372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 16, loss = 2.09102895\n",
      "Iteration 17, loss = 2.08236334\n",
      "Iteration 18, loss = 2.08349734\n",
      "Iteration 19, loss = 2.08378390\n",
      "Iteration 20, loss = 2.08262770\n",
      "Iteration 21, loss = 2.08179024\n",
      "Iteration 22, loss = 2.08186172\n",
      "Iteration 23, loss = 2.08177430\n",
      "Iteration 24, loss = 2.08044603\n",
      "Iteration 25, loss = 2.08240429\n",
      "Iteration 26, loss = 2.08251270\n",
      "Iteration 27, loss = 2.08358058\n",
      "Iteration 28, loss = 2.08382225\n",
      "Iteration 29, loss = 2.08341195\n",
      "Iteration 30, loss = 2.08106047\n",
      "Iteration 31, loss = 2.08262542\n",
      "Iteration 32, loss = 2.08191734\n",
      "Iteration 33, loss = 2.08227556\n",
      "Iteration 34, loss = 2.08196397\n",
      "Iteration 35, loss = 2.08290938\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 36, loss = 2.06551235\n",
      "Iteration 37, loss = 2.06352603\n",
      "Iteration 38, loss = 2.06366752\n",
      "Iteration 39, loss = 2.06273001\n",
      "Iteration 40, loss = 2.06243370\n",
      "Iteration 41, loss = 2.06312786\n",
      "Iteration 42, loss = 2.06276833\n",
      "Iteration 43, loss = 2.06275962\n",
      "Iteration 44, loss = 2.06297994\n",
      "Iteration 45, loss = 2.06312937\n",
      "Iteration 46, loss = 2.06313458\n",
      "Iteration 47, loss = 2.06268849\n",
      "Iteration 48, loss = 2.06329171\n",
      "Iteration 49, loss = 2.06289859\n",
      "Iteration 50, loss = 2.06221518\n",
      "Iteration 51, loss = 2.06254880\n",
      "Iteration 52, loss = 2.06246281\n",
      "Iteration 53, loss = 2.06267186\n",
      "Iteration 54, loss = 2.06301623\n",
      "Iteration 55, loss = 2.06267873\n",
      "Iteration 56, loss = 2.06306781\n",
      "Iteration 57, loss = 2.06325163\n",
      "Iteration 58, loss = 2.06245209\n",
      "Iteration 59, loss = 2.06228138\n",
      "Iteration 60, loss = 2.06278940\n",
      "Iteration 61, loss = 2.06223458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 62, loss = 2.05915586\n",
      "Iteration 63, loss = 2.05858083\n",
      "Iteration 64, loss = 2.05858641\n",
      "Iteration 65, loss = 2.05831647\n",
      "Iteration 66, loss = 2.05824932\n",
      "Iteration 67, loss = 2.05847747\n",
      "Iteration 68, loss = 2.05807199\n",
      "Iteration 69, loss = 2.05845276\n",
      "Iteration 70, loss = 2.05839588\n",
      "Iteration 71, loss = 2.05810737\n",
      "Iteration 72, loss = 2.05835744\n",
      "Iteration 73, loss = 2.05819688\n",
      "Iteration 74, loss = 2.05823203\n",
      "Iteration 75, loss = 2.05841580\n",
      "Iteration 76, loss = 2.05829992\n",
      "Iteration 77, loss = 2.05847584\n",
      "Iteration 78, loss = 2.05835086\n",
      "Iteration 79, loss = 2.05830954\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 80, loss = 2.05733164\n",
      "Iteration 81, loss = 2.05732093\n",
      "Iteration 82, loss = 2.05730710\n",
      "Iteration 83, loss = 2.05729277\n",
      "Iteration 84, loss = 2.05731567\n",
      "Iteration 85, loss = 2.05727931\n",
      "Iteration 86, loss = 2.05729927\n",
      "Iteration 87, loss = 2.05728943\n",
      "Iteration 88, loss = 2.05730666\n",
      "Iteration 89, loss = 2.05732969\n",
      "Iteration 90, loss = 2.05728801\n",
      "Iteration 91, loss = 2.05730415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 92, loss = 2.05706347\n",
      "Iteration 93, loss = 2.05706093\n",
      "Iteration 94, loss = 2.05706119\n",
      "Iteration 95, loss = 2.05706289\n",
      "Iteration 96, loss = 2.05706136\n",
      "Iteration 97, loss = 2.05705845\n",
      "Iteration 98, loss = 2.05705962\n",
      "Iteration 99, loss = 2.05705976\n",
      "Iteration 100, loss = 2.05705615\n",
      "Iteration 101, loss = 2.05705942\n",
      "Iteration 102, loss = 2.05706229\n",
      "Iteration 103, loss = 2.05705743\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.24826055\n",
      "Iteration 2, loss = 2.17442429\n",
      "Iteration 3, loss = 2.17321662\n",
      "Iteration 4, loss = 2.16720416\n",
      "Iteration 5, loss = 2.16330495\n",
      "Iteration 6, loss = 2.15922796\n",
      "Iteration 7, loss = 2.16033964\n",
      "Iteration 8, loss = 2.16579506\n",
      "Iteration 9, loss = 2.15728924\n",
      "Iteration 10, loss = 2.16406733\n",
      "Iteration 11, loss = 2.16361813\n",
      "Iteration 12, loss = 2.16276339\n",
      "Iteration 13, loss = 2.16198075\n",
      "Iteration 14, loss = 2.16295062\n",
      "Iteration 15, loss = 2.15553686\n",
      "Iteration 16, loss = 2.15621792\n",
      "Iteration 17, loss = 2.15852712\n",
      "Iteration 18, loss = 2.16719658\n",
      "Iteration 19, loss = 2.16090377\n",
      "Iteration 20, loss = 2.16520014\n",
      "Iteration 21, loss = 2.16427523\n",
      "Iteration 22, loss = 2.16845219\n",
      "Iteration 23, loss = 2.15937141\n",
      "Iteration 24, loss = 2.15893295\n",
      "Iteration 25, loss = 2.17147957\n",
      "Iteration 26, loss = 2.16798366\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 27, loss = 2.08547398\n",
      "Iteration 28, loss = 2.07774304\n",
      "Iteration 29, loss = 2.07788464\n",
      "Iteration 30, loss = 2.07476534\n",
      "Iteration 31, loss = 2.07640665\n",
      "Iteration 32, loss = 2.07611279\n",
      "Iteration 33, loss = 2.07548438\n",
      "Iteration 34, loss = 2.07535988\n",
      "Iteration 35, loss = 2.07465717\n",
      "Iteration 36, loss = 2.07699046\n",
      "Iteration 37, loss = 2.07441854\n",
      "Iteration 38, loss = 2.07534095\n",
      "Iteration 39, loss = 2.07530731\n",
      "Iteration 40, loss = 2.07636802\n",
      "Iteration 41, loss = 2.07581039\n",
      "Iteration 42, loss = 2.07603498\n",
      "Iteration 43, loss = 2.07701457\n",
      "Iteration 44, loss = 2.07634110\n",
      "Iteration 45, loss = 2.07820084\n",
      "Iteration 46, loss = 2.07555757\n",
      "Iteration 47, loss = 2.07429769\n",
      "Iteration 48, loss = 2.07403877\n",
      "Iteration 49, loss = 2.07572614\n",
      "Iteration 50, loss = 2.07667737\n",
      "Iteration 51, loss = 2.07570604\n",
      "Iteration 52, loss = 2.07507073\n",
      "Iteration 53, loss = 2.07481447\n",
      "Iteration 54, loss = 2.07633282\n",
      "Iteration 55, loss = 2.07501901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56, loss = 2.07038611\n",
      "Iteration 57, loss = 2.07599168\n",
      "Iteration 58, loss = 2.07622127\n",
      "Iteration 59, loss = 2.07557907\n",
      "Iteration 60, loss = 2.07652226\n",
      "Iteration 61, loss = 2.07466633\n",
      "Iteration 62, loss = 2.07445567\n",
      "Iteration 63, loss = 2.07474521\n",
      "Iteration 64, loss = 2.07663396\n",
      "Iteration 65, loss = 2.07585698\n",
      "Iteration 66, loss = 2.07647826\n",
      "Iteration 67, loss = 2.07442173\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 68, loss = 2.06007583\n",
      "Iteration 69, loss = 2.05787457\n",
      "Iteration 70, loss = 2.05686277\n",
      "Iteration 71, loss = 2.05711369\n",
      "Iteration 72, loss = 2.05659310\n",
      "Iteration 73, loss = 2.05654125\n",
      "Iteration 74, loss = 2.05724363\n",
      "Iteration 75, loss = 2.05703675\n",
      "Iteration 76, loss = 2.05659553\n",
      "Iteration 77, loss = 2.05685808\n",
      "Iteration 78, loss = 2.05632679\n",
      "Iteration 79, loss = 2.05697476\n",
      "Iteration 80, loss = 2.05562242\n",
      "Iteration 81, loss = 2.05596920\n",
      "Iteration 82, loss = 2.05718662\n",
      "Iteration 83, loss = 2.05644994\n",
      "Iteration 84, loss = 2.05678069\n",
      "Iteration 85, loss = 2.05667181\n",
      "Iteration 86, loss = 2.05668534\n",
      "Iteration 87, loss = 2.05679444\n",
      "Iteration 88, loss = 2.05694034\n",
      "Iteration 89, loss = 2.05681485\n",
      "Iteration 90, loss = 2.05626488\n",
      "Iteration 91, loss = 2.05758710\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 92, loss = 2.05254925\n",
      "Iteration 93, loss = 2.05229735\n",
      "Iteration 94, loss = 2.05236269\n",
      "Iteration 95, loss = 2.05229617\n",
      "Iteration 96, loss = 2.05217179\n",
      "Iteration 97, loss = 2.05222975\n",
      "Iteration 98, loss = 2.05211720\n",
      "Iteration 99, loss = 2.05238122\n",
      "Iteration 100, loss = 2.05225689\n",
      "Iteration 101, loss = 2.05228710\n",
      "Iteration 102, loss = 2.05215470\n",
      "Iteration 103, loss = 2.05216752\n",
      "Iteration 104, loss = 2.05233629\n",
      "Iteration 105, loss = 2.05211651\n",
      "Iteration 106, loss = 2.05208260\n",
      "Iteration 107, loss = 2.05247408\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 108, loss = 2.05133221\n",
      "Iteration 109, loss = 2.05118667\n",
      "Iteration 110, loss = 2.05119792\n",
      "Iteration 111, loss = 2.05119684\n",
      "Iteration 112, loss = 2.05119935\n",
      "Iteration 113, loss = 2.05118001\n",
      "Iteration 114, loss = 2.05116322\n",
      "Iteration 115, loss = 2.05118411\n",
      "Iteration 116, loss = 2.05117252\n",
      "Iteration 117, loss = 2.05117885\n",
      "Iteration 118, loss = 2.05116296\n",
      "Iteration 119, loss = 2.05117563\n",
      "Iteration 120, loss = 2.05117183\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 121, loss = 2.05093955\n",
      "Iteration 122, loss = 2.05093836\n",
      "Iteration 123, loss = 2.05093795\n",
      "Iteration 124, loss = 2.05094175\n",
      "Iteration 125, loss = 2.05093846\n",
      "Iteration 126, loss = 2.05094031\n",
      "Iteration 127, loss = 2.05093523\n",
      "Iteration 128, loss = 2.05093775\n",
      "Iteration 129, loss = 2.05093620\n",
      "Iteration 130, loss = 2.05094064\n",
      "Iteration 131, loss = 2.05094341\n",
      "Iteration 132, loss = 2.05093969\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.23967269\n",
      "Iteration 2, loss = 2.16014235\n",
      "Iteration 3, loss = 2.15866792\n",
      "Iteration 4, loss = 2.16700891\n",
      "Iteration 5, loss = 2.15744149\n",
      "Iteration 6, loss = 2.15765669\n",
      "Iteration 7, loss = 2.17052745\n",
      "Iteration 8, loss = 2.16366809\n",
      "Iteration 9, loss = 2.16302165\n",
      "Iteration 10, loss = 2.16354973\n",
      "Iteration 11, loss = 2.15790183\n",
      "Iteration 12, loss = 2.16105235\n",
      "Iteration 13, loss = 2.16565824\n",
      "Iteration 14, loss = 2.15983392\n",
      "Iteration 15, loss = 2.15430947\n",
      "Iteration 16, loss = 2.15675546\n",
      "Iteration 17, loss = 2.16634114\n",
      "Iteration 18, loss = 2.16141506\n",
      "Iteration 19, loss = 2.16147109\n",
      "Iteration 20, loss = 2.15774141\n",
      "Iteration 21, loss = 2.15906879\n",
      "Iteration 22, loss = 2.15476154\n",
      "Iteration 23, loss = 2.15931955\n",
      "Iteration 24, loss = 2.16621253\n",
      "Iteration 25, loss = 2.16607762\n",
      "Iteration 26, loss = 2.15334967\n",
      "Iteration 27, loss = 2.15633620\n",
      "Iteration 28, loss = 2.15483521\n",
      "Iteration 29, loss = 2.15708321\n",
      "Iteration 30, loss = 2.15872281\n",
      "Iteration 31, loss = 2.15306532\n",
      "Iteration 32, loss = 2.15030920\n",
      "Iteration 33, loss = 2.16350957\n",
      "Iteration 34, loss = 2.16641720\n",
      "Iteration 35, loss = 2.16542571\n",
      "Iteration 36, loss = 2.16000342\n",
      "Iteration 37, loss = 2.15580048\n",
      "Iteration 38, loss = 2.16127225\n",
      "Iteration 39, loss = 2.16097881\n",
      "Iteration 40, loss = 2.15687533\n",
      "Iteration 41, loss = 2.15089247\n",
      "Iteration 42, loss = 2.16368398\n",
      "Iteration 43, loss = 2.15384544\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 44, loss = 2.07417676\n",
      "Iteration 45, loss = 2.07044121\n",
      "Iteration 46, loss = 2.06919060\n",
      "Iteration 47, loss = 2.07004794\n",
      "Iteration 48, loss = 2.06993970\n",
      "Iteration 49, loss = 2.06956818\n",
      "Iteration 50, loss = 2.06919029\n",
      "Iteration 51, loss = 2.06829242\n",
      "Iteration 52, loss = 2.06979205\n",
      "Iteration 53, loss = 2.07017642\n",
      "Iteration 54, loss = 2.06955560\n",
      "Iteration 55, loss = 2.06772622\n",
      "Iteration 56, loss = 2.06807678\n",
      "Iteration 57, loss = 2.07178821\n",
      "Iteration 58, loss = 2.06949980\n",
      "Iteration 59, loss = 2.07039975\n",
      "Iteration 60, loss = 2.06859380\n",
      "Iteration 61, loss = 2.06569627\n",
      "Iteration 62, loss = 2.07282678\n",
      "Iteration 63, loss = 2.06781510\n",
      "Iteration 64, loss = 2.07054673\n",
      "Iteration 65, loss = 2.06956978\n",
      "Iteration 66, loss = 2.06859383\n",
      "Iteration 67, loss = 2.06863650\n",
      "Iteration 68, loss = 2.06804410\n",
      "Iteration 69, loss = 2.06947386\n",
      "Iteration 70, loss = 2.06974432\n",
      "Iteration 71, loss = 2.06960599\n",
      "Iteration 72, loss = 2.06828492\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 73, loss = 2.05202018\n",
      "Iteration 74, loss = 2.05028324\n",
      "Iteration 75, loss = 2.04990786\n",
      "Iteration 76, loss = 2.04968866\n",
      "Iteration 77, loss = 2.04937960\n",
      "Iteration 78, loss = 2.05004082\n",
      "Iteration 79, loss = 2.05031950\n",
      "Iteration 80, loss = 2.04934357\n",
      "Iteration 81, loss = 2.04975254\n",
      "Iteration 82, loss = 2.05026042\n",
      "Iteration 83, loss = 2.05016464\n",
      "Iteration 84, loss = 2.04982393\n",
      "Iteration 85, loss = 2.04955521\n",
      "Iteration 86, loss = 2.05002154\n",
      "Iteration 87, loss = 2.04998218\n",
      "Iteration 88, loss = 2.04923840\n",
      "Iteration 89, loss = 2.04959249\n",
      "Iteration 90, loss = 2.04973795\n",
      "Iteration 91, loss = 2.05023767\n",
      "Iteration 92, loss = 2.04970238\n",
      "Iteration 93, loss = 2.04967017\n",
      "Iteration 94, loss = 2.05008618\n",
      "Iteration 95, loss = 2.04935424\n",
      "Iteration 96, loss = 2.05045294\n",
      "Iteration 97, loss = 2.04983947\n",
      "Iteration 98, loss = 2.05006406\n",
      "Iteration 99, loss = 2.04954137\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 100, loss = 2.04579078\n",
      "Iteration 101, loss = 2.04505178\n",
      "Iteration 102, loss = 2.04549925\n",
      "Iteration 103, loss = 2.04531461\n",
      "Iteration 104, loss = 2.04538710\n",
      "Iteration 105, loss = 2.04534184\n",
      "Iteration 106, loss = 2.04528176\n",
      "Iteration 107, loss = 2.04516323\n",
      "Iteration 108, loss = 2.04541656\n",
      "Iteration 109, loss = 2.04544217\n",
      "Iteration 110, loss = 2.04518664\n",
      "Iteration 111, loss = 2.04532344\n",
      "Iteration 112, loss = 2.04523154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 113, loss = 2.04437428\n",
      "Iteration 114, loss = 2.04423826\n",
      "Iteration 115, loss = 2.04418739\n",
      "Iteration 116, loss = 2.04419480\n",
      "Iteration 117, loss = 2.04419541\n",
      "Iteration 118, loss = 2.04421075\n",
      "Iteration 119, loss = 2.04421770\n",
      "Iteration 120, loss = 2.04419219\n",
      "Iteration 121, loss = 2.04419690\n",
      "Iteration 122, loss = 2.04419184\n",
      "Iteration 123, loss = 2.04420159\n",
      "Iteration 124, loss = 2.04419888\n",
      "Iteration 125, loss = 2.04419974\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 126, loss = 2.04395398\n",
      "Iteration 127, loss = 2.04394688\n",
      "Iteration 128, loss = 2.04395058\n",
      "Iteration 129, loss = 2.04395269\n",
      "Iteration 130, loss = 2.04394929\n",
      "Iteration 131, loss = 2.04394908\n",
      "Iteration 132, loss = 2.04395073\n",
      "Iteration 133, loss = 2.04394843\n",
      "Iteration 134, loss = 2.04394659\n",
      "Iteration 135, loss = 2.04394924\n",
      "Iteration 136, loss = 2.04394878\n",
      "Iteration 137, loss = 2.04394928\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.30323754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 2.16341619\n",
      "Iteration 3, loss = 2.16390262\n",
      "Iteration 4, loss = 2.15214275\n",
      "Iteration 5, loss = 2.15231347\n",
      "Iteration 6, loss = 2.15366593\n",
      "Iteration 7, loss = 2.14450825\n",
      "Iteration 8, loss = 2.16142114\n",
      "Iteration 9, loss = 2.15921671\n",
      "Iteration 10, loss = 2.15451569\n",
      "Iteration 11, loss = 2.15648787\n",
      "Iteration 12, loss = 2.16157777\n",
      "Iteration 13, loss = 2.15031752\n",
      "Iteration 14, loss = 2.16187545\n",
      "Iteration 15, loss = 2.16064478\n",
      "Iteration 16, loss = 2.15495560\n",
      "Iteration 17, loss = 2.15409107\n",
      "Iteration 18, loss = 2.15765279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 19, loss = 2.07598855\n",
      "Iteration 20, loss = 2.06591012\n",
      "Iteration 21, loss = 2.06400555\n",
      "Iteration 22, loss = 2.06494369\n",
      "Iteration 23, loss = 2.06212142\n",
      "Iteration 24, loss = 2.06501611\n",
      "Iteration 25, loss = 2.06505309\n",
      "Iteration 26, loss = 2.06589916\n",
      "Iteration 27, loss = 2.06561181\n",
      "Iteration 28, loss = 2.06306301\n",
      "Iteration 29, loss = 2.06622024\n",
      "Iteration 30, loss = 2.06410400\n",
      "Iteration 31, loss = 2.06549124\n",
      "Iteration 32, loss = 2.06670422\n",
      "Iteration 33, loss = 2.06574429\n",
      "Iteration 34, loss = 2.06274510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 35, loss = 2.04821145\n",
      "Iteration 36, loss = 2.04490738\n",
      "Iteration 37, loss = 2.04512668\n",
      "Iteration 38, loss = 2.04523681\n",
      "Iteration 39, loss = 2.04499874\n",
      "Iteration 40, loss = 2.04521125\n",
      "Iteration 41, loss = 2.04428333\n",
      "Iteration 42, loss = 2.04578056\n",
      "Iteration 43, loss = 2.04521228\n",
      "Iteration 44, loss = 2.04492985\n",
      "Iteration 45, loss = 2.04489153\n",
      "Iteration 46, loss = 2.04472210\n",
      "Iteration 47, loss = 2.04490807\n",
      "Iteration 48, loss = 2.04489597\n",
      "Iteration 49, loss = 2.04524244\n",
      "Iteration 50, loss = 2.04464428\n",
      "Iteration 51, loss = 2.04497087\n",
      "Iteration 52, loss = 2.04437827\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 53, loss = 2.04038945\n",
      "Iteration 54, loss = 2.04001118\n",
      "Iteration 55, loss = 2.04035825\n",
      "Iteration 56, loss = 2.04010087\n",
      "Iteration 57, loss = 2.04034882\n",
      "Iteration 58, loss = 2.04008302\n",
      "Iteration 59, loss = 2.04015414\n",
      "Iteration 60, loss = 2.04011724\n",
      "Iteration 61, loss = 2.04027987\n",
      "Iteration 62, loss = 2.04026457\n",
      "Iteration 63, loss = 2.04033188\n",
      "Iteration 64, loss = 2.04023368\n",
      "Iteration 65, loss = 2.04023827\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 66, loss = 2.03947359\n",
      "Iteration 67, loss = 2.03920129\n",
      "Iteration 68, loss = 2.03916729\n",
      "Iteration 69, loss = 2.03914834\n",
      "Iteration 70, loss = 2.03914814\n",
      "Iteration 71, loss = 2.03915608\n",
      "Iteration 72, loss = 2.03914090\n",
      "Iteration 73, loss = 2.03914502\n",
      "Iteration 74, loss = 2.03912994\n",
      "Iteration 75, loss = 2.03915439\n",
      "Iteration 76, loss = 2.03915684\n",
      "Iteration 77, loss = 2.03914838\n",
      "Iteration 78, loss = 2.03913826\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 79, loss = 2.03889031\n",
      "Iteration 80, loss = 2.03889082\n",
      "Iteration 81, loss = 2.03889381\n",
      "Iteration 82, loss = 2.03888995\n",
      "Iteration 83, loss = 2.03889233\n",
      "Iteration 84, loss = 2.03889082\n",
      "Iteration 85, loss = 2.03889474\n",
      "Iteration 86, loss = 2.03889207\n",
      "Iteration 87, loss = 2.03889001\n",
      "Iteration 88, loss = 2.03888966\n",
      "Iteration 89, loss = 2.03889128\n",
      "Iteration 90, loss = 2.03888834\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.30478585\n",
      "Iteration 2, loss = 2.14951158\n",
      "Iteration 3, loss = 2.15297270\n",
      "Iteration 4, loss = 2.15903942\n",
      "Iteration 5, loss = 2.15364769\n",
      "Iteration 6, loss = 2.14870006\n",
      "Iteration 7, loss = 2.15027679\n",
      "Iteration 8, loss = 2.14647177\n",
      "Iteration 9, loss = 2.15026226\n",
      "Iteration 10, loss = 2.14816197\n",
      "Iteration 11, loss = 2.15367699\n",
      "Iteration 12, loss = 2.15332330\n",
      "Iteration 13, loss = 2.15522572\n",
      "Iteration 14, loss = 2.15489681\n",
      "Iteration 15, loss = 2.14291619\n",
      "Iteration 16, loss = 2.15667942\n",
      "Iteration 17, loss = 2.14892131\n",
      "Iteration 18, loss = 2.15168539\n",
      "Iteration 19, loss = 2.14739587\n",
      "Iteration 20, loss = 2.15589511\n",
      "Iteration 21, loss = 2.15037238\n",
      "Iteration 22, loss = 2.15079563\n",
      "Iteration 23, loss = 2.14503614\n",
      "Iteration 24, loss = 2.15140907\n",
      "Iteration 25, loss = 2.15032446\n",
      "Iteration 26, loss = 2.15305516\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 27, loss = 2.06858534\n",
      "Iteration 28, loss = 2.06073353\n",
      "Iteration 29, loss = 2.05728593\n",
      "Iteration 30, loss = 2.05596688\n",
      "Iteration 31, loss = 2.05942210\n",
      "Iteration 32, loss = 2.05579597\n",
      "Iteration 33, loss = 2.05804969\n",
      "Iteration 34, loss = 2.05849781\n",
      "Iteration 35, loss = 2.05774611\n",
      "Iteration 36, loss = 2.05764707\n",
      "Iteration 37, loss = 2.05741929\n",
      "Iteration 38, loss = 2.05679434\n",
      "Iteration 39, loss = 2.05642171\n",
      "Iteration 40, loss = 2.05611844\n",
      "Iteration 41, loss = 2.05668527\n",
      "Iteration 42, loss = 2.06010720\n",
      "Iteration 43, loss = 2.05760398\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 44, loss = 2.03959751\n",
      "Iteration 45, loss = 2.03768812\n",
      "Iteration 46, loss = 2.03659368\n",
      "Iteration 47, loss = 2.03717092\n",
      "Iteration 48, loss = 2.03704572\n",
      "Iteration 49, loss = 2.03641940\n",
      "Iteration 50, loss = 2.03752020\n",
      "Iteration 51, loss = 2.03639072\n",
      "Iteration 52, loss = 2.03732292\n",
      "Iteration 53, loss = 2.03689095\n",
      "Iteration 54, loss = 2.03688547\n",
      "Iteration 55, loss = 2.03654780\n",
      "Iteration 56, loss = 2.03591106\n",
      "Iteration 57, loss = 2.03682329\n",
      "Iteration 58, loss = 2.03637750\n",
      "Iteration 59, loss = 2.03633411\n",
      "Iteration 60, loss = 2.03609246\n",
      "Iteration 61, loss = 2.03693897\n",
      "Iteration 62, loss = 2.03660396\n",
      "Iteration 63, loss = 2.03665960\n",
      "Iteration 64, loss = 2.03647038\n",
      "Iteration 65, loss = 2.03650268\n",
      "Iteration 66, loss = 2.03645147\n",
      "Iteration 67, loss = 2.03731679\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 68, loss = 2.03230036\n",
      "Iteration 69, loss = 2.03224127\n",
      "Iteration 70, loss = 2.03181740\n",
      "Iteration 71, loss = 2.03202051\n",
      "Iteration 72, loss = 2.03187960\n",
      "Iteration 73, loss = 2.03183390\n",
      "Iteration 74, loss = 2.03206906\n",
      "Iteration 75, loss = 2.03197138\n",
      "Iteration 76, loss = 2.03204791\n",
      "Iteration 77, loss = 2.03202953\n",
      "Iteration 78, loss = 2.03205044\n",
      "Iteration 79, loss = 2.03190411\n",
      "Iteration 80, loss = 2.03210953\n",
      "Iteration 81, loss = 2.03194171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 82, loss = 2.03085862\n",
      "Iteration 83, loss = 2.03079732\n",
      "Iteration 84, loss = 2.03081488\n",
      "Iteration 85, loss = 2.03076775\n",
      "Iteration 86, loss = 2.03080227\n",
      "Iteration 87, loss = 2.03079441\n",
      "Iteration 88, loss = 2.03075290\n",
      "Iteration 89, loss = 2.03080593\n",
      "Iteration 90, loss = 2.03077850\n",
      "Iteration 91, loss = 2.03079145\n",
      "Iteration 92, loss = 2.03078375\n",
      "Iteration 93, loss = 2.03079815\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 94, loss = 2.03052644\n",
      "Iteration 95, loss = 2.03052823\n",
      "Iteration 96, loss = 2.03052603\n",
      "Iteration 97, loss = 2.03052011\n",
      "Iteration 98, loss = 2.03052391\n",
      "Iteration 99, loss = 2.03052066\n",
      "Iteration 100, loss = 2.03052029\n",
      "Iteration 101, loss = 2.03052227\n",
      "Iteration 102, loss = 2.03051778\n",
      "Iteration 103, loss = 2.03051906\n",
      "Iteration 104, loss = 2.03051882\n",
      "Iteration 105, loss = 2.03052126\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.29415899\n",
      "Iteration 2, loss = 2.15509741\n",
      "Iteration 3, loss = 2.15410883\n",
      "Iteration 4, loss = 2.14496002\n",
      "Iteration 5, loss = 2.15751770\n",
      "Iteration 6, loss = 2.14581987\n",
      "Iteration 7, loss = 2.15244457\n",
      "Iteration 8, loss = 2.15051268\n",
      "Iteration 9, loss = 2.14696878\n",
      "Iteration 10, loss = 2.15648951\n",
      "Iteration 11, loss = 2.14862979\n",
      "Iteration 12, loss = 2.14963823\n",
      "Iteration 13, loss = 2.14418064\n",
      "Iteration 14, loss = 2.14210919\n",
      "Iteration 15, loss = 2.15287047\n",
      "Iteration 16, loss = 2.14959810\n",
      "Iteration 17, loss = 2.14344477\n",
      "Iteration 18, loss = 2.14616801\n",
      "Iteration 19, loss = 2.14718044\n",
      "Iteration 20, loss = 2.15161507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 2.14012300\n",
      "Iteration 22, loss = 2.15233912\n",
      "Iteration 23, loss = 2.14820161\n",
      "Iteration 24, loss = 2.13925517\n",
      "Iteration 25, loss = 2.15218980\n",
      "Iteration 26, loss = 2.14707139\n",
      "Iteration 27, loss = 2.14466992\n",
      "Iteration 28, loss = 2.15324875\n",
      "Iteration 29, loss = 2.15247234\n",
      "Iteration 30, loss = 2.15120323\n",
      "Iteration 31, loss = 2.13930791\n",
      "Iteration 32, loss = 2.14378261\n",
      "Iteration 33, loss = 2.14793557\n",
      "Iteration 34, loss = 2.14662855\n",
      "Iteration 35, loss = 2.14589079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 36, loss = 2.06085739\n",
      "Iteration 37, loss = 2.05513272\n",
      "Iteration 38, loss = 2.05212925\n",
      "Iteration 39, loss = 2.05206640\n",
      "Iteration 40, loss = 2.05183587\n",
      "Iteration 41, loss = 2.05134032\n",
      "Iteration 42, loss = 2.05246776\n",
      "Iteration 43, loss = 2.05293610\n",
      "Iteration 44, loss = 2.05157627\n",
      "Iteration 45, loss = 2.05065952\n",
      "Iteration 46, loss = 2.05279210\n",
      "Iteration 47, loss = 2.05397608\n",
      "Iteration 48, loss = 2.05450209\n",
      "Iteration 49, loss = 2.05109796\n",
      "Iteration 50, loss = 2.05150158\n",
      "Iteration 51, loss = 2.05118459\n",
      "Iteration 52, loss = 2.05096975\n",
      "Iteration 53, loss = 2.05159094\n",
      "Iteration 54, loss = 2.05304386\n",
      "Iteration 55, loss = 2.05325952\n",
      "Iteration 56, loss = 2.04899963\n",
      "Iteration 57, loss = 2.05048370\n",
      "Iteration 58, loss = 2.05067051\n",
      "Iteration 59, loss = 2.05176418\n",
      "Iteration 60, loss = 2.05099831\n",
      "Iteration 61, loss = 2.05074298\n",
      "Iteration 62, loss = 2.05212241\n",
      "Iteration 63, loss = 2.05209063\n",
      "Iteration 64, loss = 2.05054100\n",
      "Iteration 65, loss = 2.05306829\n",
      "Iteration 66, loss = 2.04889045\n",
      "Iteration 67, loss = 2.05236552\n",
      "Iteration 68, loss = 2.05062157\n",
      "Iteration 69, loss = 2.05286029\n",
      "Iteration 70, loss = 2.05203975\n",
      "Iteration 71, loss = 2.05128598\n",
      "Iteration 72, loss = 2.04936131\n",
      "Iteration 73, loss = 2.05326315\n",
      "Iteration 74, loss = 2.05200076\n",
      "Iteration 75, loss = 2.05350887\n",
      "Iteration 76, loss = 2.05076195\n",
      "Iteration 77, loss = 2.05209290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 78, loss = 2.03326127\n",
      "Iteration 79, loss = 2.03151178\n",
      "Iteration 80, loss = 2.03179996\n",
      "Iteration 81, loss = 2.03133718\n",
      "Iteration 82, loss = 2.03126109\n",
      "Iteration 83, loss = 2.03180314\n",
      "Iteration 84, loss = 2.03099154\n",
      "Iteration 85, loss = 2.03158791\n",
      "Iteration 86, loss = 2.03167465\n",
      "Iteration 87, loss = 2.03200926\n",
      "Iteration 88, loss = 2.03203241\n",
      "Iteration 89, loss = 2.03152224\n",
      "Iteration 90, loss = 2.03101496\n",
      "Iteration 91, loss = 2.03198521\n",
      "Iteration 92, loss = 2.03116951\n",
      "Iteration 93, loss = 2.03125550\n",
      "Iteration 94, loss = 2.03179610\n",
      "Iteration 95, loss = 2.03180068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 96, loss = 2.02743243\n",
      "Iteration 97, loss = 2.02672164\n",
      "Iteration 98, loss = 2.02642032\n",
      "Iteration 99, loss = 2.02670943\n",
      "Iteration 100, loss = 2.02647666\n",
      "Iteration 101, loss = 2.02670338\n",
      "Iteration 102, loss = 2.02676026\n",
      "Iteration 103, loss = 2.02662242\n",
      "Iteration 104, loss = 2.02671470\n",
      "Iteration 105, loss = 2.02647331\n",
      "Iteration 106, loss = 2.02642365\n",
      "Iteration 107, loss = 2.02690164\n",
      "Iteration 108, loss = 2.02641303\n",
      "Iteration 109, loss = 2.02671147\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 110, loss = 2.02556711\n",
      "Iteration 111, loss = 2.02549531\n",
      "Iteration 112, loss = 2.02553235\n",
      "Iteration 113, loss = 2.02550093\n",
      "Iteration 114, loss = 2.02550795\n",
      "Iteration 115, loss = 2.02550586\n",
      "Iteration 116, loss = 2.02553469\n",
      "Iteration 117, loss = 2.02550288\n",
      "Iteration 118, loss = 2.02550681\n",
      "Iteration 119, loss = 2.02550947\n",
      "Iteration 120, loss = 2.02551154\n",
      "Iteration 121, loss = 2.02550444\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 122, loss = 2.02523540\n",
      "Iteration 123, loss = 2.02523706\n",
      "Iteration 124, loss = 2.02523636\n",
      "Iteration 125, loss = 2.02524045\n",
      "Iteration 126, loss = 2.02523622\n",
      "Iteration 127, loss = 2.02523869\n",
      "Iteration 128, loss = 2.02523730\n",
      "Iteration 129, loss = 2.02523539\n",
      "Iteration 130, loss = 2.02523667\n",
      "Iteration 131, loss = 2.02523548\n",
      "Iteration 132, loss = 2.02523593\n",
      "Iteration 133, loss = 2.02523763\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.34819404\n",
      "Iteration 2, loss = 2.15716603\n",
      "Iteration 3, loss = 2.14170228\n",
      "Iteration 4, loss = 2.14121394\n",
      "Iteration 5, loss = 2.14120037\n",
      "Iteration 6, loss = 2.14882141\n",
      "Iteration 7, loss = 2.14859551\n",
      "Iteration 8, loss = 2.15560749\n",
      "Iteration 9, loss = 2.14759950\n",
      "Iteration 10, loss = 2.14565744\n",
      "Iteration 11, loss = 2.14421129\n",
      "Iteration 12, loss = 2.14655523\n",
      "Iteration 13, loss = 2.13937115\n",
      "Iteration 14, loss = 2.14084113\n",
      "Iteration 15, loss = 2.15368579\n",
      "Iteration 16, loss = 2.14436641\n",
      "Iteration 17, loss = 2.14640336\n",
      "Iteration 18, loss = 2.14887664\n",
      "Iteration 19, loss = 2.15326992\n",
      "Iteration 20, loss = 2.14387318\n",
      "Iteration 21, loss = 2.14447067\n",
      "Iteration 22, loss = 2.13771584\n",
      "Iteration 23, loss = 2.14438245\n",
      "Iteration 24, loss = 2.15624065\n",
      "Iteration 25, loss = 2.14426491\n",
      "Iteration 26, loss = 2.14078164\n",
      "Iteration 27, loss = 2.14925058\n",
      "Iteration 28, loss = 2.15249984\n",
      "Iteration 29, loss = 2.15113034\n",
      "Iteration 30, loss = 2.14352318\n",
      "Iteration 31, loss = 2.14730141\n",
      "Iteration 32, loss = 2.14834284\n",
      "Iteration 33, loss = 2.14561596\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 34, loss = 2.05493290\n",
      "Iteration 35, loss = 2.04905613\n",
      "Iteration 36, loss = 2.04777080\n",
      "Iteration 37, loss = 2.04489376\n",
      "Iteration 38, loss = 2.04877399\n",
      "Iteration 39, loss = 2.04485082\n",
      "Iteration 40, loss = 2.04837559\n",
      "Iteration 41, loss = 2.04611985\n",
      "Iteration 42, loss = 2.04844403\n",
      "Iteration 43, loss = 2.04797358\n",
      "Iteration 44, loss = 2.04869066\n",
      "Iteration 45, loss = 2.04561496\n",
      "Iteration 46, loss = 2.04781367\n",
      "Iteration 47, loss = 2.04794178\n",
      "Iteration 48, loss = 2.04926348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 49, loss = 2.02647160\n",
      "Iteration 50, loss = 2.02509435\n",
      "Iteration 51, loss = 2.02594875\n",
      "Iteration 52, loss = 2.02507144\n",
      "Iteration 53, loss = 2.02501202\n",
      "Iteration 54, loss = 2.02514851\n",
      "Iteration 55, loss = 2.02499398\n",
      "Iteration 56, loss = 2.02510428\n",
      "Iteration 57, loss = 2.02520220\n",
      "Iteration 58, loss = 2.02557780\n",
      "Iteration 59, loss = 2.02561197\n",
      "Iteration 60, loss = 2.02516104\n",
      "Iteration 61, loss = 2.02540392\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 62, loss = 2.02026125\n",
      "Iteration 63, loss = 2.02010178\n",
      "Iteration 64, loss = 2.02016360\n",
      "Iteration 65, loss = 2.02014814\n",
      "Iteration 66, loss = 2.01986005\n",
      "Iteration 67, loss = 2.01995987\n",
      "Iteration 68, loss = 2.02008001\n",
      "Iteration 69, loss = 2.02003800\n",
      "Iteration 70, loss = 2.01999466\n",
      "Iteration 71, loss = 2.02011896\n",
      "Iteration 72, loss = 2.02000999\n",
      "Iteration 73, loss = 2.01975830\n",
      "Iteration 74, loss = 2.02007703\n",
      "Iteration 75, loss = 2.02007674\n",
      "Iteration 76, loss = 2.02004396\n",
      "Iteration 77, loss = 2.02013981\n",
      "Iteration 78, loss = 2.01998420\n",
      "Iteration 79, loss = 2.01999204\n",
      "Iteration 80, loss = 2.02006851\n",
      "Iteration 81, loss = 2.02005161\n",
      "Iteration 82, loss = 2.02016605\n",
      "Iteration 83, loss = 2.02002515\n",
      "Iteration 84, loss = 2.02019211\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 85, loss = 2.01893679\n",
      "Iteration 86, loss = 2.01879592\n",
      "Iteration 87, loss = 2.01876590\n",
      "Iteration 88, loss = 2.01872870\n",
      "Iteration 89, loss = 2.01880278\n",
      "Iteration 90, loss = 2.01878654\n",
      "Iteration 91, loss = 2.01877423\n",
      "Iteration 92, loss = 2.01879680\n",
      "Iteration 93, loss = 2.01877661\n",
      "Iteration 94, loss = 2.01878577\n",
      "Iteration 95, loss = 2.01881245\n",
      "Iteration 96, loss = 2.01877153\n",
      "Iteration 97, loss = 2.01874924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 98, loss = 2.01850360\n",
      "Iteration 99, loss = 2.01850020\n",
      "Iteration 100, loss = 2.01849859\n",
      "Iteration 101, loss = 2.01849787\n",
      "Iteration 102, loss = 2.01849655\n",
      "Iteration 103, loss = 2.01849707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 104, loss = 2.01849780\n",
      "Iteration 105, loss = 2.01849625\n",
      "Iteration 106, loss = 2.01849917\n",
      "Iteration 107, loss = 2.01849443\n",
      "Iteration 108, loss = 2.01849440\n",
      "Iteration 109, loss = 2.01850126\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.40155634\n",
      "Iteration 2, loss = 2.15894242\n",
      "Iteration 3, loss = 2.14424754\n",
      "Iteration 4, loss = 2.13817368\n",
      "Iteration 5, loss = 2.15189889\n",
      "Iteration 6, loss = 2.14825432\n",
      "Iteration 7, loss = 2.14510712\n",
      "Iteration 8, loss = 2.15648928\n",
      "Iteration 9, loss = 2.14721001\n",
      "Iteration 10, loss = 2.14603789\n",
      "Iteration 11, loss = 2.14571102\n",
      "Iteration 12, loss = 2.13184538\n",
      "Iteration 13, loss = 2.14496323\n",
      "Iteration 14, loss = 2.13530367\n",
      "Iteration 15, loss = 2.14825355\n",
      "Iteration 16, loss = 2.14302610\n",
      "Iteration 17, loss = 2.13487897\n",
      "Iteration 18, loss = 2.14871832\n",
      "Iteration 19, loss = 2.14487890\n",
      "Iteration 20, loss = 2.14468082\n",
      "Iteration 21, loss = 2.14515994\n",
      "Iteration 22, loss = 2.14141016\n",
      "Iteration 23, loss = 2.14493552\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 24, loss = 2.05622707\n",
      "Iteration 25, loss = 2.04213040\n",
      "Iteration 26, loss = 2.04170722\n",
      "Iteration 27, loss = 2.04007879\n",
      "Iteration 28, loss = 2.03960265\n",
      "Iteration 29, loss = 2.04013338\n",
      "Iteration 30, loss = 2.03837544\n",
      "Iteration 31, loss = 2.03991069\n",
      "Iteration 32, loss = 2.03998686\n",
      "Iteration 33, loss = 2.04252089\n",
      "Iteration 34, loss = 2.03992251\n",
      "Iteration 35, loss = 2.04047903\n",
      "Iteration 36, loss = 2.04066761\n",
      "Iteration 37, loss = 2.03936768\n",
      "Iteration 38, loss = 2.04064783\n",
      "Iteration 39, loss = 2.04297335\n",
      "Iteration 40, loss = 2.04173971\n",
      "Iteration 41, loss = 2.04223936\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 42, loss = 2.02072122\n",
      "Iteration 43, loss = 2.01880628\n",
      "Iteration 44, loss = 2.01834403\n",
      "Iteration 45, loss = 2.01771572\n",
      "Iteration 46, loss = 2.01792279\n",
      "Iteration 47, loss = 2.01810317\n",
      "Iteration 48, loss = 2.01857996\n",
      "Iteration 49, loss = 2.01821932\n",
      "Iteration 50, loss = 2.01806075\n",
      "Iteration 51, loss = 2.01832157\n",
      "Iteration 52, loss = 2.01829167\n",
      "Iteration 53, loss = 2.01789273\n",
      "Iteration 54, loss = 2.01853741\n",
      "Iteration 55, loss = 2.01856987\n",
      "Iteration 56, loss = 2.01835854\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 57, loss = 2.01356457\n",
      "Iteration 58, loss = 2.01280605\n",
      "Iteration 59, loss = 2.01286237\n",
      "Iteration 60, loss = 2.01286983\n",
      "Iteration 61, loss = 2.01260470\n",
      "Iteration 62, loss = 2.01277462\n",
      "Iteration 63, loss = 2.01293694\n",
      "Iteration 64, loss = 2.01278229\n",
      "Iteration 65, loss = 2.01287499\n",
      "Iteration 66, loss = 2.01269075\n",
      "Iteration 67, loss = 2.01268056\n",
      "Iteration 68, loss = 2.01266930\n",
      "Iteration 69, loss = 2.01294308\n",
      "Iteration 70, loss = 2.01260723\n",
      "Iteration 71, loss = 2.01288198\n",
      "Iteration 72, loss = 2.01277163\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 73, loss = 2.01166020\n",
      "Iteration 74, loss = 2.01156357\n",
      "Iteration 75, loss = 2.01149463\n",
      "Iteration 76, loss = 2.01157769\n",
      "Iteration 77, loss = 2.01153769\n",
      "Iteration 78, loss = 2.01152661\n",
      "Iteration 79, loss = 2.01150716\n",
      "Iteration 80, loss = 2.01155449\n",
      "Iteration 81, loss = 2.01152993\n",
      "Iteration 82, loss = 2.01153318\n",
      "Iteration 83, loss = 2.01152212\n",
      "Iteration 84, loss = 2.01152509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 85, loss = 2.01123562\n",
      "Iteration 86, loss = 2.01123531\n",
      "Iteration 87, loss = 2.01123325\n",
      "Iteration 88, loss = 2.01123451\n",
      "Iteration 89, loss = 2.01123827\n",
      "Iteration 90, loss = 2.01123112\n",
      "Iteration 91, loss = 2.01123012\n",
      "Iteration 92, loss = 2.01123665\n",
      "Iteration 93, loss = 2.01123478\n",
      "Iteration 94, loss = 2.01123419\n",
      "Iteration 95, loss = 2.01123146\n",
      "Iteration 96, loss = 2.01123659\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.34878636\n",
      "Iteration 2, loss = 2.14138551\n",
      "Iteration 3, loss = 2.14670911\n",
      "Iteration 4, loss = 2.13792328\n",
      "Iteration 5, loss = 2.13974373\n",
      "Iteration 6, loss = 2.13494711\n",
      "Iteration 7, loss = 2.14624127\n",
      "Iteration 8, loss = 2.13458931\n",
      "Iteration 9, loss = 2.14166304\n",
      "Iteration 10, loss = 2.15369827\n",
      "Iteration 11, loss = 2.14115241\n",
      "Iteration 12, loss = 2.14536982\n",
      "Iteration 13, loss = 2.13521112\n",
      "Iteration 14, loss = 2.12593316\n",
      "Iteration 15, loss = 2.13555956\n",
      "Iteration 16, loss = 2.13464163\n",
      "Iteration 17, loss = 2.14385470\n",
      "Iteration 18, loss = 2.13567733\n",
      "Iteration 19, loss = 2.14796194\n",
      "Iteration 20, loss = 2.13729815\n",
      "Iteration 21, loss = 2.14217871\n",
      "Iteration 22, loss = 2.13721865\n",
      "Iteration 23, loss = 2.14217672\n",
      "Iteration 24, loss = 2.14381668\n",
      "Iteration 25, loss = 2.13531468\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 26, loss = 2.04822453\n",
      "Iteration 27, loss = 2.03760168\n",
      "Iteration 28, loss = 2.03483914\n",
      "Iteration 29, loss = 2.03423594\n",
      "Iteration 30, loss = 2.03725106\n",
      "Iteration 31, loss = 2.03633616\n",
      "Iteration 32, loss = 2.03325695\n",
      "Iteration 33, loss = 2.03399362\n",
      "Iteration 34, loss = 2.03254670\n",
      "Iteration 35, loss = 2.03650762\n",
      "Iteration 36, loss = 2.03701090\n",
      "Iteration 37, loss = 2.03735377\n",
      "Iteration 38, loss = 2.03397726\n",
      "Iteration 39, loss = 2.03564245\n",
      "Iteration 40, loss = 2.03438516\n",
      "Iteration 41, loss = 2.03368126\n",
      "Iteration 42, loss = 2.03465024\n",
      "Iteration 43, loss = 2.03576007\n",
      "Iteration 44, loss = 2.03560187\n",
      "Iteration 45, loss = 2.03576080\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 46, loss = 2.01544274\n",
      "Iteration 47, loss = 2.01300054\n",
      "Iteration 48, loss = 2.01315079\n",
      "Iteration 49, loss = 2.01228791\n",
      "Iteration 50, loss = 2.01284971\n",
      "Iteration 51, loss = 2.01310557\n",
      "Iteration 52, loss = 2.01238471\n",
      "Iteration 53, loss = 2.01252951\n",
      "Iteration 54, loss = 2.01298712\n",
      "Iteration 55, loss = 2.01228368\n",
      "Iteration 56, loss = 2.01234251\n",
      "Iteration 57, loss = 2.01194716\n",
      "Iteration 58, loss = 2.01328439\n",
      "Iteration 59, loss = 2.01305030\n",
      "Iteration 60, loss = 2.01239842\n",
      "Iteration 61, loss = 2.01305174\n",
      "Iteration 62, loss = 2.01292976\n",
      "Iteration 63, loss = 2.01285643\n",
      "Iteration 64, loss = 2.01278946\n",
      "Iteration 65, loss = 2.01229335\n",
      "Iteration 66, loss = 2.01240269\n",
      "Iteration 67, loss = 2.01257656\n",
      "Iteration 68, loss = 2.01327365\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 69, loss = 2.00755692\n",
      "Iteration 70, loss = 2.00736724\n",
      "Iteration 71, loss = 2.00745557\n",
      "Iteration 72, loss = 2.00746875\n",
      "Iteration 73, loss = 2.00726269\n",
      "Iteration 74, loss = 2.00724852\n",
      "Iteration 75, loss = 2.00711753\n",
      "Iteration 76, loss = 2.00740935\n",
      "Iteration 77, loss = 2.00742139\n",
      "Iteration 78, loss = 2.00727728\n",
      "Iteration 79, loss = 2.00744709\n",
      "Iteration 80, loss = 2.00735746\n",
      "Iteration 81, loss = 2.00739957\n",
      "Iteration 82, loss = 2.00737574\n",
      "Iteration 83, loss = 2.00742383\n",
      "Iteration 84, loss = 2.00730281\n",
      "Iteration 85, loss = 2.00716984\n",
      "Iteration 86, loss = 2.00718985\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 87, loss = 2.00662405\n",
      "Iteration 88, loss = 2.00613620\n",
      "Iteration 89, loss = 2.00612267\n",
      "Iteration 90, loss = 2.00611060\n",
      "Iteration 91, loss = 2.00615323\n",
      "Iteration 92, loss = 2.00610356\n",
      "Iteration 93, loss = 2.00610728\n",
      "Iteration 94, loss = 2.00610044\n",
      "Iteration 95, loss = 2.00611023\n",
      "Iteration 96, loss = 2.00611785\n",
      "Iteration 97, loss = 2.00612402\n",
      "Iteration 98, loss = 2.00611938\n",
      "Iteration 99, loss = 2.00609059\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 100, loss = 2.00584995\n",
      "Iteration 101, loss = 2.00584309\n",
      "Iteration 102, loss = 2.00583919\n",
      "Iteration 103, loss = 2.00583258\n",
      "Iteration 104, loss = 2.00583473\n",
      "Iteration 105, loss = 2.00583067\n",
      "Iteration 106, loss = 2.00582907\n",
      "Iteration 107, loss = 2.00582983\n",
      "Iteration 108, loss = 2.00583067\n",
      "Iteration 109, loss = 2.00582970\n",
      "Iteration 110, loss = 2.00583048\n",
      "Iteration 111, loss = 2.00582869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.42559258\n",
      "Iteration 2, loss = 2.14464857\n",
      "Iteration 3, loss = 2.15176280\n",
      "Iteration 4, loss = 2.14771983\n",
      "Iteration 5, loss = 2.13565304\n",
      "Iteration 6, loss = 2.13054926\n",
      "Iteration 7, loss = 2.13460591\n",
      "Iteration 8, loss = 2.13923238\n",
      "Iteration 9, loss = 2.13303217\n",
      "Iteration 10, loss = 2.12266596\n",
      "Iteration 11, loss = 2.13378983\n",
      "Iteration 12, loss = 2.13416750\n",
      "Iteration 13, loss = 2.13737122\n",
      "Iteration 14, loss = 2.13119779\n",
      "Iteration 15, loss = 2.13481683\n",
      "Iteration 16, loss = 2.14044215\n",
      "Iteration 17, loss = 2.15267467\n",
      "Iteration 18, loss = 2.13982556\n",
      "Iteration 19, loss = 2.12604537\n",
      "Iteration 20, loss = 2.13696180\n",
      "Iteration 21, loss = 2.13451709\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 22, loss = 2.04066330\n",
      "Iteration 23, loss = 2.03158484\n",
      "Iteration 24, loss = 2.03305944\n",
      "Iteration 25, loss = 2.03129106\n",
      "Iteration 26, loss = 2.03293384\n",
      "Iteration 27, loss = 2.03217924\n",
      "Iteration 28, loss = 2.03169594\n",
      "Iteration 29, loss = 2.03352497\n",
      "Iteration 30, loss = 2.03333779\n",
      "Iteration 31, loss = 2.03151724\n",
      "Iteration 32, loss = 2.03057374\n",
      "Iteration 33, loss = 2.03176768\n",
      "Iteration 34, loss = 2.03135525\n",
      "Iteration 35, loss = 2.03532082\n",
      "Iteration 36, loss = 2.03057397\n",
      "Iteration 37, loss = 2.03199783\n",
      "Iteration 38, loss = 2.03257247\n",
      "Iteration 39, loss = 2.03013671\n",
      "Iteration 40, loss = 2.02814663\n",
      "Iteration 41, loss = 2.03216617\n",
      "Iteration 42, loss = 2.03481801\n",
      "Iteration 43, loss = 2.03191149\n",
      "Iteration 44, loss = 2.03275490\n",
      "Iteration 45, loss = 2.03566513\n",
      "Iteration 46, loss = 2.03263406\n",
      "Iteration 47, loss = 2.03332846\n",
      "Iteration 48, loss = 2.03209290\n",
      "Iteration 49, loss = 2.03132877\n",
      "Iteration 50, loss = 2.03367263\n",
      "Iteration 51, loss = 2.03195188\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 52, loss = 2.01411160\n",
      "Iteration 53, loss = 2.01015593\n",
      "Iteration 54, loss = 2.00948649\n",
      "Iteration 55, loss = 2.01047765\n",
      "Iteration 56, loss = 2.01088749\n",
      "Iteration 57, loss = 2.00982489\n",
      "Iteration 58, loss = 2.01016867\n",
      "Iteration 59, loss = 2.00891485\n",
      "Iteration 60, loss = 2.00985415\n",
      "Iteration 61, loss = 2.00960384\n",
      "Iteration 62, loss = 2.00924093\n",
      "Iteration 63, loss = 2.01045912\n",
      "Iteration 64, loss = 2.00950260\n",
      "Iteration 65, loss = 2.00998393\n",
      "Iteration 66, loss = 2.00995156\n",
      "Iteration 67, loss = 2.01010346\n",
      "Iteration 68, loss = 2.00989159\n",
      "Iteration 69, loss = 2.01026423\n",
      "Iteration 70, loss = 2.00931315\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 71, loss = 2.00524300\n",
      "Iteration 72, loss = 2.00452935\n",
      "Iteration 73, loss = 2.00456413\n",
      "Iteration 74, loss = 2.00467607\n",
      "Iteration 75, loss = 2.00416151\n",
      "Iteration 76, loss = 2.00457655\n",
      "Iteration 77, loss = 2.00453964\n",
      "Iteration 78, loss = 2.00456549\n",
      "Iteration 79, loss = 2.00434731\n",
      "Iteration 80, loss = 2.00453979\n",
      "Iteration 81, loss = 2.00446226\n",
      "Iteration 82, loss = 2.00465183\n",
      "Iteration 83, loss = 2.00444336\n",
      "Iteration 84, loss = 2.00452355\n",
      "Iteration 85, loss = 2.00455413\n",
      "Iteration 86, loss = 2.00457822\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 87, loss = 2.00340091\n",
      "Iteration 88, loss = 2.00329525\n",
      "Iteration 89, loss = 2.00328023\n",
      "Iteration 90, loss = 2.00329438\n",
      "Iteration 91, loss = 2.00329364\n",
      "Iteration 92, loss = 2.00327496\n",
      "Iteration 93, loss = 2.00326245\n",
      "Iteration 94, loss = 2.00325694\n",
      "Iteration 95, loss = 2.00325383\n",
      "Iteration 96, loss = 2.00326182\n",
      "Iteration 97, loss = 2.00328540\n",
      "Iteration 98, loss = 2.00326613\n",
      "Iteration 99, loss = 2.00328120\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 100, loss = 2.00299313\n",
      "Iteration 101, loss = 2.00298997\n",
      "Iteration 102, loss = 2.00298914\n",
      "Iteration 103, loss = 2.00298939\n",
      "Iteration 104, loss = 2.00298992\n",
      "Iteration 105, loss = 2.00299561\n",
      "Iteration 106, loss = 2.00298900\n",
      "Iteration 107, loss = 2.00298809\n",
      "Iteration 108, loss = 2.00298519\n",
      "Iteration 109, loss = 2.00298613\n",
      "Iteration 110, loss = 2.00298575\n",
      "Iteration 111, loss = 2.00298641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.40207956\n",
      "Iteration 2, loss = 2.13587435\n",
      "Iteration 3, loss = 2.13292835\n",
      "Iteration 4, loss = 2.13821757\n",
      "Iteration 5, loss = 2.13162246\n",
      "Iteration 6, loss = 2.12364346\n",
      "Iteration 7, loss = 2.13653570\n",
      "Iteration 8, loss = 2.12364580\n",
      "Iteration 9, loss = 2.13314938\n",
      "Iteration 10, loss = 2.13393790\n",
      "Iteration 11, loss = 2.13933998\n",
      "Iteration 12, loss = 2.12399853\n",
      "Iteration 13, loss = 2.14567455\n",
      "Iteration 14, loss = 2.12530080\n",
      "Iteration 15, loss = 2.13202862\n",
      "Iteration 16, loss = 2.13516709\n",
      "Iteration 17, loss = 2.12060894\n",
      "Iteration 18, loss = 2.13673616\n",
      "Iteration 19, loss = 2.13792702\n",
      "Iteration 20, loss = 2.12784530\n",
      "Iteration 21, loss = 2.12905963\n",
      "Iteration 22, loss = 2.13662740\n",
      "Iteration 23, loss = 2.13756232\n",
      "Iteration 24, loss = 2.13167797\n",
      "Iteration 25, loss = 2.12807001\n",
      "Iteration 26, loss = 2.13593674\n",
      "Iteration 27, loss = 2.12905140\n",
      "Iteration 28, loss = 2.13262699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 29, loss = 2.03484036\n",
      "Iteration 30, loss = 2.02483528\n",
      "Iteration 31, loss = 2.02463513\n",
      "Iteration 32, loss = 2.02656562\n",
      "Iteration 33, loss = 2.02651270\n",
      "Iteration 34, loss = 2.02739319\n",
      "Iteration 35, loss = 2.02670732\n",
      "Iteration 36, loss = 2.02543473\n",
      "Iteration 37, loss = 2.02631358\n",
      "Iteration 38, loss = 2.02667339\n",
      "Iteration 39, loss = 2.02688645\n",
      "Iteration 40, loss = 2.02580810\n",
      "Iteration 41, loss = 2.02524519\n",
      "Iteration 42, loss = 2.02639654\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 43, loss = 2.00681415\n",
      "Iteration 44, loss = 2.00376313\n",
      "Iteration 45, loss = 2.00350112\n",
      "Iteration 46, loss = 2.00265826\n",
      "Iteration 47, loss = 2.00243470\n",
      "Iteration 48, loss = 2.00245687\n",
      "Iteration 49, loss = 2.00307778\n",
      "Iteration 50, loss = 2.00281093\n",
      "Iteration 51, loss = 2.00318690\n",
      "Iteration 52, loss = 2.00367546\n",
      "Iteration 53, loss = 2.00309948\n",
      "Iteration 54, loss = 2.00155518\n",
      "Iteration 55, loss = 2.00292920\n",
      "Iteration 56, loss = 2.00295811\n",
      "Iteration 57, loss = 2.00317289\n",
      "Iteration 58, loss = 2.00332319\n",
      "Iteration 59, loss = 2.00216300\n",
      "Iteration 60, loss = 2.00228709\n",
      "Iteration 61, loss = 2.00300314\n",
      "Iteration 62, loss = 2.00253098\n",
      "Iteration 63, loss = 2.00213326\n",
      "Iteration 64, loss = 2.00191407\n",
      "Iteration 65, loss = 2.00322576\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 66, loss = 1.99780253\n",
      "Iteration 67, loss = 1.99750683\n",
      "Iteration 68, loss = 1.99753266\n",
      "Iteration 69, loss = 1.99741314\n",
      "Iteration 70, loss = 1.99725201\n",
      "Iteration 71, loss = 1.99749349\n",
      "Iteration 72, loss = 1.99736035\n",
      "Iteration 73, loss = 1.99736193\n",
      "Iteration 74, loss = 1.99728820\n",
      "Iteration 75, loss = 1.99721285\n",
      "Iteration 76, loss = 1.99734260\n",
      "Iteration 77, loss = 1.99736963\n",
      "Iteration 78, loss = 1.99735700\n",
      "Iteration 79, loss = 1.99731801\n",
      "Iteration 80, loss = 1.99732938\n",
      "Iteration 81, loss = 1.99736218\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 82, loss = 1.99614298\n",
      "Iteration 83, loss = 1.99608417\n",
      "Iteration 84, loss = 1.99607841\n",
      "Iteration 85, loss = 1.99605892\n",
      "Iteration 86, loss = 1.99608286\n",
      "Iteration 87, loss = 1.99608551\n",
      "Iteration 88, loss = 1.99606050\n",
      "Iteration 89, loss = 1.99602303\n",
      "Iteration 90, loss = 1.99605232\n",
      "Iteration 91, loss = 1.99608149\n",
      "Iteration 92, loss = 1.99604602\n",
      "Iteration 93, loss = 1.99605820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 94, loss = 1.99578493\n",
      "Iteration 95, loss = 1.99577765\n",
      "Iteration 96, loss = 1.99577603\n",
      "Iteration 97, loss = 1.99577577\n",
      "Iteration 98, loss = 1.99577264\n",
      "Iteration 99, loss = 1.99577327\n",
      "Iteration 100, loss = 1.99577526\n",
      "Iteration 101, loss = 1.99577451\n",
      "Iteration 102, loss = 1.99577508\n",
      "Iteration 103, loss = 1.99577161\n",
      "Iteration 104, loss = 1.99577305\n",
      "Iteration 105, loss = 1.99576965\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.46223993\n",
      "Iteration 2, loss = 2.13244337\n",
      "Iteration 3, loss = 2.12963733\n",
      "Iteration 4, loss = 2.13869813\n",
      "Iteration 5, loss = 2.13472194\n",
      "Iteration 6, loss = 2.12487786\n",
      "Iteration 7, loss = 2.13518420\n",
      "Iteration 8, loss = 2.12714289\n",
      "Iteration 9, loss = 2.12766373\n",
      "Iteration 10, loss = 2.12672509\n",
      "Iteration 11, loss = 2.13418235\n",
      "Iteration 12, loss = 2.12519903\n",
      "Iteration 13, loss = 2.12819507\n",
      "Iteration 14, loss = 2.13281712\n",
      "Iteration 15, loss = 2.13271436\n",
      "Iteration 16, loss = 2.11776037\n",
      "Iteration 17, loss = 2.12005738\n",
      "Iteration 18, loss = 2.13275921\n",
      "Iteration 19, loss = 2.12410508\n",
      "Iteration 20, loss = 2.12822660\n",
      "Iteration 21, loss = 2.12456512\n",
      "Iteration 22, loss = 2.12201215\n",
      "Iteration 23, loss = 2.12948066\n",
      "Iteration 24, loss = 2.14594554\n",
      "Iteration 25, loss = 2.12542517\n",
      "Iteration 26, loss = 2.13281770\n",
      "Iteration 27, loss = 2.13173212\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 28, loss = 2.03906787\n",
      "Iteration 29, loss = 2.02343191\n",
      "Iteration 30, loss = 2.02237617\n",
      "Iteration 31, loss = 2.02311924\n",
      "Iteration 32, loss = 2.01665738\n",
      "Iteration 33, loss = 2.02126703\n",
      "Iteration 34, loss = 2.02287016\n",
      "Iteration 35, loss = 2.02298174\n",
      "Iteration 36, loss = 2.02248467\n",
      "Iteration 37, loss = 2.02153474\n",
      "Iteration 38, loss = 2.02212014\n",
      "Iteration 39, loss = 2.02558834\n",
      "Iteration 40, loss = 2.01910647\n",
      "Iteration 41, loss = 2.02333918\n",
      "Iteration 42, loss = 2.02310631\n",
      "Iteration 43, loss = 2.02053115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 44, loss = 2.00222667\n",
      "Iteration 45, loss = 1.99696168\n",
      "Iteration 46, loss = 1.99803590\n",
      "Iteration 47, loss = 1.99745915\n",
      "Iteration 48, loss = 1.99776644\n",
      "Iteration 49, loss = 1.99705855\n",
      "Iteration 50, loss = 1.99746385\n",
      "Iteration 51, loss = 1.99662712\n",
      "Iteration 52, loss = 1.99757615\n",
      "Iteration 53, loss = 1.99722656\n",
      "Iteration 54, loss = 1.99723036\n",
      "Iteration 55, loss = 1.99769982\n",
      "Iteration 56, loss = 1.99714857\n",
      "Iteration 57, loss = 1.99733775\n",
      "Iteration 58, loss = 1.99738676\n",
      "Iteration 59, loss = 1.99651466\n",
      "Iteration 60, loss = 1.99762586\n",
      "Iteration 61, loss = 1.99763699\n",
      "Iteration 62, loss = 1.99794096\n",
      "Iteration 63, loss = 1.99724062\n",
      "Iteration 64, loss = 1.99642704\n",
      "Iteration 65, loss = 1.99682474\n",
      "Iteration 66, loss = 1.99749367\n",
      "Iteration 67, loss = 1.99699716\n",
      "Iteration 68, loss = 1.99726544\n",
      "Iteration 69, loss = 1.99693420\n",
      "Iteration 70, loss = 1.99735656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 71, loss = 1.99186007\n",
      "Iteration 72, loss = 1.99199145\n",
      "Iteration 73, loss = 1.99162039\n",
      "Iteration 74, loss = 1.99181354\n",
      "Iteration 75, loss = 1.99175534\n",
      "Iteration 76, loss = 1.99172564\n",
      "Iteration 77, loss = 1.99164835\n",
      "Iteration 78, loss = 1.99167341\n",
      "Iteration 79, loss = 1.99156572\n",
      "Iteration 80, loss = 1.99179852\n",
      "Iteration 81, loss = 1.99179261\n",
      "Iteration 82, loss = 1.99162268\n",
      "Iteration 83, loss = 1.99175334\n",
      "Iteration 84, loss = 1.99182526\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 85, loss = 1.99062397\n",
      "Iteration 86, loss = 1.99044350\n",
      "Iteration 87, loss = 1.99041984\n",
      "Iteration 88, loss = 1.99042378\n",
      "Iteration 89, loss = 1.99044851\n",
      "Iteration 90, loss = 1.99041453\n",
      "Iteration 91, loss = 1.99040777\n",
      "Iteration 92, loss = 1.99039796\n",
      "Iteration 93, loss = 1.99042715\n",
      "Iteration 94, loss = 1.99042481\n",
      "Iteration 95, loss = 1.99043042\n",
      "Iteration 96, loss = 1.99043804\n",
      "Iteration 97, loss = 1.99043796\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 98, loss = 1.99010691\n",
      "Iteration 99, loss = 1.99010177\n",
      "Iteration 100, loss = 1.99011080\n",
      "Iteration 101, loss = 1.99010209\n",
      "Iteration 102, loss = 1.99010715\n",
      "Iteration 103, loss = 1.99010701\n",
      "Iteration 104, loss = 1.99010594\n",
      "Iteration 105, loss = 1.99010528\n",
      "Iteration 106, loss = 1.99010577\n",
      "Iteration 107, loss = 1.99010312\n",
      "Iteration 108, loss = 1.99010718\n",
      "Iteration 109, loss = 1.99010416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.48743403\n",
      "Iteration 2, loss = 2.12283212\n",
      "Iteration 3, loss = 2.12266848\n",
      "Iteration 4, loss = 2.12743577\n",
      "Iteration 5, loss = 2.12438142\n",
      "Iteration 6, loss = 2.12814051\n",
      "Iteration 7, loss = 2.14233954\n",
      "Iteration 8, loss = 2.12544151\n",
      "Iteration 9, loss = 2.11832593\n",
      "Iteration 10, loss = 2.12470869\n",
      "Iteration 11, loss = 2.12925192\n",
      "Iteration 12, loss = 2.12590656\n",
      "Iteration 13, loss = 2.12256282\n",
      "Iteration 14, loss = 2.12692393\n",
      "Iteration 15, loss = 2.12456788\n",
      "Iteration 16, loss = 2.13144795\n",
      "Iteration 17, loss = 2.13315107\n",
      "Iteration 18, loss = 2.12186453\n",
      "Iteration 19, loss = 2.11851500\n",
      "Iteration 20, loss = 2.13151332\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 21, loss = 2.03050991\n",
      "Iteration 22, loss = 2.01675819\n",
      "Iteration 23, loss = 2.01607308\n",
      "Iteration 24, loss = 2.01475244\n",
      "Iteration 25, loss = 2.01405373\n",
      "Iteration 26, loss = 2.01848994\n",
      "Iteration 27, loss = 2.01716607\n",
      "Iteration 28, loss = 2.01611905\n",
      "Iteration 29, loss = 2.01859639\n",
      "Iteration 30, loss = 2.01654478\n",
      "Iteration 31, loss = 2.01732640\n",
      "Iteration 32, loss = 2.01870898\n",
      "Iteration 33, loss = 2.01713928\n",
      "Iteration 34, loss = 2.01899765\n",
      "Iteration 35, loss = 2.01483443\n",
      "Iteration 36, loss = 2.01470795\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 37, loss = 1.99785727\n",
      "Iteration 38, loss = 1.99414314\n",
      "Iteration 39, loss = 1.99452171\n",
      "Iteration 40, loss = 1.99379021\n",
      "Iteration 41, loss = 1.99410650\n",
      "Iteration 42, loss = 1.99422659\n",
      "Iteration 43, loss = 1.99429488\n",
      "Iteration 44, loss = 1.99374462\n",
      "Iteration 45, loss = 1.99390227\n",
      "Iteration 46, loss = 1.99364626\n",
      "Iteration 47, loss = 1.99395258\n",
      "Iteration 48, loss = 1.99427760\n",
      "Iteration 49, loss = 1.99308311\n",
      "Iteration 50, loss = 1.99429526\n",
      "Iteration 51, loss = 1.99357276\n",
      "Iteration 52, loss = 1.99357708\n",
      "Iteration 53, loss = 1.99397812\n",
      "Iteration 54, loss = 1.99416687\n",
      "Iteration 55, loss = 1.99369526\n",
      "Iteration 56, loss = 1.99373958\n",
      "Iteration 57, loss = 1.99392046\n",
      "Iteration 58, loss = 1.99346925\n",
      "Iteration 59, loss = 1.99421910\n",
      "Iteration 60, loss = 1.99405321\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 61, loss = 1.98874898\n",
      "Iteration 62, loss = 1.98837180\n",
      "Iteration 63, loss = 1.98833772\n",
      "Iteration 64, loss = 1.98851737\n",
      "Iteration 65, loss = 1.98821697\n",
      "Iteration 66, loss = 1.98801704\n",
      "Iteration 67, loss = 1.98825339\n",
      "Iteration 68, loss = 1.98818021\n",
      "Iteration 69, loss = 1.98835353\n",
      "Iteration 70, loss = 1.98844063\n",
      "Iteration 71, loss = 1.98819899\n",
      "Iteration 72, loss = 1.98809682\n",
      "Iteration 73, loss = 1.98823482\n",
      "Iteration 74, loss = 1.98844531\n",
      "Iteration 75, loss = 1.98803182\n",
      "Iteration 76, loss = 1.98840102\n",
      "Iteration 77, loss = 1.98826562\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 78, loss = 1.98698502\n",
      "Iteration 79, loss = 1.98696796\n",
      "Iteration 80, loss = 1.98697238\n",
      "Iteration 81, loss = 1.98693992\n",
      "Iteration 82, loss = 1.98695969\n",
      "Iteration 83, loss = 1.98694874\n",
      "Iteration 84, loss = 1.98696226\n",
      "Iteration 85, loss = 1.98697512\n",
      "Iteration 86, loss = 1.98694689\n",
      "Iteration 87, loss = 1.98695780\n",
      "Iteration 88, loss = 1.98694724\n",
      "Iteration 89, loss = 1.98695545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 1.98665760\n",
      "Iteration 91, loss = 1.98665381\n",
      "Iteration 92, loss = 1.98665338\n",
      "Iteration 93, loss = 1.98665307\n",
      "Iteration 94, loss = 1.98665321\n",
      "Iteration 95, loss = 1.98664573\n",
      "Iteration 96, loss = 1.98665047\n",
      "Iteration 97, loss = 1.98664753\n",
      "Iteration 98, loss = 1.98665034\n",
      "Iteration 99, loss = 1.98665642\n",
      "Iteration 100, loss = 1.98664796\n",
      "Iteration 101, loss = 1.98664540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.48338984\n",
      "Iteration 2, loss = 2.12943386\n",
      "Iteration 3, loss = 2.11840466\n",
      "Iteration 4, loss = 2.12865669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 2.12939394\n",
      "Iteration 6, loss = 2.12890242\n",
      "Iteration 7, loss = 2.12235717\n",
      "Iteration 8, loss = 2.12593095\n",
      "Iteration 9, loss = 2.12035982\n",
      "Iteration 10, loss = 2.13250602\n",
      "Iteration 11, loss = 2.11956119\n",
      "Iteration 12, loss = 2.12969158\n",
      "Iteration 13, loss = 2.12948420\n",
      "Iteration 14, loss = 2.11796619\n",
      "Iteration 15, loss = 2.12003867\n",
      "Iteration 16, loss = 2.12289446\n",
      "Iteration 17, loss = 2.11590100\n",
      "Iteration 18, loss = 2.12519542\n",
      "Iteration 19, loss = 2.12080098\n",
      "Iteration 20, loss = 2.12260824\n",
      "Iteration 21, loss = 2.12105927\n",
      "Iteration 22, loss = 2.12816039\n",
      "Iteration 23, loss = 2.13201489\n",
      "Iteration 24, loss = 2.12246691\n",
      "Iteration 25, loss = 2.12103772\n",
      "Iteration 26, loss = 2.12796945\n",
      "Iteration 27, loss = 2.12512660\n",
      "Iteration 28, loss = 2.12200768\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 29, loss = 2.02139318\n",
      "Iteration 30, loss = 2.01477175\n",
      "Iteration 31, loss = 2.01380275\n",
      "Iteration 32, loss = 2.01276565\n",
      "Iteration 33, loss = 2.01165089\n",
      "Iteration 34, loss = 2.01560074\n",
      "Iteration 35, loss = 2.01397090\n",
      "Iteration 36, loss = 2.01208177\n",
      "Iteration 37, loss = 2.01223009\n",
      "Iteration 38, loss = 2.01405363\n",
      "Iteration 39, loss = 2.01515317\n",
      "Iteration 40, loss = 2.01436932\n",
      "Iteration 41, loss = 2.01428835\n",
      "Iteration 42, loss = 2.01510181\n",
      "Iteration 43, loss = 2.00831209\n",
      "Iteration 44, loss = 2.01417779\n",
      "Iteration 45, loss = 2.01160392\n",
      "Iteration 46, loss = 2.01196905\n",
      "Iteration 47, loss = 2.01006340\n",
      "Iteration 48, loss = 2.01299577\n",
      "Iteration 49, loss = 2.01411360\n",
      "Iteration 50, loss = 2.01341670\n",
      "Iteration 51, loss = 2.01287252\n",
      "Iteration 52, loss = 2.01311708\n",
      "Iteration 53, loss = 2.01322503\n",
      "Iteration 54, loss = 2.01200938\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 55, loss = 1.99290861\n",
      "Iteration 56, loss = 1.98944980\n",
      "Iteration 57, loss = 1.98814237\n",
      "Iteration 58, loss = 1.98772239\n",
      "Iteration 59, loss = 1.98912292\n",
      "Iteration 60, loss = 1.98933891\n",
      "Iteration 61, loss = 1.98889105\n",
      "Iteration 62, loss = 1.98939177\n",
      "Iteration 63, loss = 1.98923689\n",
      "Iteration 64, loss = 1.98834662\n",
      "Iteration 65, loss = 1.98854288\n",
      "Iteration 66, loss = 1.98889911\n",
      "Iteration 67, loss = 1.98869641\n",
      "Iteration 68, loss = 1.98841090\n",
      "Iteration 69, loss = 1.99014392\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 70, loss = 1.98326810\n",
      "Iteration 71, loss = 1.98312921\n",
      "Iteration 72, loss = 1.98308322\n",
      "Iteration 73, loss = 1.98312744\n",
      "Iteration 74, loss = 1.98294557\n",
      "Iteration 75, loss = 1.98274590\n",
      "Iteration 76, loss = 1.98296977\n",
      "Iteration 77, loss = 1.98293623\n",
      "Iteration 78, loss = 1.98318957\n",
      "Iteration 79, loss = 1.98305126\n",
      "Iteration 80, loss = 1.98301044\n",
      "Iteration 81, loss = 1.98312022\n",
      "Iteration 82, loss = 1.98289033\n",
      "Iteration 83, loss = 1.98311464\n",
      "Iteration 84, loss = 1.98311695\n",
      "Iteration 85, loss = 1.98305289\n",
      "Iteration 86, loss = 1.98300646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 87, loss = 1.98186132\n",
      "Iteration 88, loss = 1.98171247\n",
      "Iteration 89, loss = 1.98168821\n",
      "Iteration 90, loss = 1.98167822\n",
      "Iteration 91, loss = 1.98168027\n",
      "Iteration 92, loss = 1.98166310\n",
      "Iteration 93, loss = 1.98166402\n",
      "Iteration 94, loss = 1.98168790\n",
      "Iteration 95, loss = 1.98168894\n",
      "Iteration 96, loss = 1.98165965\n",
      "Iteration 97, loss = 1.98166528\n",
      "Iteration 98, loss = 1.98165063\n",
      "Iteration 99, loss = 1.98166762\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 100, loss = 1.98136671\n",
      "Iteration 101, loss = 1.98136260\n",
      "Iteration 102, loss = 1.98136078\n",
      "Iteration 103, loss = 1.98135561\n",
      "Iteration 104, loss = 1.98136006\n",
      "Iteration 105, loss = 1.98135619\n",
      "Iteration 106, loss = 1.98135568\n",
      "Iteration 107, loss = 1.98135814\n",
      "Iteration 108, loss = 1.98135420\n",
      "Iteration 109, loss = 1.98135392\n",
      "Iteration 110, loss = 1.98135164\n",
      "Iteration 111, loss = 1.98135531\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.46349156\n",
      "Iteration 2, loss = 2.13028635\n",
      "Iteration 3, loss = 2.12454885\n",
      "Iteration 4, loss = 2.12038861\n",
      "Iteration 5, loss = 2.11262409\n",
      "Iteration 6, loss = 2.12362242\n",
      "Iteration 7, loss = 2.12231918\n",
      "Iteration 8, loss = 2.11905867\n",
      "Iteration 9, loss = 2.12193383\n",
      "Iteration 10, loss = 2.12411730\n",
      "Iteration 11, loss = 2.11796715\n",
      "Iteration 12, loss = 2.11315596\n",
      "Iteration 13, loss = 2.11918126\n",
      "Iteration 14, loss = 2.11000387\n",
      "Iteration 15, loss = 2.11878372\n",
      "Iteration 16, loss = 2.12627221\n",
      "Iteration 17, loss = 2.11741765\n",
      "Iteration 18, loss = 2.12594563\n",
      "Iteration 19, loss = 2.12525324\n",
      "Iteration 20, loss = 2.11975918\n",
      "Iteration 21, loss = 2.11528237\n",
      "Iteration 22, loss = 2.11248207\n",
      "Iteration 23, loss = 2.11726787\n",
      "Iteration 24, loss = 2.12001927\n",
      "Iteration 25, loss = 2.12528139\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 26, loss = 2.01692874\n",
      "Iteration 27, loss = 2.00484869\n",
      "Iteration 28, loss = 2.00508765\n",
      "Iteration 29, loss = 2.00986582\n",
      "Iteration 30, loss = 2.00811034\n",
      "Iteration 31, loss = 2.00754156\n",
      "Iteration 32, loss = 2.00740841\n",
      "Iteration 33, loss = 2.00363890\n",
      "Iteration 34, loss = 2.00773530\n",
      "Iteration 35, loss = 2.00770563\n",
      "Iteration 36, loss = 2.00862066\n",
      "Iteration 37, loss = 2.00575241\n",
      "Iteration 38, loss = 2.00701564\n",
      "Iteration 39, loss = 2.00498029\n",
      "Iteration 40, loss = 2.00412865\n",
      "Iteration 41, loss = 2.00468522\n",
      "Iteration 42, loss = 2.00533677\n",
      "Iteration 43, loss = 2.00681253\n",
      "Iteration 44, loss = 2.00787333\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 45, loss = 1.98345105\n",
      "Iteration 46, loss = 1.98241799\n",
      "Iteration 47, loss = 1.98072272\n",
      "Iteration 48, loss = 1.98162122\n",
      "Iteration 49, loss = 1.98183901\n",
      "Iteration 50, loss = 1.98173341\n",
      "Iteration 51, loss = 1.98192384\n",
      "Iteration 52, loss = 1.98182460\n",
      "Iteration 53, loss = 1.98125633\n",
      "Iteration 54, loss = 1.98110832\n",
      "Iteration 55, loss = 1.98116800\n",
      "Iteration 56, loss = 1.98212441\n",
      "Iteration 57, loss = 1.98163228\n",
      "Iteration 58, loss = 1.98101394\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 59, loss = 1.97578485\n",
      "Iteration 60, loss = 1.97571132\n",
      "Iteration 61, loss = 1.97559449\n",
      "Iteration 62, loss = 1.97591384\n",
      "Iteration 63, loss = 1.97576861\n",
      "Iteration 64, loss = 1.97558311\n",
      "Iteration 65, loss = 1.97576470\n",
      "Iteration 66, loss = 1.97565758\n",
      "Iteration 67, loss = 1.97562089\n",
      "Iteration 68, loss = 1.97568439\n",
      "Iteration 69, loss = 1.97574381\n",
      "Iteration 70, loss = 1.97560512\n",
      "Iteration 71, loss = 1.97562137\n",
      "Iteration 72, loss = 1.97589319\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 73, loss = 1.97438920\n",
      "Iteration 74, loss = 1.97436561\n",
      "Iteration 75, loss = 1.97426269\n",
      "Iteration 76, loss = 1.97431224\n",
      "Iteration 77, loss = 1.97433600\n",
      "Iteration 78, loss = 1.97434340\n",
      "Iteration 79, loss = 1.97429356\n",
      "Iteration 80, loss = 1.97433676\n",
      "Iteration 81, loss = 1.97432432\n",
      "Iteration 82, loss = 1.97431689\n",
      "Iteration 83, loss = 1.97429984\n",
      "Iteration 84, loss = 1.97430642\n",
      "Iteration 85, loss = 1.97432370\n",
      "Iteration 86, loss = 1.97431947\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 87, loss = 1.97400056\n",
      "Iteration 88, loss = 1.97399830\n",
      "Iteration 89, loss = 1.97399804\n",
      "Iteration 90, loss = 1.97399697\n",
      "Iteration 91, loss = 1.97399339\n",
      "Iteration 92, loss = 1.97398939\n",
      "Iteration 93, loss = 1.97399156\n",
      "Iteration 94, loss = 1.97399205\n",
      "Iteration 95, loss = 1.97399052\n",
      "Iteration 96, loss = 1.97399814\n",
      "Iteration 97, loss = 1.97399261\n",
      "Iteration 98, loss = 1.97399230\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.50952001\n",
      "Iteration 2, loss = 2.11889046\n",
      "Iteration 3, loss = 2.11720617\n",
      "Iteration 4, loss = 2.11339339\n",
      "Iteration 5, loss = 2.11155063\n",
      "Iteration 6, loss = 2.10599852\n",
      "Iteration 7, loss = 2.12210965\n",
      "Iteration 8, loss = 2.11185923\n",
      "Iteration 9, loss = 2.11826834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 2.11201262\n",
      "Iteration 11, loss = 2.11326078\n",
      "Iteration 12, loss = 2.11666438\n",
      "Iteration 13, loss = 2.11484062\n",
      "Iteration 14, loss = 2.11762416\n",
      "Iteration 15, loss = 2.12330717\n",
      "Iteration 16, loss = 2.12186928\n",
      "Iteration 17, loss = 2.11971847\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 18, loss = 2.01734822\n",
      "Iteration 19, loss = 2.00343217\n",
      "Iteration 20, loss = 2.00497918\n",
      "Iteration 21, loss = 2.00427185\n",
      "Iteration 22, loss = 2.00410824\n",
      "Iteration 23, loss = 2.00115702\n",
      "Iteration 24, loss = 2.00489599\n",
      "Iteration 25, loss = 2.00526859\n",
      "Iteration 26, loss = 2.00477720\n",
      "Iteration 27, loss = 2.00520447\n",
      "Iteration 28, loss = 2.00379925\n",
      "Iteration 29, loss = 2.00673266\n",
      "Iteration 30, loss = 2.00508739\n",
      "Iteration 31, loss = 2.00277557\n",
      "Iteration 32, loss = 2.00435682\n",
      "Iteration 33, loss = 2.00310338\n",
      "Iteration 34, loss = 2.00251622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 35, loss = 1.98587462\n",
      "Iteration 36, loss = 1.98023933\n",
      "Iteration 37, loss = 1.97872347\n",
      "Iteration 38, loss = 1.97922130\n",
      "Iteration 39, loss = 1.97874994\n",
      "Iteration 40, loss = 1.98001955\n",
      "Iteration 41, loss = 1.97815837\n",
      "Iteration 42, loss = 1.97861856\n",
      "Iteration 43, loss = 1.97875740\n",
      "Iteration 44, loss = 1.97983604\n",
      "Iteration 45, loss = 1.97935632\n",
      "Iteration 46, loss = 1.97907187\n",
      "Iteration 47, loss = 1.97918823\n",
      "Iteration 48, loss = 1.97862564\n",
      "Iteration 49, loss = 1.97906889\n",
      "Iteration 50, loss = 1.97908879\n",
      "Iteration 51, loss = 1.97942287\n",
      "Iteration 52, loss = 1.97885499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 53, loss = 1.97366903\n",
      "Iteration 54, loss = 1.97329488\n",
      "Iteration 55, loss = 1.97337370\n",
      "Iteration 56, loss = 1.97323317\n",
      "Iteration 57, loss = 1.97324599\n",
      "Iteration 58, loss = 1.97325394\n",
      "Iteration 59, loss = 1.97314202\n",
      "Iteration 60, loss = 1.97336489\n",
      "Iteration 61, loss = 1.97324886\n",
      "Iteration 62, loss = 1.97327679\n",
      "Iteration 63, loss = 1.97332804\n",
      "Iteration 64, loss = 1.97340679\n",
      "Iteration 65, loss = 1.97320349\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 66, loss = 1.97194085\n",
      "Iteration 67, loss = 1.97188645\n",
      "Iteration 68, loss = 1.97187773\n",
      "Iteration 69, loss = 1.97187794\n",
      "Iteration 70, loss = 1.97186121\n",
      "Iteration 71, loss = 1.97189238\n",
      "Iteration 72, loss = 1.97184898\n",
      "Iteration 73, loss = 1.97189028\n",
      "Iteration 74, loss = 1.97185514\n",
      "Iteration 75, loss = 1.97184231\n",
      "Iteration 76, loss = 1.97188850\n",
      "Iteration 77, loss = 1.97189855\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 78, loss = 1.97156296\n",
      "Iteration 79, loss = 1.97155502\n",
      "Iteration 80, loss = 1.97155654\n",
      "Iteration 81, loss = 1.97155418\n",
      "Iteration 82, loss = 1.97155655\n",
      "Iteration 83, loss = 1.97155131\n",
      "Iteration 84, loss = 1.97155235\n",
      "Iteration 85, loss = 1.97155190\n",
      "Iteration 86, loss = 1.97154918\n",
      "Iteration 87, loss = 1.97154881\n",
      "Iteration 88, loss = 1.97155701\n",
      "Iteration 89, loss = 1.97154896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.54227077\n",
      "Iteration 2, loss = 2.12782910\n",
      "Iteration 3, loss = 2.12276426\n",
      "Iteration 4, loss = 2.12127659\n",
      "Iteration 5, loss = 2.11608044\n",
      "Iteration 6, loss = 2.10398888\n",
      "Iteration 7, loss = 2.11886896\n",
      "Iteration 8, loss = 2.11521136\n",
      "Iteration 9, loss = 2.11248616\n",
      "Iteration 10, loss = 2.11876483\n",
      "Iteration 11, loss = 2.10375705\n",
      "Iteration 12, loss = 2.10013446\n",
      "Iteration 13, loss = 2.11442619\n",
      "Iteration 14, loss = 2.11418236\n",
      "Iteration 15, loss = 2.11430540\n",
      "Iteration 16, loss = 2.12383976\n",
      "Iteration 17, loss = 2.12125961\n",
      "Iteration 18, loss = 2.11285220\n",
      "Iteration 19, loss = 2.10637949\n",
      "Iteration 20, loss = 2.11296717\n",
      "Iteration 21, loss = 2.11021310\n",
      "Iteration 22, loss = 2.12247013\n",
      "Iteration 23, loss = 2.12124807\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 24, loss = 2.01250587\n",
      "Iteration 25, loss = 2.00204478\n",
      "Iteration 26, loss = 2.00104413\n",
      "Iteration 27, loss = 1.99868499\n",
      "Iteration 28, loss = 1.99828687\n",
      "Iteration 29, loss = 2.00056829\n",
      "Iteration 30, loss = 2.00168446\n",
      "Iteration 31, loss = 2.00143610\n",
      "Iteration 32, loss = 2.00009432\n",
      "Iteration 33, loss = 1.99958921\n",
      "Iteration 34, loss = 1.99845187\n",
      "Iteration 35, loss = 2.00294201\n",
      "Iteration 36, loss = 2.00092015\n",
      "Iteration 37, loss = 2.00286529\n",
      "Iteration 38, loss = 1.99884019\n",
      "Iteration 39, loss = 2.00055069\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 40, loss = 1.97926972\n",
      "Iteration 41, loss = 1.97596770\n",
      "Iteration 42, loss = 1.97439942\n",
      "Iteration 43, loss = 1.97521102\n",
      "Iteration 44, loss = 1.97550346\n",
      "Iteration 45, loss = 1.97485239\n",
      "Iteration 46, loss = 1.97514507\n",
      "Iteration 47, loss = 1.97491489\n",
      "Iteration 48, loss = 1.97464359\n",
      "Iteration 49, loss = 1.97486308\n",
      "Iteration 50, loss = 1.97538620\n",
      "Iteration 51, loss = 1.97485556\n",
      "Iteration 52, loss = 1.97545658\n",
      "Iteration 53, loss = 1.97522452\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 54, loss = 1.96964442\n",
      "Iteration 55, loss = 1.96964222\n",
      "Iteration 56, loss = 1.96953147\n",
      "Iteration 57, loss = 1.96925487\n",
      "Iteration 58, loss = 1.96949940\n",
      "Iteration 59, loss = 1.96937488\n",
      "Iteration 60, loss = 1.96949164\n",
      "Iteration 61, loss = 1.96948636\n",
      "Iteration 62, loss = 1.96900315\n",
      "Iteration 63, loss = 1.96916461\n",
      "Iteration 64, loss = 1.96944128\n",
      "Iteration 65, loss = 1.96952199\n",
      "Iteration 66, loss = 1.96943374\n",
      "Iteration 67, loss = 1.96947208\n",
      "Iteration 68, loss = 1.96945011\n",
      "Iteration 69, loss = 1.96925248\n",
      "Iteration 70, loss = 1.96916851\n",
      "Iteration 71, loss = 1.96963512\n",
      "Iteration 72, loss = 1.96943744\n",
      "Iteration 73, loss = 1.96949207\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 74, loss = 1.96814101\n",
      "Iteration 75, loss = 1.96801927\n",
      "Iteration 76, loss = 1.96800512\n",
      "Iteration 77, loss = 1.96804161\n",
      "Iteration 78, loss = 1.96798251\n",
      "Iteration 79, loss = 1.96801184\n",
      "Iteration 80, loss = 1.96801676\n",
      "Iteration 81, loss = 1.96803557\n",
      "Iteration 82, loss = 1.96803152\n",
      "Iteration 83, loss = 1.96799362\n",
      "Iteration 84, loss = 1.96802367\n",
      "Iteration 85, loss = 1.96801084\n",
      "Iteration 86, loss = 1.96803620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 87, loss = 1.96770564\n",
      "Iteration 88, loss = 1.96770392\n",
      "Iteration 89, loss = 1.96770265\n",
      "Iteration 90, loss = 1.96769853\n",
      "Iteration 91, loss = 1.96769624\n",
      "Iteration 92, loss = 1.96769228\n",
      "Iteration 93, loss = 1.96769830\n",
      "Iteration 94, loss = 1.96769244\n",
      "Iteration 95, loss = 1.96769483\n",
      "Iteration 96, loss = 1.96769175\n",
      "Iteration 97, loss = 1.96769815\n",
      "Iteration 98, loss = 1.96769250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.51969292\n",
      "Iteration 2, loss = 2.11974841\n",
      "Iteration 3, loss = 2.12312467\n",
      "Iteration 4, loss = 2.12225684\n",
      "Iteration 5, loss = 2.11429400\n",
      "Iteration 6, loss = 2.11668144\n",
      "Iteration 7, loss = 2.11412171\n",
      "Iteration 8, loss = 2.11991852\n",
      "Iteration 9, loss = 2.10951171\n",
      "Iteration 10, loss = 2.11035013\n",
      "Iteration 11, loss = 2.11744131\n",
      "Iteration 12, loss = 2.11029190\n",
      "Iteration 13, loss = 2.10985239\n",
      "Iteration 14, loss = 2.10870096\n",
      "Iteration 15, loss = 2.11067636\n",
      "Iteration 16, loss = 2.11669463\n",
      "Iteration 17, loss = 2.10871440\n",
      "Iteration 18, loss = 2.11837612\n",
      "Iteration 19, loss = 2.09759344\n",
      "Iteration 20, loss = 2.11256864\n",
      "Iteration 21, loss = 2.10957156\n",
      "Iteration 22, loss = 2.12038944\n",
      "Iteration 23, loss = 2.11463087\n",
      "Iteration 24, loss = 2.11717130\n",
      "Iteration 25, loss = 2.11014103\n",
      "Iteration 26, loss = 2.12232967\n",
      "Iteration 27, loss = 2.12625608\n",
      "Iteration 28, loss = 2.11679460\n",
      "Iteration 29, loss = 2.11302580\n",
      "Iteration 30, loss = 2.10603350\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 31, loss = 2.01073107\n",
      "Iteration 32, loss = 1.99697706\n",
      "Iteration 33, loss = 1.99531514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 1.99709234\n",
      "Iteration 35, loss = 1.99694562\n",
      "Iteration 36, loss = 1.99427794\n",
      "Iteration 37, loss = 1.99805739\n",
      "Iteration 38, loss = 1.99565379\n",
      "Iteration 39, loss = 1.99282647\n",
      "Iteration 40, loss = 1.99670754\n",
      "Iteration 41, loss = 1.99556904\n",
      "Iteration 42, loss = 1.99174775\n",
      "Iteration 43, loss = 1.99612730\n",
      "Iteration 44, loss = 1.99690320\n",
      "Iteration 45, loss = 1.99390559\n",
      "Iteration 46, loss = 1.99370851\n",
      "Iteration 47, loss = 1.99606119\n",
      "Iteration 48, loss = 1.99591962\n",
      "Iteration 49, loss = 1.99618794\n",
      "Iteration 50, loss = 1.99416942\n",
      "Iteration 51, loss = 1.99535188\n",
      "Iteration 52, loss = 1.99379421\n",
      "Iteration 53, loss = 1.99663193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 54, loss = 1.97321765\n",
      "Iteration 55, loss = 1.97100935\n",
      "Iteration 56, loss = 1.97047248\n",
      "Iteration 57, loss = 1.97035786\n",
      "Iteration 58, loss = 1.96985191\n",
      "Iteration 59, loss = 1.96996133\n",
      "Iteration 60, loss = 1.97075891\n",
      "Iteration 61, loss = 1.97044203\n",
      "Iteration 62, loss = 1.97095719\n",
      "Iteration 63, loss = 1.97062136\n",
      "Iteration 64, loss = 1.97054426\n",
      "Iteration 65, loss = 1.96985895\n",
      "Iteration 66, loss = 1.97023085\n",
      "Iteration 67, loss = 1.96899307\n",
      "Iteration 68, loss = 1.97070533\n",
      "Iteration 69, loss = 1.97076521\n",
      "Iteration 70, loss = 1.97025816\n",
      "Iteration 71, loss = 1.97136682\n",
      "Iteration 72, loss = 1.97015332\n",
      "Iteration 73, loss = 1.97030666\n",
      "Iteration 74, loss = 1.97106222\n",
      "Iteration 75, loss = 1.96976452\n",
      "Iteration 76, loss = 1.97013362\n",
      "Iteration 77, loss = 1.97042806\n",
      "Iteration 78, loss = 1.97049670\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 79, loss = 1.96511344\n",
      "Iteration 80, loss = 1.96448087\n",
      "Iteration 81, loss = 1.96463604\n",
      "Iteration 82, loss = 1.96427235\n",
      "Iteration 83, loss = 1.96428462\n",
      "Iteration 84, loss = 1.96444415\n",
      "Iteration 85, loss = 1.96409034\n",
      "Iteration 86, loss = 1.96438165\n",
      "Iteration 87, loss = 1.96447398\n",
      "Iteration 88, loss = 1.96441884\n",
      "Iteration 89, loss = 1.96440909\n",
      "Iteration 90, loss = 1.96429589\n",
      "Iteration 91, loss = 1.96446630\n",
      "Iteration 92, loss = 1.96432559\n",
      "Iteration 93, loss = 1.96466236\n",
      "Iteration 94, loss = 1.96452010\n",
      "Iteration 95, loss = 1.96439674\n",
      "Iteration 96, loss = 1.96443984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 97, loss = 1.96305998\n",
      "Iteration 98, loss = 1.96298001\n",
      "Iteration 99, loss = 1.96296078\n",
      "Iteration 100, loss = 1.96296555\n",
      "Iteration 101, loss = 1.96297911\n",
      "Iteration 102, loss = 1.96296204\n",
      "Iteration 103, loss = 1.96295749\n",
      "Iteration 104, loss = 1.96298566\n",
      "Iteration 105, loss = 1.96293744\n",
      "Iteration 106, loss = 1.96295248\n",
      "Iteration 107, loss = 1.96298268\n",
      "Iteration 108, loss = 1.96298449\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 109, loss = 1.96264288\n",
      "Iteration 110, loss = 1.96264211\n",
      "Iteration 111, loss = 1.96263587\n",
      "Iteration 112, loss = 1.96263433\n",
      "Iteration 113, loss = 1.96263508\n",
      "Iteration 114, loss = 1.96263245\n",
      "Iteration 115, loss = 1.96263373\n",
      "Iteration 116, loss = 1.96263339\n",
      "Iteration 117, loss = 1.96263192\n",
      "Iteration 118, loss = 1.96263086\n",
      "Iteration 119, loss = 1.96263461\n",
      "Iteration 120, loss = 1.96263074\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.62321826\n",
      "Iteration 2, loss = 2.12668994\n",
      "Iteration 3, loss = 2.11997143\n",
      "Iteration 4, loss = 2.11630930\n",
      "Iteration 5, loss = 2.11638940\n",
      "Iteration 6, loss = 2.11335610\n",
      "Iteration 7, loss = 2.11573077\n",
      "Iteration 8, loss = 2.11096619\n",
      "Iteration 9, loss = 2.10904032\n",
      "Iteration 10, loss = 2.12015104\n",
      "Iteration 11, loss = 2.12664904\n",
      "Iteration 12, loss = 2.10956359\n",
      "Iteration 13, loss = 2.11904352\n",
      "Iteration 14, loss = 2.10739898\n",
      "Iteration 15, loss = 2.11830864\n",
      "Iteration 16, loss = 2.11697181\n",
      "Iteration 17, loss = 2.10290245\n",
      "Iteration 18, loss = 2.10334457\n",
      "Iteration 19, loss = 2.11539813\n",
      "Iteration 20, loss = 2.10836936\n",
      "Iteration 21, loss = 2.11748181\n",
      "Iteration 22, loss = 2.11357135\n",
      "Iteration 23, loss = 2.11714412\n",
      "Iteration 24, loss = 2.10909513\n",
      "Iteration 25, loss = 2.11450828\n",
      "Iteration 26, loss = 2.09570680\n",
      "Iteration 27, loss = 2.10940268\n",
      "Iteration 28, loss = 2.11516482\n",
      "Iteration 29, loss = 2.11842749\n",
      "Iteration 30, loss = 2.10524342\n",
      "Iteration 31, loss = 2.11782215\n",
      "Iteration 32, loss = 2.11216763\n",
      "Iteration 33, loss = 2.11942387\n",
      "Iteration 34, loss = 2.11091398\n",
      "Iteration 35, loss = 2.11347171\n",
      "Iteration 36, loss = 2.11138814\n",
      "Iteration 37, loss = 2.11046670\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 38, loss = 2.00118185\n",
      "Iteration 39, loss = 1.99019523\n",
      "Iteration 40, loss = 1.99133174\n",
      "Iteration 41, loss = 1.99310857\n",
      "Iteration 42, loss = 1.99331972\n",
      "Iteration 43, loss = 1.99233446\n",
      "Iteration 44, loss = 1.99117067\n",
      "Iteration 45, loss = 1.99128369\n",
      "Iteration 46, loss = 1.99050666\n",
      "Iteration 47, loss = 1.99296407\n",
      "Iteration 48, loss = 1.99099273\n",
      "Iteration 49, loss = 1.99085048\n",
      "Iteration 50, loss = 1.99085766\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 51, loss = 1.96994003\n",
      "Iteration 52, loss = 1.96490242\n",
      "Iteration 53, loss = 1.96560770\n",
      "Iteration 54, loss = 1.96557122\n",
      "Iteration 55, loss = 1.96561374\n",
      "Iteration 56, loss = 1.96499260\n",
      "Iteration 57, loss = 1.96523456\n",
      "Iteration 58, loss = 1.96522584\n",
      "Iteration 59, loss = 1.96567323\n",
      "Iteration 60, loss = 1.96572625\n",
      "Iteration 61, loss = 1.96570330\n",
      "Iteration 62, loss = 1.96555184\n",
      "Iteration 63, loss = 1.96528398\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 64, loss = 1.95962894\n",
      "Iteration 65, loss = 1.95931561\n",
      "Iteration 66, loss = 1.95940094\n",
      "Iteration 67, loss = 1.95917151\n",
      "Iteration 68, loss = 1.95899566\n",
      "Iteration 69, loss = 1.95929463\n",
      "Iteration 70, loss = 1.95918298\n",
      "Iteration 71, loss = 1.95905738\n",
      "Iteration 72, loss = 1.95946869\n",
      "Iteration 73, loss = 1.95920688\n",
      "Iteration 74, loss = 1.95929187\n",
      "Iteration 75, loss = 1.95943298\n",
      "Iteration 76, loss = 1.95922661\n",
      "Iteration 77, loss = 1.95925890\n",
      "Iteration 78, loss = 1.95935100\n",
      "Iteration 79, loss = 1.95946397\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 80, loss = 1.95785112\n",
      "Iteration 81, loss = 1.95782770\n",
      "Iteration 82, loss = 1.95780413\n",
      "Iteration 83, loss = 1.95782998\n",
      "Iteration 84, loss = 1.95779467\n",
      "Iteration 85, loss = 1.95781859\n",
      "Iteration 86, loss = 1.95783202\n",
      "Iteration 87, loss = 1.95780505\n",
      "Iteration 88, loss = 1.95781360\n",
      "Iteration 89, loss = 1.95780679\n",
      "Iteration 90, loss = 1.95781832\n",
      "Iteration 91, loss = 1.95779638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 92, loss = 1.95748871\n",
      "Iteration 93, loss = 1.95748857\n",
      "Iteration 94, loss = 1.95748207\n",
      "Iteration 95, loss = 1.95748329\n",
      "Iteration 96, loss = 1.95748349\n",
      "Iteration 97, loss = 1.95748330\n",
      "Iteration 98, loss = 1.95748437\n",
      "Iteration 99, loss = 1.95747775\n",
      "Iteration 100, loss = 1.95748165\n",
      "Iteration 101, loss = 1.95747793\n",
      "Iteration 102, loss = 1.95747849\n",
      "Iteration 103, loss = 1.95747873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.57871227\n",
      "Iteration 2, loss = 2.11534871\n",
      "Iteration 3, loss = 2.10767850\n",
      "Iteration 4, loss = 2.11710888\n",
      "Iteration 5, loss = 2.11371494\n",
      "Iteration 6, loss = 2.10299032\n",
      "Iteration 7, loss = 2.10444432\n",
      "Iteration 8, loss = 2.11256921\n",
      "Iteration 9, loss = 2.10056727\n",
      "Iteration 10, loss = 2.09956699\n",
      "Iteration 11, loss = 2.11182784\n",
      "Iteration 12, loss = 2.10490505\n",
      "Iteration 13, loss = 2.11412074\n",
      "Iteration 14, loss = 2.10308599\n",
      "Iteration 15, loss = 2.10706539\n",
      "Iteration 16, loss = 2.11518820\n",
      "Iteration 17, loss = 2.11293963\n",
      "Iteration 18, loss = 2.10341457\n",
      "Iteration 19, loss = 2.10214896\n",
      "Iteration 20, loss = 2.10960679\n",
      "Iteration 21, loss = 2.10295020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 22, loss = 1.99465565\n",
      "Iteration 23, loss = 1.98447430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 1.98838214\n",
      "Iteration 25, loss = 1.98833548\n",
      "Iteration 26, loss = 1.98880767\n",
      "Iteration 27, loss = 1.98787366\n",
      "Iteration 28, loss = 1.98278939\n",
      "Iteration 29, loss = 1.98863247\n",
      "Iteration 30, loss = 1.98549428\n",
      "Iteration 31, loss = 1.98409997\n",
      "Iteration 32, loss = 1.98746099\n",
      "Iteration 33, loss = 1.98454978\n",
      "Iteration 34, loss = 1.98943175\n",
      "Iteration 35, loss = 1.98707132\n",
      "Iteration 36, loss = 1.98555448\n",
      "Iteration 37, loss = 1.98750570\n",
      "Iteration 38, loss = 1.98764184\n",
      "Iteration 39, loss = 1.98823953\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 40, loss = 1.96499822\n",
      "Iteration 41, loss = 1.96119217\n",
      "Iteration 42, loss = 1.96086805\n",
      "Iteration 43, loss = 1.96004501\n",
      "Iteration 44, loss = 1.96033678\n",
      "Iteration 45, loss = 1.96069907\n",
      "Iteration 46, loss = 1.96067282\n",
      "Iteration 47, loss = 1.96006674\n",
      "Iteration 48, loss = 1.95951299\n",
      "Iteration 49, loss = 1.95959677\n",
      "Iteration 50, loss = 1.96052277\n",
      "Iteration 51, loss = 1.96062168\n",
      "Iteration 52, loss = 1.95900120\n",
      "Iteration 53, loss = 1.95988465\n",
      "Iteration 54, loss = 1.96031204\n",
      "Iteration 55, loss = 1.95979067\n",
      "Iteration 56, loss = 1.95932544\n",
      "Iteration 57, loss = 1.96030684\n",
      "Iteration 58, loss = 1.95906728\n",
      "Iteration 59, loss = 1.96104383\n",
      "Iteration 60, loss = 1.95968816\n",
      "Iteration 61, loss = 1.96070382\n",
      "Iteration 62, loss = 1.95978028\n",
      "Iteration 63, loss = 1.95926649\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 64, loss = 1.95469976\n",
      "Iteration 65, loss = 1.95389612\n",
      "Iteration 66, loss = 1.95362164\n",
      "Iteration 67, loss = 1.95381312\n",
      "Iteration 68, loss = 1.95361210\n",
      "Iteration 69, loss = 1.95386435\n",
      "Iteration 70, loss = 1.95380694\n",
      "Iteration 71, loss = 1.95386071\n",
      "Iteration 72, loss = 1.95378049\n",
      "Iteration 73, loss = 1.95394195\n",
      "Iteration 74, loss = 1.95359805\n",
      "Iteration 75, loss = 1.95391828\n",
      "Iteration 76, loss = 1.95375171\n",
      "Iteration 77, loss = 1.95378499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 78, loss = 1.95245547\n",
      "Iteration 79, loss = 1.95230678\n",
      "Iteration 80, loss = 1.95230335\n",
      "Iteration 81, loss = 1.95225994\n",
      "Iteration 82, loss = 1.95225651\n",
      "Iteration 83, loss = 1.95228496\n",
      "Iteration 84, loss = 1.95224657\n",
      "Iteration 85, loss = 1.95229404\n",
      "Iteration 86, loss = 1.95223728\n",
      "Iteration 87, loss = 1.95226914\n",
      "Iteration 88, loss = 1.95224533\n",
      "Iteration 89, loss = 1.95226575\n",
      "Iteration 90, loss = 1.95227395\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 91, loss = 1.95192405\n",
      "Iteration 92, loss = 1.95192161\n",
      "Iteration 93, loss = 1.95191745\n",
      "Iteration 94, loss = 1.95191984\n",
      "Iteration 95, loss = 1.95192335\n",
      "Iteration 96, loss = 1.95191905\n",
      "Iteration 97, loss = 1.95191615\n",
      "Iteration 98, loss = 1.95192151\n",
      "Iteration 99, loss = 1.95191966\n",
      "Iteration 100, loss = 1.95192412\n",
      "Iteration 101, loss = 1.95192167\n",
      "Iteration 102, loss = 1.95191990\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.53405384\n",
      "Iteration 2, loss = 2.11848406\n",
      "Iteration 3, loss = 2.10122015\n",
      "Iteration 4, loss = 2.09773081\n",
      "Iteration 5, loss = 2.10658955\n",
      "Iteration 6, loss = 2.12564358\n",
      "Iteration 7, loss = 2.10824221\n",
      "Iteration 8, loss = 2.09744254\n",
      "Iteration 9, loss = 2.11692101\n",
      "Iteration 10, loss = 2.10205528\n",
      "Iteration 11, loss = 2.11052654\n",
      "Iteration 12, loss = 2.10887140\n",
      "Iteration 13, loss = 2.11940662\n",
      "Iteration 14, loss = 2.10577066\n",
      "Iteration 15, loss = 2.10780353\n",
      "Iteration 16, loss = 2.10090434\n",
      "Iteration 17, loss = 2.11030689\n",
      "Iteration 18, loss = 2.10263720\n",
      "Iteration 19, loss = 2.10377184\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 20, loss = 1.99734291\n",
      "Iteration 21, loss = 1.98619651\n",
      "Iteration 22, loss = 1.98324501\n",
      "Iteration 23, loss = 1.98317669\n",
      "Iteration 24, loss = 1.98555442\n",
      "Iteration 25, loss = 1.98452421\n",
      "Iteration 26, loss = 1.98411267\n",
      "Iteration 27, loss = 1.98159752\n",
      "Iteration 28, loss = 1.98322335\n",
      "Iteration 29, loss = 1.97968022\n",
      "Iteration 30, loss = 1.98714080\n",
      "Iteration 31, loss = 1.98451121\n",
      "Iteration 32, loss = 1.98369159\n",
      "Iteration 33, loss = 1.98452341\n",
      "Iteration 34, loss = 1.98371082\n",
      "Iteration 35, loss = 1.98398142\n",
      "Iteration 36, loss = 1.98401603\n",
      "Iteration 37, loss = 1.98333950\n",
      "Iteration 38, loss = 1.98565047\n",
      "Iteration 39, loss = 1.97983183\n",
      "Iteration 40, loss = 1.98209532\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 41, loss = 1.96111246\n",
      "Iteration 42, loss = 1.95778949\n",
      "Iteration 43, loss = 1.95650690\n",
      "Iteration 44, loss = 1.95704032\n",
      "Iteration 45, loss = 1.95668204\n",
      "Iteration 46, loss = 1.95658385\n",
      "Iteration 47, loss = 1.95620323\n",
      "Iteration 48, loss = 1.95633702\n",
      "Iteration 49, loss = 1.95603002\n",
      "Iteration 50, loss = 1.95662470\n",
      "Iteration 51, loss = 1.95677773\n",
      "Iteration 52, loss = 1.95664161\n",
      "Iteration 53, loss = 1.95566700\n",
      "Iteration 54, loss = 1.95533693\n",
      "Iteration 55, loss = 1.95654136\n",
      "Iteration 56, loss = 1.95609159\n",
      "Iteration 57, loss = 1.95608250\n",
      "Iteration 58, loss = 1.95661613\n",
      "Iteration 59, loss = 1.95591960\n",
      "Iteration 60, loss = 1.95632920\n",
      "Iteration 61, loss = 1.95582476\n",
      "Iteration 62, loss = 1.95726387\n",
      "Iteration 63, loss = 1.95663532\n",
      "Iteration 64, loss = 1.95555932\n",
      "Iteration 65, loss = 1.95626374\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 66, loss = 1.95068800\n",
      "Iteration 67, loss = 1.95011990\n",
      "Iteration 68, loss = 1.95004925\n",
      "Iteration 69, loss = 1.94995565\n",
      "Iteration 70, loss = 1.94978224\n",
      "Iteration 71, loss = 1.95004711\n",
      "Iteration 72, loss = 1.95005613\n",
      "Iteration 73, loss = 1.94965576\n",
      "Iteration 74, loss = 1.95024761\n",
      "Iteration 75, loss = 1.95008614\n",
      "Iteration 76, loss = 1.95007469\n",
      "Iteration 77, loss = 1.94995713\n",
      "Iteration 78, loss = 1.94999380\n",
      "Iteration 79, loss = 1.94997221\n",
      "Iteration 80, loss = 1.94979102\n",
      "Iteration 81, loss = 1.94986919\n",
      "Iteration 82, loss = 1.94980639\n",
      "Iteration 83, loss = 1.95019223\n",
      "Iteration 84, loss = 1.95016950\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 85, loss = 1.94862459\n",
      "Iteration 86, loss = 1.94850171\n",
      "Iteration 87, loss = 1.94853794\n",
      "Iteration 88, loss = 1.94852683\n",
      "Iteration 89, loss = 1.94852256\n",
      "Iteration 90, loss = 1.94852381\n",
      "Iteration 91, loss = 1.94853128\n",
      "Iteration 92, loss = 1.94853778\n",
      "Iteration 93, loss = 1.94855174\n",
      "Iteration 94, loss = 1.94850061\n",
      "Iteration 95, loss = 1.94852411\n",
      "Iteration 96, loss = 1.94853494\n",
      "Iteration 97, loss = 1.94853050\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 98, loss = 1.94818704\n",
      "Iteration 99, loss = 1.94818041\n",
      "Iteration 100, loss = 1.94818025\n",
      "Iteration 101, loss = 1.94817654\n",
      "Iteration 102, loss = 1.94816975\n",
      "Iteration 103, loss = 1.94817444\n",
      "Iteration 104, loss = 1.94817576\n",
      "Iteration 105, loss = 1.94817320\n",
      "Iteration 106, loss = 1.94817318\n",
      "Iteration 107, loss = 1.94817561\n",
      "Iteration 108, loss = 1.94817295\n",
      "Iteration 109, loss = 1.94817873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.58964517\n",
      "Iteration 2, loss = 2.10533222\n",
      "Iteration 3, loss = 2.10226531\n",
      "Iteration 4, loss = 2.11599469\n",
      "Iteration 5, loss = 2.09809322\n",
      "Iteration 6, loss = 2.10725275\n",
      "Iteration 7, loss = 2.10988174\n",
      "Iteration 8, loss = 2.12740805\n",
      "Iteration 9, loss = 2.10758435\n",
      "Iteration 10, loss = 2.10192518\n",
      "Iteration 11, loss = 2.11350880\n",
      "Iteration 12, loss = 2.10989837\n",
      "Iteration 13, loss = 2.10991867\n",
      "Iteration 14, loss = 2.10889801\n",
      "Iteration 15, loss = 2.10358093\n",
      "Iteration 16, loss = 2.10683410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 17, loss = 1.98855369\n",
      "Iteration 18, loss = 1.98382434\n",
      "Iteration 19, loss = 1.97829566\n",
      "Iteration 20, loss = 1.97587316\n",
      "Iteration 21, loss = 1.97980339\n",
      "Iteration 22, loss = 1.97932067\n",
      "Iteration 23, loss = 1.97726977\n",
      "Iteration 24, loss = 1.97750053\n",
      "Iteration 25, loss = 1.97670448\n",
      "Iteration 26, loss = 1.97526744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 1.97966391\n",
      "Iteration 28, loss = 1.97778937\n",
      "Iteration 29, loss = 1.97836120\n",
      "Iteration 30, loss = 1.98088985\n",
      "Iteration 31, loss = 1.97778663\n",
      "Iteration 32, loss = 1.97802484\n",
      "Iteration 33, loss = 1.97935379\n",
      "Iteration 34, loss = 1.97816143\n",
      "Iteration 35, loss = 1.97798217\n",
      "Iteration 36, loss = 1.98055498\n",
      "Iteration 37, loss = 1.97897599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 38, loss = 1.95627794\n",
      "Iteration 39, loss = 1.95229971\n",
      "Iteration 40, loss = 1.95096198\n",
      "Iteration 41, loss = 1.95203289\n",
      "Iteration 42, loss = 1.95105028\n",
      "Iteration 43, loss = 1.95142199\n",
      "Iteration 44, loss = 1.95142491\n",
      "Iteration 45, loss = 1.95109433\n",
      "Iteration 46, loss = 1.95088665\n",
      "Iteration 47, loss = 1.95066459\n",
      "Iteration 48, loss = 1.95219501\n",
      "Iteration 49, loss = 1.95222790\n",
      "Iteration 50, loss = 1.95240780\n",
      "Iteration 51, loss = 1.95267795\n",
      "Iteration 52, loss = 1.95114415\n",
      "Iteration 53, loss = 1.95087897\n",
      "Iteration 54, loss = 1.94994206\n",
      "Iteration 55, loss = 1.95196555\n",
      "Iteration 56, loss = 1.95115165\n",
      "Iteration 57, loss = 1.95180839\n",
      "Iteration 58, loss = 1.95167383\n",
      "Iteration 59, loss = 1.95142516\n",
      "Iteration 60, loss = 1.95240715\n",
      "Iteration 61, loss = 1.95172971\n",
      "Iteration 62, loss = 1.95180950\n",
      "Iteration 63, loss = 1.95152207\n",
      "Iteration 64, loss = 1.95209135\n",
      "Iteration 65, loss = 1.95139907\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 66, loss = 1.94560584\n",
      "Iteration 67, loss = 1.94539835\n",
      "Iteration 68, loss = 1.94519565\n",
      "Iteration 69, loss = 1.94517110\n",
      "Iteration 70, loss = 1.94510871\n",
      "Iteration 71, loss = 1.94474160\n",
      "Iteration 72, loss = 1.94516044\n",
      "Iteration 73, loss = 1.94521298\n",
      "Iteration 74, loss = 1.94485862\n",
      "Iteration 75, loss = 1.94500269\n",
      "Iteration 76, loss = 1.94509028\n",
      "Iteration 77, loss = 1.94490019\n",
      "Iteration 78, loss = 1.94468538\n",
      "Iteration 79, loss = 1.94525730\n",
      "Iteration 80, loss = 1.94512967\n",
      "Iteration 81, loss = 1.94498019\n",
      "Iteration 82, loss = 1.94520500\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 83, loss = 1.94372043\n",
      "Iteration 84, loss = 1.94360659\n",
      "Iteration 85, loss = 1.94353775\n",
      "Iteration 86, loss = 1.94354394\n",
      "Iteration 87, loss = 1.94358484\n",
      "Iteration 88, loss = 1.94355397\n",
      "Iteration 89, loss = 1.94355406\n",
      "Iteration 90, loss = 1.94355646\n",
      "Iteration 91, loss = 1.94357439\n",
      "Iteration 92, loss = 1.94355667\n",
      "Iteration 93, loss = 1.94358843\n",
      "Iteration 94, loss = 1.94351877\n",
      "Iteration 95, loss = 1.94356690\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 96, loss = 1.94321334\n",
      "Iteration 97, loss = 1.94321106\n",
      "Iteration 98, loss = 1.94321175\n",
      "Iteration 99, loss = 1.94320983\n",
      "Iteration 100, loss = 1.94320749\n",
      "Iteration 101, loss = 1.94320791\n",
      "Iteration 102, loss = 1.94320681\n",
      "Iteration 103, loss = 1.94320672\n",
      "Iteration 104, loss = 1.94320997\n",
      "Iteration 105, loss = 1.94320772\n",
      "Iteration 106, loss = 1.94320399\n",
      "Iteration 107, loss = 1.94321195\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.68080723\n",
      "Iteration 2, loss = 2.10997844\n",
      "Iteration 3, loss = 2.10893604\n",
      "Iteration 4, loss = 2.09972608\n",
      "Iteration 5, loss = 2.09495011\n",
      "Iteration 6, loss = 2.11980888\n",
      "Iteration 7, loss = 2.11061186\n",
      "Iteration 8, loss = 2.11208295\n",
      "Iteration 9, loss = 2.09512393\n",
      "Iteration 10, loss = 2.09509772\n",
      "Iteration 11, loss = 2.09855011\n",
      "Iteration 12, loss = 2.10842495\n",
      "Iteration 13, loss = 2.10081734\n",
      "Iteration 14, loss = 2.10631241\n",
      "Iteration 15, loss = 2.09645654\n",
      "Iteration 16, loss = 2.11072494\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 17, loss = 1.99473362\n",
      "Iteration 18, loss = 1.97757615\n",
      "Iteration 19, loss = 1.97796760\n",
      "Iteration 20, loss = 1.97887007\n",
      "Iteration 21, loss = 1.97870673\n",
      "Iteration 22, loss = 1.97594688\n",
      "Iteration 23, loss = 1.98099366\n",
      "Iteration 24, loss = 1.97732095\n",
      "Iteration 25, loss = 1.98144140\n",
      "Iteration 26, loss = 1.97655393\n",
      "Iteration 27, loss = 1.97572401\n",
      "Iteration 28, loss = 1.97826402\n",
      "Iteration 29, loss = 1.97689346\n",
      "Iteration 30, loss = 1.97336259\n",
      "Iteration 31, loss = 1.97948477\n",
      "Iteration 32, loss = 1.97789835\n",
      "Iteration 33, loss = 1.97544547\n",
      "Iteration 34, loss = 1.97600137\n",
      "Iteration 35, loss = 1.97494998\n",
      "Iteration 36, loss = 1.97608118\n",
      "Iteration 37, loss = 1.97660736\n",
      "Iteration 38, loss = 1.97733815\n",
      "Iteration 39, loss = 1.97369439\n",
      "Iteration 40, loss = 1.97288221\n",
      "Iteration 41, loss = 1.97429017\n",
      "Iteration 42, loss = 1.97814971\n",
      "Iteration 43, loss = 1.97896160\n",
      "Iteration 44, loss = 1.97679957\n",
      "Iteration 45, loss = 1.97587271\n",
      "Iteration 46, loss = 1.97509265\n",
      "Iteration 47, loss = 1.97879000\n",
      "Iteration 48, loss = 1.97487978\n",
      "Iteration 49, loss = 1.97691983\n",
      "Iteration 50, loss = 1.97415778\n",
      "Iteration 51, loss = 1.97668881\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 52, loss = 1.95335223\n",
      "Iteration 53, loss = 1.95053584\n",
      "Iteration 54, loss = 1.95046048\n",
      "Iteration 55, loss = 1.95014903\n",
      "Iteration 56, loss = 1.94990467\n",
      "Iteration 57, loss = 1.94969353\n",
      "Iteration 58, loss = 1.94900594\n",
      "Iteration 59, loss = 1.95050234\n",
      "Iteration 60, loss = 1.94941394\n",
      "Iteration 61, loss = 1.95067057\n",
      "Iteration 62, loss = 1.94973573\n",
      "Iteration 63, loss = 1.95009509\n",
      "Iteration 64, loss = 1.95024629\n",
      "Iteration 65, loss = 1.94985024\n",
      "Iteration 66, loss = 1.94894928\n",
      "Iteration 67, loss = 1.94928573\n",
      "Iteration 68, loss = 1.95054829\n",
      "Iteration 69, loss = 1.95058450\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 70, loss = 1.94445151\n",
      "Iteration 71, loss = 1.94330077\n",
      "Iteration 72, loss = 1.94365249\n",
      "Iteration 73, loss = 1.94363430\n",
      "Iteration 74, loss = 1.94340523\n",
      "Iteration 75, loss = 1.94352680\n",
      "Iteration 76, loss = 1.94342860\n",
      "Iteration 77, loss = 1.94326946\n",
      "Iteration 78, loss = 1.94351485\n",
      "Iteration 79, loss = 1.94341159\n",
      "Iteration 80, loss = 1.94361560\n",
      "Iteration 81, loss = 1.94343831\n",
      "Iteration 82, loss = 1.94354063\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 83, loss = 1.94205659\n",
      "Iteration 84, loss = 1.94197791\n",
      "Iteration 85, loss = 1.94191615\n",
      "Iteration 86, loss = 1.94195362\n",
      "Iteration 87, loss = 1.94186832\n",
      "Iteration 88, loss = 1.94194507\n",
      "Iteration 89, loss = 1.94197383\n",
      "Iteration 90, loss = 1.94194843\n",
      "Iteration 91, loss = 1.94194922\n",
      "Iteration 92, loss = 1.94194887\n",
      "Iteration 93, loss = 1.94192215\n",
      "Iteration 94, loss = 1.94191200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 95, loss = 1.94161660\n",
      "Iteration 96, loss = 1.94160043\n",
      "Iteration 97, loss = 1.94159205\n",
      "Iteration 98, loss = 1.94159427\n",
      "Iteration 99, loss = 1.94159519\n",
      "Iteration 100, loss = 1.94159354\n",
      "Iteration 101, loss = 1.94159342\n",
      "Iteration 102, loss = 1.94158756\n",
      "Iteration 103, loss = 1.94158595\n",
      "Iteration 104, loss = 1.94158659\n",
      "Iteration 105, loss = 1.94158919\n",
      "Iteration 106, loss = 1.94158688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.62018505\n",
      "Iteration 2, loss = 2.10350664\n",
      "Iteration 3, loss = 2.10141941\n",
      "Iteration 4, loss = 2.10032195\n",
      "Iteration 5, loss = 2.10776988\n",
      "Iteration 6, loss = 2.10902040\n",
      "Iteration 7, loss = 2.10271093\n",
      "Iteration 8, loss = 2.10240757\n",
      "Iteration 9, loss = 2.10307606\n",
      "Iteration 10, loss = 2.09861706\n",
      "Iteration 11, loss = 2.10052000\n",
      "Iteration 12, loss = 2.08732002\n",
      "Iteration 13, loss = 2.10417916\n",
      "Iteration 14, loss = 2.08799066\n",
      "Iteration 15, loss = 2.10327378\n",
      "Iteration 16, loss = 2.09543934\n",
      "Iteration 17, loss = 2.09268365\n",
      "Iteration 18, loss = 2.09508429\n",
      "Iteration 19, loss = 2.09752211\n",
      "Iteration 20, loss = 2.10126825\n",
      "Iteration 21, loss = 2.10592415\n",
      "Iteration 22, loss = 2.10526826\n",
      "Iteration 23, loss = 2.10509171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 24, loss = 1.97971205\n",
      "Iteration 25, loss = 1.97598372\n",
      "Iteration 26, loss = 1.97286772\n",
      "Iteration 27, loss = 1.97432462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 1.97329652\n",
      "Iteration 29, loss = 1.97285665\n",
      "Iteration 30, loss = 1.96994293\n",
      "Iteration 31, loss = 1.97225697\n",
      "Iteration 32, loss = 1.97078276\n",
      "Iteration 33, loss = 1.97204936\n",
      "Iteration 34, loss = 1.97335033\n",
      "Iteration 35, loss = 1.97108177\n",
      "Iteration 36, loss = 1.97279479\n",
      "Iteration 37, loss = 1.97400646\n",
      "Iteration 38, loss = 1.97275696\n",
      "Iteration 39, loss = 1.97429705\n",
      "Iteration 40, loss = 1.97341216\n",
      "Iteration 41, loss = 1.97409485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 42, loss = 1.94932320\n",
      "Iteration 43, loss = 1.94592496\n",
      "Iteration 44, loss = 1.94578874\n",
      "Iteration 45, loss = 1.94480845\n",
      "Iteration 46, loss = 1.94580192\n",
      "Iteration 47, loss = 1.94489876\n",
      "Iteration 48, loss = 1.94465296\n",
      "Iteration 49, loss = 1.94529833\n",
      "Iteration 50, loss = 1.94543610\n",
      "Iteration 51, loss = 1.94557357\n",
      "Iteration 52, loss = 1.94511085\n",
      "Iteration 53, loss = 1.94519077\n",
      "Iteration 54, loss = 1.94543508\n",
      "Iteration 55, loss = 1.94517497\n",
      "Iteration 56, loss = 1.94540749\n",
      "Iteration 57, loss = 1.94570670\n",
      "Iteration 58, loss = 1.94572376\n",
      "Iteration 59, loss = 1.94573813\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 60, loss = 1.93985407\n",
      "Iteration 61, loss = 1.93899689\n",
      "Iteration 62, loss = 1.93872942\n",
      "Iteration 63, loss = 1.93900654\n",
      "Iteration 64, loss = 1.93891210\n",
      "Iteration 65, loss = 1.93894136\n",
      "Iteration 66, loss = 1.93901513\n",
      "Iteration 67, loss = 1.93898443\n",
      "Iteration 68, loss = 1.93877307\n",
      "Iteration 69, loss = 1.93908929\n",
      "Iteration 70, loss = 1.93882327\n",
      "Iteration 71, loss = 1.93881889\n",
      "Iteration 72, loss = 1.93877598\n",
      "Iteration 73, loss = 1.93866706\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 74, loss = 1.93774220\n",
      "Iteration 75, loss = 1.93735503\n",
      "Iteration 76, loss = 1.93734064\n",
      "Iteration 77, loss = 1.93727758\n",
      "Iteration 78, loss = 1.93732926\n",
      "Iteration 79, loss = 1.93729532\n",
      "Iteration 80, loss = 1.93729261\n",
      "Iteration 81, loss = 1.93729106\n",
      "Iteration 82, loss = 1.93728682\n",
      "Iteration 83, loss = 1.93729470\n",
      "Iteration 84, loss = 1.93730291\n",
      "Iteration 85, loss = 1.93730175\n",
      "Iteration 86, loss = 1.93728423\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 87, loss = 1.93695173\n",
      "Iteration 88, loss = 1.93695105\n",
      "Iteration 89, loss = 1.93694739\n",
      "Iteration 90, loss = 1.93694274\n",
      "Iteration 91, loss = 1.93694182\n",
      "Iteration 92, loss = 1.93693888\n",
      "Iteration 93, loss = 1.93693695\n",
      "Iteration 94, loss = 1.93693839\n",
      "Iteration 95, loss = 1.93694275\n",
      "Iteration 96, loss = 1.93694211\n",
      "Iteration 97, loss = 1.93693803\n",
      "Iteration 98, loss = 1.93694068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.64697966\n",
      "Iteration 2, loss = 2.10995197\n",
      "Iteration 3, loss = 2.08608541\n",
      "Iteration 4, loss = 2.10381651\n",
      "Iteration 5, loss = 2.09614964\n",
      "Iteration 6, loss = 2.09633743\n",
      "Iteration 7, loss = 2.09250424\n",
      "Iteration 8, loss = 2.10946025\n",
      "Iteration 9, loss = 2.09445929\n",
      "Iteration 10, loss = 2.09539884\n",
      "Iteration 11, loss = 2.10223361\n",
      "Iteration 12, loss = 2.09956065\n",
      "Iteration 13, loss = 2.09602823\n",
      "Iteration 14, loss = 2.09527924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 15, loss = 1.97306934\n",
      "Iteration 16, loss = 1.96979206\n",
      "Iteration 17, loss = 1.96682016\n",
      "Iteration 18, loss = 1.96961453\n",
      "Iteration 19, loss = 1.96960320\n",
      "Iteration 20, loss = 1.96825150\n",
      "Iteration 21, loss = 1.97181282\n",
      "Iteration 22, loss = 1.96979233\n",
      "Iteration 23, loss = 1.97205647\n",
      "Iteration 24, loss = 1.96523927\n",
      "Iteration 25, loss = 1.96829364\n",
      "Iteration 26, loss = 1.96634515\n",
      "Iteration 27, loss = 1.96892569\n",
      "Iteration 28, loss = 1.96747470\n",
      "Iteration 29, loss = 1.96916608\n",
      "Iteration 30, loss = 1.96854306\n",
      "Iteration 31, loss = 1.96778567\n",
      "Iteration 32, loss = 1.96940022\n",
      "Iteration 33, loss = 1.96789836\n",
      "Iteration 34, loss = 1.96909425\n",
      "Iteration 35, loss = 1.97076453\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 36, loss = 1.94457731\n",
      "Iteration 37, loss = 1.94111189\n",
      "Iteration 38, loss = 1.94073021\n",
      "Iteration 39, loss = 1.94020676\n",
      "Iteration 40, loss = 1.94126830\n",
      "Iteration 41, loss = 1.94131005\n",
      "Iteration 42, loss = 1.94071487\n",
      "Iteration 43, loss = 1.94053006\n",
      "Iteration 44, loss = 1.94058469\n",
      "Iteration 45, loss = 1.94120029\n",
      "Iteration 46, loss = 1.94062852\n",
      "Iteration 47, loss = 1.94062632\n",
      "Iteration 48, loss = 1.94139107\n",
      "Iteration 49, loss = 1.94128550\n",
      "Iteration 50, loss = 1.94043008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 51, loss = 1.93599326\n",
      "Iteration 52, loss = 1.93456658\n",
      "Iteration 53, loss = 1.93421461\n",
      "Iteration 54, loss = 1.93441194\n",
      "Iteration 55, loss = 1.93450440\n",
      "Iteration 56, loss = 1.93445572\n",
      "Iteration 57, loss = 1.93442702\n",
      "Iteration 58, loss = 1.93457560\n",
      "Iteration 59, loss = 1.93431349\n",
      "Iteration 60, loss = 1.93377477\n",
      "Iteration 61, loss = 1.93438460\n",
      "Iteration 62, loss = 1.93461949\n",
      "Iteration 63, loss = 1.93462332\n",
      "Iteration 64, loss = 1.93434425\n",
      "Iteration 65, loss = 1.93438694\n",
      "Iteration 66, loss = 1.93442115\n",
      "Iteration 67, loss = 1.93422572\n",
      "Iteration 68, loss = 1.93428579\n",
      "Iteration 69, loss = 1.93451297\n",
      "Iteration 70, loss = 1.93446065\n",
      "Iteration 71, loss = 1.93417910\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 72, loss = 1.93304902\n",
      "Iteration 73, loss = 1.93285996\n",
      "Iteration 74, loss = 1.93284816\n",
      "Iteration 75, loss = 1.93285539\n",
      "Iteration 76, loss = 1.93282984\n",
      "Iteration 77, loss = 1.93286633\n",
      "Iteration 78, loss = 1.93283907\n",
      "Iteration 79, loss = 1.93285181\n",
      "Iteration 80, loss = 1.93283264\n",
      "Iteration 81, loss = 1.93281729\n",
      "Iteration 82, loss = 1.93282076\n",
      "Iteration 83, loss = 1.93280920\n",
      "Iteration 84, loss = 1.93286307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 85, loss = 1.93249253\n",
      "Iteration 86, loss = 1.93248866\n",
      "Iteration 87, loss = 1.93248330\n",
      "Iteration 88, loss = 1.93248688\n",
      "Iteration 89, loss = 1.93248482\n",
      "Iteration 90, loss = 1.93247739\n",
      "Iteration 91, loss = 1.93247816\n",
      "Iteration 92, loss = 1.93247505\n",
      "Iteration 93, loss = 1.93247916\n",
      "Iteration 94, loss = 1.93248429\n",
      "Iteration 95, loss = 1.93248244\n",
      "Iteration 96, loss = 1.93248374\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.72907382\n",
      "Iteration 2, loss = 2.09491980\n",
      "Iteration 3, loss = 2.09131244\n",
      "Iteration 4, loss = 2.09529878\n",
      "Iteration 5, loss = 2.09969742\n",
      "Iteration 6, loss = 2.09915220\n",
      "Iteration 7, loss = 2.07968849\n",
      "Iteration 8, loss = 2.09871303\n",
      "Iteration 9, loss = 2.10186004\n",
      "Iteration 10, loss = 2.09278209\n",
      "Iteration 11, loss = 2.09112662\n",
      "Iteration 12, loss = 2.08766011\n",
      "Iteration 13, loss = 2.09704507\n",
      "Iteration 14, loss = 2.10665104\n",
      "Iteration 15, loss = 2.07693559\n",
      "Iteration 16, loss = 2.09704702\n",
      "Iteration 17, loss = 2.09575943\n",
      "Iteration 18, loss = 2.09510766\n",
      "Iteration 19, loss = 2.08979524\n",
      "Iteration 20, loss = 2.10799660\n",
      "Iteration 21, loss = 2.09744876\n",
      "Iteration 22, loss = 2.09157862\n",
      "Iteration 23, loss = 2.09248434\n",
      "Iteration 24, loss = 2.09752364\n",
      "Iteration 25, loss = 2.09454381\n",
      "Iteration 26, loss = 2.09519529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 27, loss = 1.98011055\n",
      "Iteration 28, loss = 1.96472715\n",
      "Iteration 29, loss = 1.96638820\n",
      "Iteration 30, loss = 1.96758614\n",
      "Iteration 31, loss = 1.96593903\n",
      "Iteration 32, loss = 1.96412717\n",
      "Iteration 33, loss = 1.96605801\n",
      "Iteration 34, loss = 1.96308663\n",
      "Iteration 35, loss = 1.96634780\n",
      "Iteration 36, loss = 1.96869005\n",
      "Iteration 37, loss = 1.96675755\n",
      "Iteration 38, loss = 1.96634895\n",
      "Iteration 39, loss = 1.96473355\n",
      "Iteration 40, loss = 1.96653102\n",
      "Iteration 41, loss = 1.96464790\n",
      "Iteration 42, loss = 1.96505707\n",
      "Iteration 43, loss = 1.96692026\n",
      "Iteration 44, loss = 1.96604161\n",
      "Iteration 45, loss = 1.96400079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 1.94221941\n",
      "Iteration 47, loss = 1.93697984\n",
      "Iteration 48, loss = 1.93715115\n",
      "Iteration 49, loss = 1.93627315\n",
      "Iteration 50, loss = 1.93628462\n",
      "Iteration 51, loss = 1.93640583\n",
      "Iteration 52, loss = 1.93646837\n",
      "Iteration 53, loss = 1.93613113\n",
      "Iteration 54, loss = 1.93655218\n",
      "Iteration 55, loss = 1.93699001\n",
      "Iteration 56, loss = 1.93715748\n",
      "Iteration 57, loss = 1.93725183\n",
      "Iteration 58, loss = 1.93723099\n",
      "Iteration 59, loss = 1.93694473\n",
      "Iteration 60, loss = 1.93634281\n",
      "Iteration 61, loss = 1.93664009\n",
      "Iteration 62, loss = 1.93649839\n",
      "Iteration 63, loss = 1.93636510\n",
      "Iteration 64, loss = 1.93670016\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 65, loss = 1.93065647\n",
      "Iteration 66, loss = 1.93008292\n",
      "Iteration 67, loss = 1.93029234\n",
      "Iteration 68, loss = 1.93036489\n",
      "Iteration 69, loss = 1.93010331\n",
      "Iteration 70, loss = 1.93010544\n",
      "Iteration 71, loss = 1.93016479\n",
      "Iteration 72, loss = 1.92948476\n",
      "Iteration 73, loss = 1.93008604\n",
      "Iteration 74, loss = 1.92997905\n",
      "Iteration 75, loss = 1.93020316\n",
      "Iteration 76, loss = 1.92991728\n",
      "Iteration 77, loss = 1.93009199\n",
      "Iteration 78, loss = 1.93018951\n",
      "Iteration 79, loss = 1.93018244\n",
      "Iteration 80, loss = 1.92993175\n",
      "Iteration 81, loss = 1.93008969\n",
      "Iteration 82, loss = 1.93003624\n",
      "Iteration 83, loss = 1.93012205\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 84, loss = 1.92871861\n",
      "Iteration 85, loss = 1.92855344\n",
      "Iteration 86, loss = 1.92848188\n",
      "Iteration 87, loss = 1.92851188\n",
      "Iteration 88, loss = 1.92848757\n",
      "Iteration 89, loss = 1.92850706\n",
      "Iteration 90, loss = 1.92849202\n",
      "Iteration 91, loss = 1.92850710\n",
      "Iteration 92, loss = 1.92849950\n",
      "Iteration 93, loss = 1.92848323\n",
      "Iteration 94, loss = 1.92847815\n",
      "Iteration 95, loss = 1.92849412\n",
      "Iteration 96, loss = 1.92848387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 97, loss = 1.92815486\n",
      "Iteration 98, loss = 1.92813977\n",
      "Iteration 99, loss = 1.92813909\n",
      "Iteration 100, loss = 1.92813202\n",
      "Iteration 101, loss = 1.92813369\n",
      "Iteration 102, loss = 1.92813192\n",
      "Iteration 103, loss = 1.92812951\n",
      "Iteration 104, loss = 1.92813543\n",
      "Iteration 105, loss = 1.92813160\n",
      "Iteration 106, loss = 1.92812719\n",
      "Iteration 107, loss = 1.92813409\n",
      "Iteration 108, loss = 1.92813401\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.74522604\n",
      "Iteration 2, loss = 2.09512092\n",
      "Iteration 3, loss = 2.10395690\n",
      "Iteration 4, loss = 2.09275187\n",
      "Iteration 5, loss = 2.08621729\n",
      "Iteration 6, loss = 2.10469358\n",
      "Iteration 7, loss = 2.09184568\n",
      "Iteration 8, loss = 2.08508141\n",
      "Iteration 9, loss = 2.09898150\n",
      "Iteration 10, loss = 2.09198434\n",
      "Iteration 11, loss = 2.08372853\n",
      "Iteration 12, loss = 2.10029235\n",
      "Iteration 13, loss = 2.08657961\n",
      "Iteration 14, loss = 2.09226711\n",
      "Iteration 15, loss = 2.09002451\n",
      "Iteration 16, loss = 2.09755798\n",
      "Iteration 17, loss = 2.09375193\n",
      "Iteration 18, loss = 2.10291384\n",
      "Iteration 19, loss = 2.09214191\n",
      "Iteration 20, loss = 2.10015477\n",
      "Iteration 21, loss = 2.09492831\n",
      "Iteration 22, loss = 2.10422696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 23, loss = 1.97560384\n",
      "Iteration 24, loss = 1.96520070\n",
      "Iteration 25, loss = 1.96276355\n",
      "Iteration 26, loss = 1.95953789\n",
      "Iteration 27, loss = 1.96023170\n",
      "Iteration 28, loss = 1.96058443\n",
      "Iteration 29, loss = 1.96197512\n",
      "Iteration 30, loss = 1.96047120\n",
      "Iteration 31, loss = 1.96231346\n",
      "Iteration 32, loss = 1.96028723\n",
      "Iteration 33, loss = 1.95986820\n",
      "Iteration 34, loss = 1.96447750\n",
      "Iteration 35, loss = 1.95914601\n",
      "Iteration 36, loss = 1.96182051\n",
      "Iteration 37, loss = 1.95919969\n",
      "Iteration 38, loss = 1.96037608\n",
      "Iteration 39, loss = 1.96080374\n",
      "Iteration 40, loss = 1.96019298\n",
      "Iteration 41, loss = 1.96220999\n",
      "Iteration 42, loss = 1.95837817\n",
      "Iteration 43, loss = 1.96372364\n",
      "Iteration 44, loss = 1.95988869\n",
      "Iteration 45, loss = 1.96188066\n",
      "Iteration 46, loss = 1.96420393\n",
      "Iteration 47, loss = 1.96451161\n",
      "Iteration 48, loss = 1.96270697\n",
      "Iteration 49, loss = 1.96202265\n",
      "Iteration 50, loss = 1.96073352\n",
      "Iteration 51, loss = 1.95787287\n",
      "Iteration 52, loss = 1.96191087\n",
      "Iteration 53, loss = 1.96212561\n",
      "Iteration 54, loss = 1.96448693\n",
      "Iteration 55, loss = 1.96259245\n",
      "Iteration 56, loss = 1.96070946\n",
      "Iteration 57, loss = 1.95820257\n",
      "Iteration 58, loss = 1.96235139\n",
      "Iteration 59, loss = 1.96335607\n",
      "Iteration 60, loss = 1.96059889\n",
      "Iteration 61, loss = 1.96257274\n",
      "Iteration 62, loss = 1.96146844\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 63, loss = 1.93752449\n",
      "Iteration 64, loss = 1.93352023\n",
      "Iteration 65, loss = 1.93250972\n",
      "Iteration 66, loss = 1.93253252\n",
      "Iteration 67, loss = 1.93301720\n",
      "Iteration 68, loss = 1.93379117\n",
      "Iteration 69, loss = 1.93317243\n",
      "Iteration 70, loss = 1.93362623\n",
      "Iteration 71, loss = 1.93229452\n",
      "Iteration 72, loss = 1.93304410\n",
      "Iteration 73, loss = 1.93308443\n",
      "Iteration 74, loss = 1.93296515\n",
      "Iteration 75, loss = 1.93249805\n",
      "Iteration 76, loss = 1.93203827\n",
      "Iteration 77, loss = 1.93280409\n",
      "Iteration 78, loss = 1.93275694\n",
      "Iteration 79, loss = 1.93331182\n",
      "Iteration 80, loss = 1.93295622\n",
      "Iteration 81, loss = 1.93336567\n",
      "Iteration 82, loss = 1.93203891\n",
      "Iteration 83, loss = 1.93337150\n",
      "Iteration 84, loss = 1.93305786\n",
      "Iteration 85, loss = 1.93308325\n",
      "Iteration 86, loss = 1.93266498\n",
      "Iteration 87, loss = 1.93407563\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 88, loss = 1.92603510\n",
      "Iteration 89, loss = 1.92640268\n",
      "Iteration 90, loss = 1.92619411\n",
      "Iteration 91, loss = 1.92593825\n",
      "Iteration 92, loss = 1.92620614\n",
      "Iteration 93, loss = 1.92591370\n",
      "Iteration 94, loss = 1.92636768\n",
      "Iteration 95, loss = 1.92565896\n",
      "Iteration 96, loss = 1.92611198\n",
      "Iteration 97, loss = 1.92599253\n",
      "Iteration 98, loss = 1.92594876\n",
      "Iteration 99, loss = 1.92588840\n",
      "Iteration 100, loss = 1.92590484\n",
      "Iteration 101, loss = 1.92592168\n",
      "Iteration 102, loss = 1.92606007\n",
      "Iteration 103, loss = 1.92627205\n",
      "Iteration 104, loss = 1.92596055\n",
      "Iteration 105, loss = 1.92623075\n",
      "Iteration 106, loss = 1.92607008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 107, loss = 1.92469956\n",
      "Iteration 108, loss = 1.92455406\n",
      "Iteration 109, loss = 1.92449510\n",
      "Iteration 110, loss = 1.92450482\n",
      "Iteration 111, loss = 1.92449318\n",
      "Iteration 112, loss = 1.92452991\n",
      "Iteration 113, loss = 1.92450848\n",
      "Iteration 114, loss = 1.92450838\n",
      "Iteration 115, loss = 1.92448376\n",
      "Iteration 116, loss = 1.92450185\n",
      "Iteration 117, loss = 1.92452580\n",
      "Iteration 118, loss = 1.92447632\n",
      "Iteration 119, loss = 1.92449181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 120, loss = 1.92413107\n",
      "Iteration 121, loss = 1.92413273\n",
      "Iteration 122, loss = 1.92412436\n",
      "Iteration 123, loss = 1.92411921\n",
      "Iteration 124, loss = 1.92412239\n",
      "Iteration 125, loss = 1.92411961\n",
      "Iteration 126, loss = 1.92411860\n",
      "Iteration 127, loss = 1.92412089\n",
      "Iteration 128, loss = 1.92412536\n",
      "Iteration 129, loss = 1.92411798\n",
      "Iteration 130, loss = 1.92412135\n",
      "Iteration 131, loss = 1.92411890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.69409281\n",
      "Iteration 2, loss = 2.09454551\n",
      "Iteration 3, loss = 2.09273982\n",
      "Iteration 4, loss = 2.10015344\n",
      "Iteration 5, loss = 2.09357182\n",
      "Iteration 6, loss = 2.09025467\n",
      "Iteration 7, loss = 2.10020009\n",
      "Iteration 8, loss = 2.09413504\n",
      "Iteration 9, loss = 2.07810238\n",
      "Iteration 10, loss = 2.08408322\n",
      "Iteration 11, loss = 2.08649788\n",
      "Iteration 12, loss = 2.09642043\n",
      "Iteration 13, loss = 2.09280457\n",
      "Iteration 14, loss = 2.09754393\n",
      "Iteration 15, loss = 2.09024767\n",
      "Iteration 16, loss = 2.09692048\n",
      "Iteration 17, loss = 2.09370864\n",
      "Iteration 18, loss = 2.09194923\n",
      "Iteration 19, loss = 2.09788122\n",
      "Iteration 20, loss = 2.09454726\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 21, loss = 1.97293186\n",
      "Iteration 22, loss = 1.96247572\n",
      "Iteration 23, loss = 1.95767438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 1.95663333\n",
      "Iteration 25, loss = 1.95841653\n",
      "Iteration 26, loss = 1.96106202\n",
      "Iteration 27, loss = 1.95857336\n",
      "Iteration 28, loss = 1.95906066\n",
      "Iteration 29, loss = 1.95838300\n",
      "Iteration 30, loss = 1.95936773\n",
      "Iteration 31, loss = 1.95624872\n",
      "Iteration 32, loss = 1.96069534\n",
      "Iteration 33, loss = 1.95870472\n",
      "Iteration 34, loss = 1.95654679\n",
      "Iteration 35, loss = 1.96167135\n",
      "Iteration 36, loss = 1.95775164\n",
      "Iteration 37, loss = 1.95775096\n",
      "Iteration 38, loss = 1.95838794\n",
      "Iteration 39, loss = 1.95887276\n",
      "Iteration 40, loss = 1.95953121\n",
      "Iteration 41, loss = 1.95862651\n",
      "Iteration 42, loss = 1.95704220\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 43, loss = 1.93521585\n",
      "Iteration 44, loss = 1.93025974\n",
      "Iteration 45, loss = 1.93074314\n",
      "Iteration 46, loss = 1.93070583\n",
      "Iteration 47, loss = 1.92876150\n",
      "Iteration 48, loss = 1.93084791\n",
      "Iteration 49, loss = 1.93000231\n",
      "Iteration 50, loss = 1.92986973\n",
      "Iteration 51, loss = 1.93045289\n",
      "Iteration 52, loss = 1.92998129\n",
      "Iteration 53, loss = 1.93012639\n",
      "Iteration 54, loss = 1.93016736\n",
      "Iteration 55, loss = 1.92932477\n",
      "Iteration 56, loss = 1.92996646\n",
      "Iteration 57, loss = 1.92917491\n",
      "Iteration 58, loss = 1.92938649\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 59, loss = 1.92393799\n",
      "Iteration 60, loss = 1.92321412\n",
      "Iteration 61, loss = 1.92288826\n",
      "Iteration 62, loss = 1.92320754\n",
      "Iteration 63, loss = 1.92310208\n",
      "Iteration 64, loss = 1.92301914\n",
      "Iteration 65, loss = 1.92305496\n",
      "Iteration 66, loss = 1.92302243\n",
      "Iteration 67, loss = 1.92337887\n",
      "Iteration 68, loss = 1.92340293\n",
      "Iteration 69, loss = 1.92300655\n",
      "Iteration 70, loss = 1.92311896\n",
      "Iteration 71, loss = 1.92274986\n",
      "Iteration 72, loss = 1.92306323\n",
      "Iteration 73, loss = 1.92294306\n",
      "Iteration 74, loss = 1.92310450\n",
      "Iteration 75, loss = 1.92307710\n",
      "Iteration 76, loss = 1.92292591\n",
      "Iteration 77, loss = 1.92294480\n",
      "Iteration 78, loss = 1.92328412\n",
      "Iteration 79, loss = 1.92313753\n",
      "Iteration 80, loss = 1.92314035\n",
      "Iteration 81, loss = 1.92313864\n",
      "Iteration 82, loss = 1.92309637\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 83, loss = 1.92167115\n",
      "Iteration 84, loss = 1.92149655\n",
      "Iteration 85, loss = 1.92145976\n",
      "Iteration 86, loss = 1.92149779\n",
      "Iteration 87, loss = 1.92147020\n",
      "Iteration 88, loss = 1.92148345\n",
      "Iteration 89, loss = 1.92150715\n",
      "Iteration 90, loss = 1.92149106\n",
      "Iteration 91, loss = 1.92149336\n",
      "Iteration 92, loss = 1.92147822\n",
      "Iteration 93, loss = 1.92148026\n",
      "Iteration 94, loss = 1.92149016\n",
      "Iteration 95, loss = 1.92145638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 96, loss = 1.92111832\n",
      "Iteration 97, loss = 1.92111813\n",
      "Iteration 98, loss = 1.92111790\n",
      "Iteration 99, loss = 1.92111271\n",
      "Iteration 100, loss = 1.92110870\n",
      "Iteration 101, loss = 1.92110658\n",
      "Iteration 102, loss = 1.92110797\n",
      "Iteration 103, loss = 1.92110428\n",
      "Iteration 104, loss = 1.92110804\n",
      "Iteration 105, loss = 1.92110645\n",
      "Iteration 106, loss = 1.92111005\n",
      "Iteration 107, loss = 1.92110501\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3.71166865\n",
      "Iteration 2, loss = 2.10132021\n",
      "Iteration 3, loss = 2.09720288\n",
      "Iteration 4, loss = 2.08893980\n",
      "Iteration 5, loss = 2.09613681\n",
      "Iteration 6, loss = 2.09529378\n",
      "Iteration 7, loss = 2.09886470\n",
      "Iteration 8, loss = 2.09049844\n",
      "Iteration 9, loss = 2.09179940\n",
      "Iteration 10, loss = 2.08203776\n",
      "Iteration 11, loss = 2.08983802\n",
      "Iteration 12, loss = 2.09016171\n",
      "Iteration 13, loss = 2.10255226\n",
      "Iteration 14, loss = 2.09316378\n",
      "Iteration 15, loss = 2.08529346\n",
      "Iteration 16, loss = 2.08452951\n",
      "Iteration 17, loss = 2.09101491\n",
      "Iteration 18, loss = 2.08647533\n",
      "Iteration 19, loss = 2.09206766\n",
      "Iteration 20, loss = 2.08057977\n",
      "Iteration 21, loss = 2.09310853\n",
      "Iteration 22, loss = 2.09837324\n",
      "Iteration 23, loss = 2.08483125\n",
      "Iteration 24, loss = 2.08140758\n",
      "Iteration 25, loss = 2.08565734\n",
      "Iteration 26, loss = 2.07351898\n",
      "Iteration 27, loss = 2.07986237\n",
      "Iteration 28, loss = 2.08359981\n",
      "Iteration 29, loss = 2.10003145\n",
      "Iteration 30, loss = 2.09309950\n",
      "Iteration 31, loss = 2.09378135\n",
      "Iteration 32, loss = 2.07770076\n",
      "Iteration 33, loss = 2.08933309\n",
      "Iteration 34, loss = 2.09244523\n",
      "Iteration 35, loss = 2.08264932\n",
      "Iteration 36, loss = 2.09025249\n",
      "Iteration 37, loss = 2.08847223\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 38, loss = 1.97214936\n",
      "Iteration 39, loss = 1.95625413\n",
      "Iteration 40, loss = 1.95511475\n",
      "Iteration 41, loss = 1.95937975\n",
      "Iteration 42, loss = 1.95604149\n",
      "Iteration 43, loss = 1.95487230\n",
      "Iteration 44, loss = 1.95710506\n",
      "Iteration 45, loss = 1.95661728\n",
      "Iteration 46, loss = 1.95631957\n",
      "Iteration 47, loss = 1.95653025\n",
      "Iteration 48, loss = 1.95740022\n",
      "Iteration 49, loss = 1.95804771\n",
      "Iteration 50, loss = 1.96000905\n",
      "Iteration 51, loss = 1.95236726\n",
      "Iteration 52, loss = 1.95640288\n",
      "Iteration 53, loss = 1.95716102\n",
      "Iteration 54, loss = 1.95820403\n",
      "Iteration 55, loss = 1.95409033\n",
      "Iteration 56, loss = 1.95605839\n",
      "Iteration 57, loss = 1.95878173\n",
      "Iteration 58, loss = 1.95696367\n",
      "Iteration 59, loss = 1.95586524\n",
      "Iteration 60, loss = 1.95599577\n",
      "Iteration 61, loss = 1.95561794\n",
      "Iteration 62, loss = 1.95409872\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 63, loss = 1.93057294\n",
      "Iteration 64, loss = 1.92841298\n",
      "Iteration 65, loss = 1.92694716\n",
      "Iteration 66, loss = 1.92734671\n",
      "Iteration 67, loss = 1.92691011\n",
      "Iteration 68, loss = 1.92809161\n",
      "Iteration 69, loss = 1.92746826\n",
      "Iteration 70, loss = 1.92719762\n",
      "Iteration 71, loss = 1.92618058\n",
      "Iteration 72, loss = 1.92735531\n",
      "Iteration 73, loss = 1.92719407\n",
      "Iteration 74, loss = 1.92800757\n",
      "Iteration 75, loss = 1.92712984\n",
      "Iteration 76, loss = 1.92763271\n",
      "Iteration 77, loss = 1.92653181\n",
      "Iteration 78, loss = 1.92776401\n",
      "Iteration 79, loss = 1.92770471\n",
      "Iteration 80, loss = 1.92771399\n",
      "Iteration 81, loss = 1.92628633\n",
      "Iteration 82, loss = 1.92724970\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 83, loss = 1.92177704\n",
      "Iteration 84, loss = 1.92068543\n",
      "Iteration 85, loss = 1.92052285\n",
      "Iteration 86, loss = 1.92045434\n",
      "Iteration 87, loss = 1.92053060\n",
      "Iteration 88, loss = 1.92050880\n",
      "Iteration 89, loss = 1.92048673\n",
      "Iteration 90, loss = 1.92063359\n",
      "Iteration 91, loss = 1.92061167\n",
      "Iteration 92, loss = 1.92048650\n",
      "Iteration 93, loss = 1.92054407\n",
      "Iteration 94, loss = 1.92064770\n",
      "Iteration 95, loss = 1.92051048\n",
      "Iteration 96, loss = 1.92049362\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 97, loss = 1.91913159\n",
      "Iteration 98, loss = 1.91898221\n",
      "Iteration 99, loss = 1.91892442\n",
      "Iteration 100, loss = 1.91895537\n",
      "Iteration 101, loss = 1.91892899\n",
      "Iteration 102, loss = 1.91889703\n",
      "Iteration 103, loss = 1.91891554\n",
      "Iteration 104, loss = 1.91890671\n",
      "Iteration 105, loss = 1.91894116\n",
      "Iteration 106, loss = 1.91892508\n",
      "Iteration 107, loss = 1.91889524\n",
      "Iteration 108, loss = 1.91887205\n",
      "Iteration 109, loss = 1.91888583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 110, loss = 1.91854211\n",
      "Iteration 111, loss = 1.91853754\n",
      "Iteration 112, loss = 1.91853788\n",
      "Iteration 113, loss = 1.91852899\n",
      "Iteration 114, loss = 1.91853282\n",
      "Iteration 115, loss = 1.91853499\n",
      "Iteration 116, loss = 1.91853073\n",
      "Iteration 117, loss = 1.91852211\n",
      "Iteration 118, loss = 1.91852609\n",
      "Iteration 119, loss = 1.91852800\n",
      "Iteration 120, loss = 1.91852879\n",
      "Iteration 121, loss = 1.91852820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dc7c974700>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clfArray = []\n",
    "clfResults = []\n",
    "for i in range (1,50):\n",
    "    clfArray.append(MLPClassifier(hidden_layer_sizes=i, activation='tanh', solver='sgd', batch_size=1, alpha=1, learning_rate='adaptive', verbose=1))\n",
    "    clfArray[i-1].fit(X_train,y_train)\n",
    "    clfResults.append(clfArray[i-1].score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence du nombre de neurones de la couche cachée sur le taux de reconnaissance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dc03973fd0>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.title(\"Influence du nombre de neurones de la couche cachée sur le taux de reconnaissance\")\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('nombre de neurones de la couche cachée')\n",
    "plt.annotate('méthode du coude : nombre de neurones = 10, score = ' + str(clfResults[10]) , xy= (10,0.68), xytext=( 20,0.5) ,arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "\n",
    "plt.plot(list(range(1,50)),clfResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.88915254\n",
      "Iteration 2, loss = 2.22628645\n",
      "Iteration 3, loss = 2.22348970\n",
      "Iteration 4, loss = 2.22219160\n",
      "Iteration 5, loss = 2.22283714\n",
      "Iteration 6, loss = 2.22155834\n",
      "Iteration 7, loss = 2.22253966\n",
      "Iteration 8, loss = 2.22426942\n",
      "Iteration 9, loss = 2.22437272\n",
      "Iteration 10, loss = 2.21829154\n",
      "Iteration 11, loss = 2.22446723\n",
      "Iteration 12, loss = 2.22038185\n",
      "Iteration 13, loss = 2.22275631\n",
      "Iteration 14, loss = 2.21817294\n",
      "Iteration 15, loss = 2.22217025\n",
      "Iteration 16, loss = 2.22428062\n",
      "Iteration 17, loss = 2.22473520\n",
      "Iteration 18, loss = 2.22111026\n",
      "Iteration 19, loss = 2.22290786\n",
      "Iteration 20, loss = 2.22178390\n",
      "Iteration 21, loss = 2.21787305\n",
      "Iteration 22, loss = 2.22101235\n",
      "Iteration 23, loss = 2.22335784\n",
      "Iteration 24, loss = 2.22265299\n",
      "Iteration 25, loss = 2.22185218\n",
      "Iteration 26, loss = 2.21628130\n",
      "Iteration 27, loss = 2.22427480\n",
      "Iteration 28, loss = 2.21708072\n",
      "Iteration 29, loss = 2.22332120\n",
      "Iteration 30, loss = 2.22115042\n",
      "Iteration 31, loss = 2.22288292\n",
      "Iteration 32, loss = 2.21558398\n",
      "Iteration 33, loss = 2.22051744\n",
      "Iteration 34, loss = 2.21691775\n",
      "Iteration 35, loss = 2.22566957\n",
      "Iteration 36, loss = 2.22113158\n",
      "Iteration 37, loss = 2.21927576\n",
      "Iteration 38, loss = 2.21684276\n",
      "Iteration 39, loss = 2.21814233\n",
      "Iteration 40, loss = 2.21435290\n",
      "Iteration 41, loss = 2.21871398\n",
      "Iteration 42, loss = 2.22003941\n",
      "Iteration 43, loss = 2.21651509\n",
      "Iteration 44, loss = 2.22114020\n",
      "Iteration 45, loss = 2.22027969\n",
      "Iteration 46, loss = 2.22063477\n",
      "Iteration 47, loss = 2.22404380\n",
      "Iteration 48, loss = 2.22178537\n",
      "Iteration 49, loss = 2.21976127\n",
      "Iteration 50, loss = 2.21726168\n",
      "Iteration 51, loss = 2.22348823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 52, loss = 2.17587320\n",
      "Iteration 53, loss = 2.17288724\n",
      "Iteration 54, loss = 2.17333850\n",
      "Iteration 55, loss = 2.17346763\n",
      "Iteration 56, loss = 2.17243859\n",
      "Iteration 57, loss = 2.17268503\n",
      "Iteration 58, loss = 2.17287923\n",
      "Iteration 59, loss = 2.17199007\n",
      "Iteration 60, loss = 2.17239506\n",
      "Iteration 61, loss = 2.17234355\n",
      "Iteration 62, loss = 2.17241597\n",
      "Iteration 63, loss = 2.17311257\n",
      "Iteration 64, loss = 2.17183240\n",
      "Iteration 65, loss = 2.17316031\n",
      "Iteration 66, loss = 2.17178260\n",
      "Iteration 67, loss = 2.17204130\n",
      "Iteration 68, loss = 2.17214381\n",
      "Iteration 69, loss = 2.17261902\n",
      "Iteration 70, loss = 2.17106213\n",
      "Iteration 71, loss = 2.17152728\n",
      "Iteration 72, loss = 2.17143702\n",
      "Iteration 73, loss = 2.17204439\n",
      "Iteration 74, loss = 2.17267768\n",
      "Iteration 75, loss = 2.17217549\n",
      "Iteration 76, loss = 2.17268648\n",
      "Iteration 77, loss = 2.17258505\n",
      "Iteration 78, loss = 2.17310876\n",
      "Iteration 79, loss = 2.17356296\n",
      "Iteration 80, loss = 2.17325426\n",
      "Iteration 81, loss = 2.17169928\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 82, loss = 2.16322306\n",
      "Iteration 83, loss = 2.16248091\n",
      "Iteration 84, loss = 2.16193629\n",
      "Iteration 85, loss = 2.16195467\n",
      "Iteration 86, loss = 2.16223132\n",
      "Iteration 87, loss = 2.16204571\n",
      "Iteration 88, loss = 2.16217979\n",
      "Iteration 89, loss = 2.16210362\n",
      "Iteration 90, loss = 2.16216443\n",
      "Iteration 91, loss = 2.16204882\n",
      "Iteration 92, loss = 2.16226122\n",
      "Iteration 93, loss = 2.16229885\n",
      "Iteration 94, loss = 2.16206698\n",
      "Iteration 95, loss = 2.16166973\n",
      "Iteration 96, loss = 2.16205715\n",
      "Iteration 97, loss = 2.16229175\n",
      "Iteration 98, loss = 2.16179372\n",
      "Iteration 99, loss = 2.16226107\n",
      "Iteration 100, loss = 2.16175514\n",
      "Iteration 101, loss = 2.16218411\n",
      "Iteration 102, loss = 2.16196380\n",
      "Iteration 103, loss = 2.16223582\n",
      "Iteration 104, loss = 2.16192600\n",
      "Iteration 105, loss = 2.16230107\n",
      "Iteration 106, loss = 2.16192490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 107, loss = 2.15974182\n",
      "Iteration 108, loss = 2.15973340\n",
      "Iteration 109, loss = 2.15958083\n",
      "Iteration 110, loss = 2.15972553\n",
      "Iteration 111, loss = 2.15958232\n",
      "Iteration 112, loss = 2.15971048\n",
      "Iteration 113, loss = 2.15965824\n",
      "Iteration 114, loss = 2.15953092\n",
      "Iteration 115, loss = 2.15965647\n",
      "Iteration 116, loss = 2.15953146\n",
      "Iteration 117, loss = 2.15972459\n",
      "Iteration 118, loss = 2.15971876\n",
      "Iteration 119, loss = 2.15968455\n",
      "Iteration 120, loss = 2.15970122\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 121, loss = 2.15910374\n",
      "Iteration 122, loss = 2.15910361\n",
      "Iteration 123, loss = 2.15911375\n",
      "Iteration 124, loss = 2.15909080\n",
      "Iteration 125, loss = 2.15908530\n",
      "Iteration 126, loss = 2.15909542\n",
      "Iteration 127, loss = 2.15909286\n",
      "Iteration 128, loss = 2.15909815\n",
      "Iteration 129, loss = 2.15910869\n",
      "Iteration 130, loss = 2.15911317\n",
      "Iteration 131, loss = 2.15909555\n",
      "Iteration 132, loss = 2.15910277\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 133, loss = 2.15897312\n",
      "Iteration 134, loss = 2.15897121\n",
      "Iteration 135, loss = 2.15897230\n",
      "Iteration 136, loss = 2.15897550\n",
      "Iteration 137, loss = 2.15897289\n",
      "Iteration 138, loss = 2.15897139\n",
      "Iteration 139, loss = 2.15897161\n",
      "Iteration 140, loss = 2.15897164\n",
      "Iteration 141, loss = 2.15897111\n",
      "Iteration 142, loss = 2.15897175\n",
      "Iteration 143, loss = 2.15897195\n",
      "Iteration 144, loss = 2.15897206\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6037037037037037"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfCV = MLPClassifier(hidden_layer_sizes=10, activation='tanh',validation_fraction=0.2, solver='sgd', batch_size=1, alpha=1, learning_rate='adaptive', verbose=1)\n",
    "clfCV.fit(X_train,y_train)\n",
    "clfCV.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
